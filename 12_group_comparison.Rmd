# Comparing groups

```{r echo=FALSE}
library(tidyverse)
library(r4np)
library(rstatix)

wvs_nona <- read_csv("wvs_nona.csv") %>% 
  mutate_if(is.character, as_factor)
```

Social Sciences is about the study of human beings and their interactions. As such, we frequently want to compare two or more groups of human beings, organisations, teams, countries, etc., with each other to see whether they are similar or different from each other. Sometimes we also want to track individuals over time and see how they may have changed in some way or other. In short, comparing groups is an essential technique to make inferences and helps us better understand the diversity that surrounds us.

If we want to perform a group comparison we have to consider which technique is most appropriate for the data we have. Some of it might be related to the type of data we have collected, other aspects might be linked to the distribution of the data. More specifically, before we apply any statistical technique we have to consider at least the following:

-   missing data (see Chapter \@ref(dealing-with-missing-data)),

-   outliers (see Chapter \@ref(dealing-with-outliers), and

-   the assumptions made by analytical techniques about our data.

While we covered missing data and outliers in previous chapters, we have yet to discuss assumptions. For group comparisons there are three main questions we need to answer:

-   Are the groups big enough to be compared, i.e. are they comparable?

-   Is my data parametric or non-parametric? (see Chapter \@ref(parametric-or-non-parametric))

-   How many groups do I wish to compare?

-   Are these groups paired or unpaired?

In the following we will look at group comparisons for parametric and non-parametric data in each category and use the `wvs_nona` dataset, i.e. the `wvs` data frame after we performed imputation (see also Chapter \@ref(replacing-removing-missing-data)). Since we already covered how to test whether data is parametric or non-parametric we will forgo this step out of pure convenience and to remain succinct. We also ignore any potential outliers or missing data. The case studies at the end of the book provide realistic examples of how to perform groups comparisons with a new set of data from start to finish (see Chapter \@ref()). Thus, parametric and non-parametric tests will be demonstrated with the same dataset and the same variables.

## Comparability: Apples vs Oranges {#comparability-apples-vs-oranges}

Before we can jump into group comparisons we need to make ourselves aware of whether our groups can be compared in the first place. 'Comparability' should not be confused with 'are the groups equal'. In many cases, we don't want groups to be equal in terms of participants, e.g. between-subject studies. On the other hand, we might want groups to be perfectly equal when we perform within-subject studies. Thus, asking whether groups are comparable is unrelated to whether the subjects in our study are the same. Instead, we are looking at characteristics of our groups. Some commonly considered characteristics include:

-   *size*: Are the groups about equally large?

-   *time*: Was the data collected around the same time?

-   *exogenous variables*: Is the distribution of characteristics we are not interested in, approximately the same across groups?

When we compare groups we want to minimise the systematic differences that are not the primary focus of our study. Using the right sampling technique can help with this matter. For example, using a random sample and performing a random allocation to groups can help with achieving comparable groups and remove systematic differences in a way no other sampling strategy can. However, there is still no guarantuee that they will be comparable (see also @altman1985comparability and @berger2006review). Besides, we also face the challenge that in Social Sciences we do not have the option of random sampling. For example, International Business studies heavily rely on lists provided by others, e.g. the European Union, Fortune 500, etc., personal judgement and convenience sampling and only a small proportion actually perform probability sampling [@yang2006-IBmethods]. In short, there is no reason to worry if your sampling technique is not random to begin with. However, it emphasises the need to understand your sample and your groups thoroughly.

In order to inspect characteristics of groups we wish to compare, we can use descriptive statistics as we covered them in Chapter \@ref(descriptive-statistics). The only aspect that is different is that we apply these techniques to subsets of our data and not the entire dataset.

For example, we might wish to compare female and male Egyptians (see Chapter \@ref(two-unpaired-groups)). If we wanted to make sure these two groups can be compared we might have to check (among other characteristics) whether their age is distributed similarly. We can use the functions we already know to create a plot to investigate this matter. We could either use a boxplot, or, a bit more accurate, a density plot using the `ggridges` package.

```{r Comparability of Egyptians, echo=TRUE, message=FALSE}
# Only select participants from 'Egypt'
comp <- wvs_nona %>%
  filter(country == "Egypt")

comp %>%
  ggplot(aes(x = age,
             y = gender,
             fill = gender)) +
  ggridges::geom_density_ridges(bandwidth = 4)
```

As we can see, the distribution of `age` across both `gender` groups is fairly similar and likely not different between groups. Of course, we could also statistically explore this using a suitable test before performing the main group comparison. However, we first have to understand how we can perform them.

In the following chapters we will largely rely the package `rstatix` which offers a pipe-friendly approach to using the built-in functions to perform our group comparisons. However, you are welcome to also try the basic functions as well, which you can find in Chapter \@ref(appendix).

## Comparing two groups {#comparing-two-groups}

The simplest of comparisons is the one where you only have two groups. These groups could either consist of different people (unpaired) or represent two measurements of the same individuals (paired).

### Two Unpaired groups {#two-unpaired-groups}

An unpaired group test assumes that the observations in each group are not related to each other, for example that the participants in each group are different individuals.

Our first comparison will be participants from `Egypt` and we want to understand whether male and female citizens in this country perceive that they have `freedom_of_choice`.

We first can compare these two groups using our trusty `geom_boxplot` (or any variation of it).

```{r Two unpaired groups visualisation, echo=TRUE}
# Compute the mean for and size of each group 
group_means <- comp %>% 
  group_by(gender) %>% 
  summarise(g_mean = mean(freedom_of_choice),
            n = n())

group_means

# Create our data visualisation
comp %>% 
  ggplot(aes(x = gender, y = freedom_of_choice, fill = gender)) +
  geom_boxplot() +
  
  # Add the mean for each group
  geom_point(data = group_means,
             aes(x = gender, y = g_mean),
             shape = 3,
             size = 2)
```

While the distribution looks similar, we can notice that the median and the mean (marked by the cross inside the boxplot) are slighter higher for `male` participants. Thus, we can suspect slight differences between these two groups, but we do not know whether these differences are significant or not. To consider the significance (remember Chapter \@ref(significance)) and the effect size (see Table \@ref(tab:effect-size-cohen)) we have to perform statistical tests.

Table \@ref(tab:comparing-two-groups-unpaired) summarises the different tests and functions to perform the group comparison computationally. It is important to note that the parametric test compares the means of two groups, while the non-parametric test compares medians. All of these tests turn significant if the differences between groups is large enough. Thus, significant results can be read as 'these groups are significantly different from each other'. Of course, if the difference is not significant, the groups are considered to be not different from each other. For parametric tests, i.e. `t.test(),` it is also important to indicate whether the variances between these two groups are equal or not. Remember this was one of the assumptions for parametric tests. The Welch t-test can be used if the variances are not equal, but all other criteria for normality are met. By setting `var.equal = TRUE`, a regular T-Test would be performed. By defaulty, `t.test` assumes that variances are not equal. Make sure you test for homogeneity of variance before making your decision (see Chapter \@ref(homogeneity-of-variance).

+----------------+----------------+-------------------------------+-------------+---------------+
| Assumption     | Test           | Function                      | Effect size | Function      |
+================+================+===============================+=============+===============+
| Parametric     | T-Test         | `t_test(var.equal = TRUE)`    | Cohen's d   | `cohens_d()`  |
|                |                |                               |             |               |
|                | Welch T-Test   | `t_test(var.equal = FALSE)`   |             |               |
+----------------+----------------+-------------------------------+-------------+---------------+
| Non-parametric | Mann-Whitney U | `wilcox_test(paired = FALSE)` | Wilcoxon R  | `wilcoxonR()` |
+----------------+----------------+-------------------------------+-------------+---------------+

: (\#tab:comparing-two-groups-unpaired)Comparing two unpaired groups (effect size functions from package `effectsize`, except for `wilcoxonR()` from `rcompanion`

With this information in hand, we can start comparing the female Egyptians with the male Egyptians using the parametric and the non-parametric test for illustration purposes only. By setting `detailed = TRUE` we can obtain the maximum amount for certain comparisons information. In such cases it is advisable to use `glimpse()`. This will make the output easier to read because each row presents one piece of information, rather than having one row with many columns.

```{r Computation two unpaired groups, echo=TRUE}
# T-Test
comp %>% t_test(freedom_of_choice ~ gender,
                var.equal = TRUE,
                detailed = TRUE) %>% 
  glimpse()

# Welch t-test (var.equal = FALSE by default)
comp %>% t_test(freedom_of_choice ~ gender,
                var.equal = FALSE,
                detailed = TRUE) %>% 
  glimpse()

# Mann-Withney U test
comp %>%
  wilcox_test(freedom_of_choice ~ gender,
              detailed = TRUE) %>%
  glimpse()
```

You might notice that the notation within the functions for group tests looks somewhat different to what we are used to, i.e. we use the `~` ('tilde') symbol. Some functions take a formula as their attribute and to distinguish the dependent and independent variable from each other we use `~`. A more generic notation of how formulas in functions work is shown below, where `DV` stands for dependent variable and `IV` stands for independent variable:

::: {#formulas-in-functions align="center"}
function(formula = DV \~ IV)
:::

Group comparisons, even for multiple groups, we usually only have one independent variable, i.e. the grouping variable. Grouping variables are usually of the type `factor`. In case of two groups, we have two levels present in this factor, e.g. `gender`. If there are multiple groups, the factor contains multiple levels, e.g. `country`.

No matter which test we run, it appears as if the difference is significant. However, how big is the difference? The answer to this is provided by the effect size. The interpretation of what the effect size is, follows the explanations in Chapter \@ref(correlations), where we looked at the strength of the correlation of two variables. However, different effect size measures imply that we have to use different benchmarks. To help us a bit with the interpretation we can use the `effectsize` package and their set of `interpret_...()` functions (see also [Indices of Effect Size](https://easystats.github.io/effectsize/articles/interpret.html "Indices of Effect Size"){target="blank"}). Sometimes, there are even more than one way of computing the effect size. For example for the Mann-Whitney test we can choose between the classic Wilcoxon R or the rank-biserial correlation coefficient. In practice, you have to be explicit about how you computed the effect size. The differences between the two measures are often marginal and a matter of taste (or should I say: Your reviewers' taste). Throughout this chapter I will rely on the effect sizes most commonly found in Social Sciences publications. However, feel free to explore other indices as well, especially those offered in the `effectsize` package as well.

```{r Computation of effect size of two unpaired groups, echo=TRUE}
# After parametric test
d <- comp %>% cohens_d(freedom_of_choice ~ gender,
                  var.equal = TRUE,
                  ci = TRUE)

effectsize::interpret_d(d$effsize)

# After non-parametric test
wr <- comp %>% wilcox_effsize(freedom_of_choice ~ gender,
                        ci = TRUE)

effectsize::interpret_r(wr$effsize)
```

Looking at our test results, the female Egyptians perceive `freedom_of_choice` differently from their male counterparts. This is in line with our boxplots. However, the effect sizes tend to be small, which means the differences between the two groups is marginal. Similar to correlations, group comparisons need to be analysed in two stages answering two questions:

1.  Is the difference between groups significant?

2.  Is the difference small, medium or large?

The combination of both analytical steps gives us a comprehensive answer to our research question and enables us to derive with meaningful conclusions. This applies to all group comparisons covered in this book.

### Two Paired groups {#two-paired-groups}

Sometimes, we are not interested in the difference between subjects, but within them, i.e. we want to know whether the same person provides similar or different responses at two different times. Thus, it becomes evident that observations need to be somehow linked to each other. Paired groups are often found and used in longitudinal studies and in experimental studies (e.g. pre-test vs post-test). For example, if we look at our `imdb_top_250` dataset we can see that some directors have more than one movie in the top 250. Therefore, we could be curious to know whether earlier movies of directors have been significantly more successful than their later ones.

```{r Directors and their movies, echo=TRUE}
imdb_top_250 %>%
  group_by(director) %>%
  summarise(n = n()) %>%
  arrange(desc(n))
```

For this investigation we use the modified dataset `dir_mov` which only contains movies of directors who have two or more movies listed in the IMDb Top 250s. Where directors had more than two movies, I randomly sampled two movies. Thus, there is a certain limitation to our dataset.

We can use boxplots to compare earlier movies (i.e. `1`) with later movies (i.e. `2`) across all directors. Thus, each director is reflected in both groups with one of their movies and therefore the same directors can be found in each group. As a measure of success we use the `imdb_rating`.

```{r Comparing two paired groups data visualisation boxplot, echo=TRUE}
dir_mov %>% 
  ggplot(aes(x = movie, y = imdb_rating, fill = movie)) +
  geom_boxplot()
```

The boxplots look almost identical which suggests that the rating of movies in both groups has not changed significantly. However, the boxplot can only show a summary statistics for each group. Thus, it only implies that the movies in group `1` have about the same ratings as the movies in group `2`. If we want to visualise how the ratings have changed for each director from the first to the second movie, we can create a point plot and draw lines with `geom_line()` to connect the movies in each group. A line that movies up indicates that the second movie was rated higher than the first one and vice versa.

```{r Comparing two paired groups data visualisation line plot, echo=TRUE}
dir_mov %>%
  ggplot(aes(x = movie, y = imdb_rating, colour = director)) +
  geom_point() +
  geom_line(aes(group = director)) +
  # Remove the legend
  theme(legend.position = "none")
```

Based on this plot we have to revise our interpretation slightly. Directors who received particularly high ratings on their first movie (i.e. the top 3 in group `1`) scored much lower on the second movie. First, we can notice from our boxplots that these movies count as outliers, and second, obtaining such high scores on a movie is tough to replicate. Needless to say, all these movies are rated as very good, otherwise they would not be in this list. It is worth noting that the way the y axis is scaled emphasises differences. Thus, a difference between a rating of 9 and 8.5 appears large. If we change the range of the y axis to '0-10', the differences appear marginal, but it reflects (1) the possible length of the scale (IMDb ratings range from 0-10) and (2) the magnitude in change relative to the entire scale. More often than note, this 'zoom-in' effect is sometimes used to create the illusion of large differences were there are none. Be aware when you present your findings not to create visualisations that could be misleading.

```{r Comparing two paired groups data visualisation line plot and full y scale, echo=TRUE}
dir_mov %>%
  ggplot(aes(x = movie, y = imdb_rating, colour = director)) +
  geom_point() +
  geom_line(aes(group = director)) +
  # Remove the legend
  theme(legend.position = "none")+
  # Manuall define the y axis range
  ylim(0,10)
```

Considering the revised plot, we likely can predict what the statistical test will show. Table \@ref(tab:comparing-two-groups-paired) summarises which tests and functions need to be performed if our data is parametric or non-parametric. In both cases, the functions are the same to those of the unpaired group comparisons, but we need to add the attribute `paired = TRUE`. Still, the interpretations between the unpaired and paired tests remain the same. Also, be aware that some tests have changed in name, e.g. the Mann-Whitney U test has become the Wilcoxon Signed Rank Test. even though we use the same functions as before, by changing the attributed of `paired` we also change the computational technique to obtain the results. Thus, be aware that the same function can perform different computations.

| Assumption     | Test                      | Function                     | Effect size | Function            |
|----------------|---------------------------|------------------------------|-------------|---------------------|
| Parametric     | T-Test                    | `t_test(paired = TRUE)`      | Cohen's d   | `cohens_d()`        |
| Non-parametric | Wilcoxon Signed Rank Test | `wilcox_test(paired = TRUE)` | Wilcoxon r  | `wilcoxonPairedR()` |

: (\#tab:comparing-two-groups-paired)Comparing two unpaired groups (effect size functions from package `effectsize`, except for `wilcoxonPairedR()` from `rcompanion`)

Let's apply the functions to find out whether the differences we can see in our plots matter.

```{r Comparing two paired groups data computation, echo=TRUE, warning=FALSE}
# Paired T-Test
dir_mov %>% t_test(imdb_rating ~ movie,
                   paired = TRUE,
                   var.equal = TRUE,
                   detailed = TRUE) %>% 
  glimpse()

## Wilcoxon Signed Rank Test
dir_mov %>% wilcox_test(imdb_rating ~ movie,
                         paired = TRUE)
```

As expected, the paired tests reveal that the differences in rating between the first movie and the second movie are not significant. Usually, there would be no reason to follow this up with the computation of effect sizes, because we only need to do this if the differences are statistically significant. However, nothing can stop us from still doing so.

```{r Comparing two paired groups data effec sizes, echo=TRUE}
## After T-Test
d <- cohens_d(imdb_rating ~ movie,
               data = dir_mov,
               paired = TRUE,
               var.equal = TRUE)

effectsize::interpret_d(d$effsize)

# After Wilcoxon Signed Rank Test
wr <- dir_mov %>% wilcox_effsize(imdb_rating ~ movie,
                              paired = TRUE, ci = TRUE)

effectsize::interpret_r(wr$effsize, rules = "cohen1988")
```

As we would expect, the effect sizes are very small too, irrespective of whether we treat our data as parametric or non-parametric. After all, being a successful director ranked in the IMDb top 250 seems to imply that other movies are equally successful, but remember the limitations of our dataset before drawing your final conclusions.

## Comparing more than two groups {#comparing-more-than-two-groups}

Often we find ourselves in situations where comparing two groups is not enough. Instead, we might be faced with three or more groups fairly quickly. For example, the `wvs` dataset let's us look at 48 different countries, all of which we could compare very quickly with just a few lines of code. In the following chapters we look at how we can perform the same type of analysis as before, but with multiple unpaired and paired groups using R. Similarly to the two-samples group comparison, we cover the parametric and non-parametric approaches.

### Multiple unpaired groups {#multiple-unpaired-groups}

Have you ever been wondering whether people in different countries are equally satisfied with their lives? You might have a rough guess that it is not the case, because the social, economic and political environment might place an import role. If you live in a country that is affected by social conflicts, one's life satisfaction might be drastically lower. In the following we take a look at three countries `Iraq`, `Japan` and `Korea`. I did not only chose these countries out of personal interest, but because they nicely demonstrate the purpose of the chapter, i.e. finding out whether there are differences in the perception of `satisfaction` across three countries. At any time, feel free to remove the `filter()` function to gain the results of all countries in the dataset, but prepare for slightly longer computations. We first create the dataset which only contains the three desired countries.

```{r create dataset for multiple unpaired group comparison, echo=TRUE}
mcomp <- wvs_nona %>%
  filter(country == "Iraq" |
           country == "Japan" |
           country == "Korea")
```

Similar to before, we can use the `ggridges` package to draw density plots for each group. This has the added benefit that we can compare the distribution of data for each group and see whether the assumption of normality is likely met or not. On the other hand, we lose the option to easily identify any outliers. You win some and you lose some.

```{r Multiple unpaired groups data visualisation, echo=TRUE}
mcomp %>%
  group_by(country) %>%
  ggplot(aes(x = satisfaction,
             y = reorder(country, satisfaction),
             fill = country)) +
  ggridges::stat_density_ridges(bandwidth = 0.6,
                                quantile_lines = TRUE,   # adds median indicator
                                quantiles = (0.5)) +
  # Remove legend
  theme(legend.position = "none")
```

The plot shows us that `Japan` and `Korea` appear to be very similar if not identical (based on the median), but `Iraq` appears to be different from the other two groups. When performing a multiple group comparison we can follow similar steps as before with two groups, i.e.

-   perform the comparison,

-   determine the effect size, and

-   interpret the effect size.

Table \@ref(tab:comparing-multiple-groups-unpaired) summarises which test needs to be chosen to compare multiple unpaired groups and their corresponding effect size measures.

+----------------+----------------------+--------------------------------------------+------------------------+-----------------------------------------------+
| Assumption     | Test                 | Function for test                          | Effect size            | Function for effect size[^group_comparison-1] |
+================+======================+============================================+========================+===============================================+
| Parametric     | ANOVA                | -   `anova_test` (assumes equal variances) | Eta squared            | `eta_squared()`                               |
|                |                      | -   `oneway.test(var.equal = TRUE/FALSE)`  |                        |                                               |
+----------------+----------------------+--------------------------------------------+------------------------+-----------------------------------------------+
| Non-parametric | Kruskall-Wallis test | `kruskal_test()`                           | Epsilon squared (rank) | `rank_epsilon_squared()`                      |
+----------------+----------------------+--------------------------------------------+------------------------+-----------------------------------------------+

: (\#tab:comparing-multiple-groups-unpaired)Comparing multiple unpaired groups (effect size functions from package `effectsize`)

[^group_comparison-1]: These functions are taken from the `effectsize` package.

Let's begin by conducting the group comparison. As you will notice, `rstatix` currently does not support a parametric test where `var.equal = FALSE`. Therefore we need to fall back to the underlying function `oneway.test(var.equal = FALSE)`

```{r Multiple unpaired groups computation, echo=TRUE, message=FALSE}
# ANOVA
## equal variances assumed
mcomp %>%
  anova_test(satisfaction ~ country,
              detailed = TRUE)

## Equal variances not assumed
(test <- oneway.test(satisfaction ~ country,
            data = mcomp,
            var.equal = FALSE))

# Kruskall-Wallis test
## Perform comparison
(test <- mcomp %>%
  kruskal_test(satisfaction ~ country))
```

While `anova_test()` does provide the effect size automatically, i.e. generalised eta squared (`ges`), this is not the case for the other two approaches. Therefore, we have to use the `effectsize` package to help us out. Packages often can get you a long way and make your life easier, but it is good to know alternatives in case a single package does not give you what you need.

```{r Multiple unpaired groups effect sizes, echo=TRUE, message=FALSE}
# # After ANOVA with var.equal = FALSE
# effectsize::eta_squared(test)
# 
# # effect size rank epsilon squared
# effectsize::rank_epsilon_squared(mcomp$satisfaction ~ mcomp$gender)
# 
# # effect size eta squared (an alternative to epsilon squared)
# mcomp %>% kruskal_effsize(satisfaction ~ country)
```

The results show that there is a significant and large difference between these groups. You might argue that this is actually not quite true. Considering our plot, we know that Japan and Korea do not look like as if they are significantly different. Multiple group comparisons only consider differences across all three groups. Therefore, if one group lies far away from the other groups, the test will turn significant and even provide a large enough effect size to consider it important. However, these tests do not provide information which differences between groups are significant. To gain more clarification about this, we need to incorporate another step, so called 'post-hoc tests'. These tests compare two groups at a time, which is why they are also known as 'pairwise comparisons'. Compared to regular two-sample tests, these perform corrections of the `p`values for mulitple testing, which is necessary. However, there are many different 'post-hoc' tests one can choose from. @field2013discovering (p.459) nicely outlines the different scenarios and provides recommends to navigate this slightly complex field of post-hoc tests to follow-up a one-way ANOVA. Table \@ref(tab:post-hoc-tests) provides an overview of his suggestions.

+----------------------------------+-----------------+--------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Equal sample size                | Equal variances | Post-hot tests     | Functions in R                                                                                                                                                                                   |
+==================================+=================+====================+==================================================================================================================================================================================================+
| YES                              | YES             | -   REGWQ,         | -   `mutoss::regwq()`[^group_comparison-2]                                                                                                                                                       |
|                                  |                 |                    |                                                                                                                                                                                                  |
|                                  |                 | -   Tukey,         | -   `rstatix::tukey_hsd()`                                                                                                                                                                       |
|                                  |                 |                    |                                                                                                                                                                                                  |
|                                  |                 | -   Bonferroni     | -   `pairwise.t.test(p.adjust.method = "bonferroni")`                                                                                                                                            |
+----------------------------------+-----------------+--------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| NO (slightly different)          | YES             | -   Gabriel        |                                                                                                                                                                                                  |
+----------------------------------+-----------------+--------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| YES                              | YES             | -   Hochberg's GT2 | -   Not available in R and should not be confused with `pairwise.t.test(p.adjust.method = "hochberg")`, which is based on @hochberg1988sharper. The GT2, however, is based on @hochberg1974some. |
+----------------------------------+-----------------+--------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| NO (not ideal for small samples) | NO              | -   Games-Howell   | -   `rstatix::games_howell_test()`                                                                                                                                                               |
+----------------------------------+-----------------+--------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

: (\#tab:post-hoc-tests) Different post-hoc tests for different scenarios (parametric)

[^group_comparison-2]: In order to use this package, it is necessary to install a series of other packages found on [bioconductor.org](https://www.bioconductor.org "bioconductor.org"){target="blank"}

You might be surprised to see that there are also post-hoc tests for parametric group comparisons when the assumption of equal variances is not assumed. Would we not have to use a non-parametric test for our group comparison instead? Well, empirical studies have demonstrated that ANOVAs tend to produce robust results, even if the assumption of normality (e.g. @blanca2017non) is not given, or there is some degree of heterogeneity of variance between groups [@tomarken1986comparison]. In other words, there can be some leniancy (or flexibility?) when it comes to the violation of parametric assumptions. If you want to reside on the save side, you should ensure you know your data and its properties. If in doubt, non-parametric tests are also available.

If we want to follow up the Kruskall-Wallis test, i.e. the non-parametric equivalent to the one-way ANOVA, we can make use of two post-hoc tests:

-   Dunn Test: `rstatix::dunn_test()` [@dinno2015nonparametric]
-   Pairwise comparison with Bonferroni (and other) correction: `pairwise.wilcox.test()`.

Below are some examples of how you would use these functions in your project. However, be aware that some of the post-hoc tests are not or not well implemented yet in R. Here I show the most important ones which likely serve you in 90% of the cases.

```{r Multiple group comparison post-hoc tests, echo=TRUE}
# POST_HOC TEST FOR PARAMETRIC DATA
# Bonferroni
pairwise.t.test(mcomp$satisfaction, mcomp$country, p.adjust.method = "bonferroni")

# Tukey
mcomp %>% tukey_hsd(satisfaction ~ country)

# Games-Howell
mcomp %>% games_howell_test(satisfaction ~ country)

# POST-HOC TEST FOR NON-PARAMETRIC DATA
mcomp %>% dunn_test(satisfaction ~ country)
# or
pairwise.wilcox.test(mcomp$satisfaction, mcomp$country, p.adjust.method = "holm")
```

As we can see, no matter which function we use, the interpretation of the results remain the same on this occasion.

### Multiple paired groups {#multiple-paired-groups}

Additional condition if you compare three or more groups with each other. Might not be good to call them groups actually, because it refers to different measures by the same person usually over an extended period of time.

Is an extension of the linear model - so why not simply use a regression instead. Is it because regression primarily use quantitative data and not categorical type data?

repeated measures ANOVA (find dataset)

```{r Repeated measures ANOVA, echo=TRUE}
# anova_test()
# 
# eta_squared()
# omega_squared()
```

Friedman Test (find dataset)

```{r Friedman test, echo=TRUE}
# # NON=PARAMETRIC COMPARISON
# friedman_test()
# 
# # Effect size
# kendalls_w()
```

A useful overview of all the possible options of comparing groups and how to obtain their effect sizes can also be found here

<https://indrajeetpatil.github.io/statsExpressions/articles/stats_details.html>

## Comparing groups based on factors: Contingency tables {#chi-squared-test}

Unpaired

```{r Contigency tables unpaired computation, echo=TRUE}
# wvs_nona %>% infer::chisq_test(relationship_status ~ gender)
# 
# cramers_v()
```

Paired

```{r Contigency tables paired computation, echo=TRUE}
# mcnemar.test()
# 
# # Effect size
# cohens_g()
```

## A cheatsheet to guide your own group comparisons {#cheatsheet-group-comparisons}

To summarise what we have covered in this section, we can consider

```{r How to decide on the right group comparison, echo=FALSE}
# DiagrammeR::mermaid("
#         graph TB
# 
#         data(my data)
#         datatype{type of data}
#         quan[quantitative]
#         qual[qualitative]
#         paracheck{<center>check for <br> assumptions</center>}
#         addlin[additivity and linerarity]
#         normal[normality]
#         homvar[homogeneity of variance]
#         indep[independence]
#         para[parametric]
#         npara[non-parametric]
#         
#         data --> datatype
#         
#         subgraph 
#         datatype --> quan
#         datatype --> qual
#         end
#         
#         
#         quan --> paracheck
#         qual --> npara
#         
#         paracheck --> addlin
#         
#         subgraph 
#         addlin -- yes --> indep
#         indep -- yes --> normal
#         normal -- yes --> homvar
#         end
# 
#         addlin -- no --> npara
#         indep -- no --> npara
#         normal -- no --> npara
#         homvar -- no --> npara
# 
#         homvar -- yes --> para
#         
#         # hwg[How many groups?]
#         ")
# 

```

% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={R for Non-Programmers: A Guide for Social Scientists},
  pdfauthor={Daniel Dauber},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{R for Non-Programmers: A Guide for Social Scientists}
\author{Daniel Dauber}
\date{2021-08-11}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{welcome}{%
\chapter*{Welcome üëã}\label{welcome}}
\addcontentsline{toc}{chapter}{Welcome üëã}

\includegraphics{images/chapter_00_img/r_for_non_programmers_logo.png}

Welcome to \emph{R for Non-Programmers: A guide for Social Scientists}. This book is intended to be of help to everyone who wishes to enter the world of R Programming, but not necessarily for the purpose to become a programmer. Instead, this book intends to convey key concepts in data analysis, especially quantitative research.

{[}Add more{]}

\hypertarget{acknowledgments}{%
\chapter*{Acknowledgments üôè}\label{acknowledgments}}
\addcontentsline{toc}{chapter}{Acknowledgments üôè}

Special thanks are due to my wife, who supported me in so many ways to get this book completed. Also, I would like to thank my son, who patiently watched me sitting at the computer type this book up.

\hypertarget{readme-before-you-get-started}{%
\chapter{\texorpdfstring{\texttt{Readme.} Before you get started}{Readme. Before you get started}}\label{readme-before-you-get-started}}

\hypertarget{a-starting-point-and-reference-book}{%
\section{A starting point and reference book}\label{a-starting-point-and-reference-book}}

\texttt{to\ be\ added}

\hypertarget{download-the-companion-r-package}{%
\section{Download the companion R package}\label{download-the-companion-r-package}}

\texttt{to\ be\ added}

\begin{itemize}
\item
  Explain where to download
\item
  Explain how examples are shown in this book (code in the first grey box, console results in the second grey box
\end{itemize}

\hypertarget{a-tidyverse-approach-with-some-basic-r}{%
\section{A `tidyverse' approach with some basic R}\label{a-tidyverse-approach-with-some-basic-r}}

\texttt{to\ be\ added}

\hypertarget{why-learn-a-programming-language-as-a-non-programmer}{%
\chapter{Why learn a programming language as a non-programmer?}\label{why-learn-a-programming-language-as-a-non-programmer}}

\emph{`R'}, it is not just a letter you learn in primary school, but a powerful programming language. While it is used for a lot of quantitative data analysis, it has grown over the years to become a powerful tool that excels (\#no-pun-intended) in handling data and performing customised computations with quantitative and qualitative data.

\emph{R}~is now one of my core tools to perform various types of analysis because I can use it in many different ways, for example,

\begin{itemize}
\tightlist
\item
  statistical analysis,
\item
  corpus analysis,
\item
  development of online dashboards to dynamically generate interactive data visualisations,
\item
  connection to social media APIs for data collection,
\item
  Creation of reporting systems to provide individualised feedback to research participants,
\item
  Drafting and writing research articles, etc.
\end{itemize}

\begin{quote}
\emph{Learning R is like learning a foreign language. If you like learning languages, then `R' is just another one.}
\end{quote}

While \emph{R} has become a comprehensive tool for data scientists, it has yet to find its way into the mainstream field of Social Sciences. Why? Well, learning programming languages is not necessarily something that feels comfortable to everyone. It is not like Microsoft Word, where you can open the software and explore it through trial and error. Learning a programming language is like learning a foreign language: You have to learn vocabulary, grammar and syntax. Similar to learning a new language, programming languages also have steep learning curves and require quite some commitment.

For this reason, most people do not even dare to learn it because it is time-consuming and often not considered a \emph{`core method'} in Social Sciences disciplines. Apart from that, tools like SPSS have very intuitive interfaces, which seem much easier to use (or not?). However, the feeling of having \emph{`mastered'} \emph{R} (although one might never be able to claim this) can be extremely rewarding.

I guess this introduction was not necessarily helpful in convincing you to learn any programming language. However, despite those initial hurdles, there are a series of advantages to consider. Below I list some good reasons to learn a programming language as they pertain to my own experiences.

\hypertarget{learning-new-tools-to-analyse-your-data-is-always-essential}{%
\section{Learning new tools to analyse your data is always essential}\label{learning-new-tools-to-analyse-your-data-is-always-essential}}

Theories change over time, and new insights into certain social phenomena are published every day. Thus, your knowledge might get outdated quite quickly. This is not so much the case for research methods knowledge. Typically, analytical techniques remain over many years. We still use the mean, mode, quartiles, standard deviation, etc., to describe our quantitative data. Still, there are always new computational methods that help us to crunch the numbers even more. \emph{R} is a tool that allows you to venture into new analytical territory because it is open source. Thousands of developers provide cutting-edge research methods free of charge for you to try with your data. You can find them on platforms like \href{https://github.com}{GitHub}. \emph{R} is like a giant supermarket, where all products are available for free. However, to read the labels on the product packaging and understand what they are, you have to learn the language used in this supermarket.

\hypertarget{programming-languages-enhance-your-conceptual-thinking}{%
\section{Programming languages enhance your conceptual thinking}\label{programming-languages-enhance-your-conceptual-thinking}}

While I have no empirical evidence for this, I am very certain it is true. While I would argue that my conceptual thinking is quite good, I would not necessarily say that I was born with it. Programming languages are very logical. Any error in your code will make you fail to execute it properly. Sometimes you face challenges in creating the correct code to solve a problem. Through creative abstract thinking (I should copyright this term), you start to approach your problems differently, whether it is a coding problem or a problem in any other context. For example, I know many students enjoy the process of qualitative coding. However, they often struggle to detach their insights from the actual data and synthesise ideas on an abstract and more generic level. Qualitative researchers might refer to this as challenges in '\emph{second-order deconstruction of meaning'}. This process of abstraction is a skill that needs to be honed, nurtured and practised. From my experience, programming languages are one way to achieve this, but they might not be recognised for this just yet.

\hypertarget{programming-languages-allow-you-to-look-at-your-data-from-a-different-angle}{%
\section{Programming languages allow you to look at your data from a different angle}\label{programming-languages-allow-you-to-look-at-your-data-from-a-different-angle}}

There are certainly commonly known and well-established techniques regarding how you should analyse your data rigorously. However, it can be quite some fun to try techniques outside your disciplines. This does not only apply to programming languages, of course. Sometimes, learning about a new research method enables you to look at your current tools in very different ways too. One of the biggest challenges for any researcher is to reflect on your work. Learning new and maybe even \emph{`strange'} tools can help with this. Admittedly, sometimes you might find out that some new tools are also a dead-end. Still, you might have learned something valuable through the process of engaging with your data differently. So shake off the rust of your analytical routine and blow some fresh air into your research methods.

\hypertarget{learning-any-programming-language-will-help-you-learn-other-programming-languages.}{%
\section{Learning any programming language will help you learn other programming languages.}\label{learning-any-programming-language-will-help-you-learn-other-programming-languages.}}

Once you understand the logic of one language, you will find it relatively easy to understand new programming languages. Of course, if you wanted to, you could become the next '\emph{Neo'} (from \href{https://www.imdb.com/title/tt0133093/?ref_=ext_shr_lnk}{`The Matrix')} and change the reality of your research forever. On a more serious note, though, if you know any programming language already, learning R will be easier because you have accrued some basic understanding of these particular types of languages.

Having considered everything of the above, do you feel ready for your next foreign language?

\hypertarget{setting-up-r-and-rstudio}{%
\chapter{Setting up R and RStudio}\label{setting-up-r-and-rstudio}}

Every journey starts with gathering the right equipment. This intellectual journey is not much different. The first step that every '\emph{R} novice has to face is to set everything up to get started. There are essentially two strategies:

\begin{itemize}
\tightlist
\item
  Install \href{https://www.r-project.org}{\emph{R}}and \href{https://www.rstudio.com}{RStudio}
\end{itemize}

or

\begin{itemize}
\tightlist
\item
  Run RStudio in a browser via \href{https://rstudio.cloud}{RStudio Cloud}
\end{itemize}

While installing~\emph{R}~and Studio requires more time and effort, I strongly recommend it, especially if you want to work offline or make good use of your computer's CPU. However, if you are not sure yet whether you enjoy learning~\emph{R}, you might wish to look at RStudio Cloud first. Either way, you can follow the examples of this book no matter which choice you make.

\hypertarget{installing-r}{%
\section{Installing R}\label{installing-r}}

The core module of our programming is R itself, and since it is an open-source project, it is available for free on Windows, Mac and Linux computers. Here is what you need to do to install it properly on your computer of choice:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Go to \href{https://www.r-project.org\%5D(https://www.r-project.org)}{www.r-project.org}

  \includegraphics{images/chapter_03_img/r_project/00_r_project_page.png}
\item
  Click on \texttt{CRAN} where it says download.
\item
  Choose a server in your country (all of them work, but downloads will perform quicker).

  \includegraphics{images/chapter_03_img/r_project/01_r_project_cran_mirror.png}
\item
  Select the operating system for your computer.

  \includegraphics{images/chapter_03_img/r_project/02_r_project_os_choice.png}
\item
  Select the version you want to install (I recommend the latest version)

  \includegraphics{images/chapter_03_img/r_project/03_r_project_version_choice.png}
\item
  Open the downloaded file and follow the installation instructions. (I recommend leaving the suggested settings as they are).
\end{enumerate}

This was relatively easy. You now have \emph{R} installed. Technically you can start using \emph{R} for your research, but there is one more tool I strongly advise installing: RStudio.

\hypertarget{installing-rstudio}{%
\section{Installing RStudio}\label{installing-rstudio}}

\emph{R} by itself is just the *`beating heart'* of \emph{R} programming, but it has no particular user interface. If you want buttons to click and actually `see' what you are doing, there is no better way than RStudio. RStudio is an \emph{integrated development environment} (IDE) and will be our primary tool to interact with \emph{R}. It is the only software you need to do all the fun parts and, of course, to follow along with the examples of this book. To install RStudio perform the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Go to \href{https://www.rstudio.com\%5D(https://www.rstudio.com)}{www.rstudio.com}.

  \includegraphics{images/chapter_03_img/rstudio/01_rstudio_main_page.png}
\item
  Go to \texttt{Products\ \textgreater{}\ RStudio}

  \includegraphics{images/chapter_03_img/rstudio/02_rstudio_main_page_menu.png}
\item
  On this page, scroll down and select \texttt{RStudio\ Desktop}

  \includegraphics{images/chapter_03_img/rstudio/03_rstudio_select_version.png}
\item
  Select the \texttt{\textquotesingle{}Open\ Source\ Edition\textquotesingle{}} option by clicking on '\texttt{Download\ RStudio\ Desktop\textquotesingle{}}

  \includegraphics{images/chapter_03_img/rstudio/04_rstudio_select_edition.png}
\item
  As a last step, scroll down where it shows you a download button for your operating system. The website will automatically detect this. You also get a nice reminder to install `R' first, in case you have not done so yet.

  \includegraphics{images/chapter_03_img/rstudio/05_rstudio_download.png}
\item
  Open the downloaded file and follow the installation instructions (again, keep it to the default settings as much as possible)
\end{enumerate}

Congratulations, you are all set up to learn \emph{R}. From now on you only need to start RStudio and not \emph{R}. Of course, if you are the curious, nothing shall stop you to try \emph{R} without RStudio.

\hypertarget{when-you-first-start-rstudio}{%
\section{When you first start RStudio}\label{when-you-first-start-rstudio}}

Before you start programming away, you might want to make some tweaks to your settings right away to have a better experience (in my humble opinion). I recommend at least the following two changes by clicking on \texttt{RStudio\ \textgreater{}\ Preferences} or press \texttt{‚åò/Ctrl\ +\ ,}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  In the \texttt{Code\ \textgreater{}\ Editing} tab, make sure to have at least the first five options ticked, especially the \texttt{Auto-indent\ code\ after\ paste}. This setting will save time when trying to format your coding appropriately, making it easier to read. Indentation is the primary way of making your code look more readable and less like a series of characters that appear almost random.

  \includegraphics{images/chapter_03_img/rstudio_preferences/00_rstudio_preferences_editing.png}
\item
  In the \texttt{Display} tab, you might want to have the first three options selected. In particular, \texttt{Highlight\ selected\ line} is helpful because, in more complicated code, it is helpful to see where your cursor is.

  \includegraphics{images/chapter_03_img/rstudio_preferences/01_rstudio_preferences_display.png}
\end{enumerate}

Of course, if you wish to customise your workspace further, you can do so. The visually most impactful way to alter the default appearance of RStudio is to select Appearance and pick a completely different colour theme. Feel free to browse through various options and see what you prefer. There is no right or wrong here.

\includegraphics{images/chapter_03_img/rstudio_preferences/02_rstudio_preferences_appearance.png}

\hypertarget{updating-r-and-rstudio}{%
\section{Updating R and RStudio: Living at the pulse of innovation}\label{updating-r-and-rstudio}}

While not strictly something that helps you become a better programmer, this advice might come in handy to avoid turning into a frustrated programmer. When you update your software, you need to update R and RStudio separately from each other. While both R and RStudio work closely with each other, they still constitute separate pieces of software. Thus, it is essential to keep in mind that updating RStudio will not automatically update R. This can become problematic if specific packages you installed via RStudio (like a fancy learning algorithm) might not be compatible with earlier versions of R. Also, additional R packages developed by other people are separate pieces and are updated too, independently from R and RStudio.

I know what you are thinking: This already sounds complicated and cumbersome. However, rest assured, we take a look at how you can easily update all your packages with RStudio. Thus, all you need to remember is:~\emph{R}~needs to be updated separately from everything else.

\hypertarget{rstudio-cloud}{%
\section{RStudio Cloud}\label{rstudio-cloud}}

\texttt{to\ be\ completed}

\hypertarget{the-rstudio-interface}{%
\chapter{The RStudio Interface}\label{the-rstudio-interface}}

The RStudio interface is composed of quadrants, each of which fulfils a unique purpose:

\begin{itemize}
\item
  The \texttt{Console} window,
\item
  The \texttt{Source} window,
\item
  The \texttt{Environment\ /\ History\ /\ Connections\ /\ Tutorial} window, and
\item
  The \texttt{Files\ /\ Plots\ /\ Packages\ /\ Help\ /\ Viewer} window
\end{itemize}

You might only see three windows and wonder where the \texttt{Source} window has gone in your version of RStudio. In order to use it you have to either open a file or create a new one. You can create a new file by selecting \texttt{File\ \textgreater{}\ New\ File\ \textgreater{}\ R\ Script} in the menu bar, or use the keyboard shortcut \texttt{Ctrl+Shift+N} on PC and \texttt{Cmd+Shift+N} on Mac.

I will briefly explain the purpose of each window/pane and how they are relevant to your work in \emph{R}.

\hypertarget{the-console-window}{%
\section{The Console window}\label{the-console-window}}

The console is located in the bottom-left, and it is where you often will find the output of your coding and computations. It is also possible to write code directly into the console. Let's try the following example by calculating the sum of \texttt{10\ +\ 5}. Click into the console with your mouse, type the calculation into your console and hit \texttt{Enter/Return\ ‚Üµ} on your keyboard. The result should be pretty obvious:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We type the below into the console üëá}
\DecValTok{10}\SpecialCharTok{+}\DecValTok{5}
\DocumentationTok{\#\# [1] 15}
\end{Highlighting}
\end{Shaded}

Here is a screenshot of how it should look like at your end in RStudio:

\includegraphics{images/chapter_04_img/02_console_window/console_algebra.png}

You just successfully performed your first successful computation. I know, this is not quite impressive just yet. \emph{R} is undoubtedly more than just a giant calculator.

In the top right of the console, you find a symbol that looks like a broom. This one is quite an important one because it clears your console. Sometimes the console can become very cluttered and difficult to read. If you want to remove whatever you computed, you can click the broom icon and clear the console of all text. I use it so frequently that I strongly recommend learning the keyboard shortcut, which is \texttt{Ctrl+L} on PC and Mac.

\hypertarget{the-source-window}{%
\section{The Source window}\label{the-source-window}}

In the top left, you can find the source window. The term `source' can be understood as any type of file, e.g.~data, programming code, notes, etc. The source panel can fulfil many functions, such as:

\begin{itemize}
\item
  Inspect data in an Excel-like format (LINK TO RELEVANT CHAPTER)
\item
  Open programming code, e.g.~an R Script (LINK TO RELEVANT CHAPTER)
\item
  Open other text-based file formats, e.g.

  \begin{itemize}
  \item
    Plain text (.txt),
  \item
    Markdown (.md),
  \item
    Websites (.html),
  \item
    LaTeX (.tex),
  \item
    BibTex (.bib),
  \end{itemize}
\item
  Edit scripts with code in it,
\item
  Run the analysis you have written.
\end{itemize}

\includegraphics{images/chapter_04_img/03_source_window/01_rstudio_source.png}

In other words, the source window will show you whatever file you are interested in, as long as RStudio can read it - and no, Microsoft Office Documents are not supported. Another limitation of the source window is that it can only show text-based files. So opening images, etc. would not work.

\hypertarget{the-environment-history-connections-tutorial-window}{%
\section{The Environment / History / Connections / Tutorial window}\label{the-environment-history-connections-tutorial-window}}

The window in the top right shows multiples panes. The first pane is called \emph{Environment} and shows you objects which are available for computation. One of the first objects you will create is your dataset because, without data, we cannot perform any analysis. Thus, one object might be your data. Another object could be a plot showing the number of male and female participants in your study. To find out how to create objects yourself, you can take a glimpse at (INSERT CHAPTER X). Besides datasets and plots, you will also find other objects here, e.g.~lists, vectors and functions you created yourself. Don't worry if none of these words makes sense at this point. We will cover each of them in the upcoming chapters. For now, remember this is a place where you can find different objects you created.

\includegraphics{images/chapter_04_img/04_environment_history_etc/01_rstudio_environment.png}

The \emph{History} pane is very easy to understand. Whatever computation you run in the Console will be stored. So you can go back and see what you coded and rerun that code. Remember the example from above where we computed the sum of \texttt{10+5}? This computation is stored in the history of RStudio, and you can rerun it by clicking on \texttt{10+5} in the history pane and then click on \texttt{To\ Console}. This will insert \texttt{10+5} back into the Console, and we can hit \texttt{Return\ ‚Üµ} to retrieve the result. You also have the option to copy the code into an existing or new R Script by clicking on \texttt{To\ Source}. By doing this, you can save this computation on your computer and reuse it later. Finally, if you would like to store your history, you can do so by clicking on the \texttt{floppy\ disk\ symbol}. There are two more buttons in this pane, one allows you to delete individual entries in the history, and the last one, a \texttt{broom}, clears the entire history (irrevocably).

\includegraphics{images/chapter_04_img/04_environment_history_etc/02_rstudio_history.png}

The pane \emph{Connections} allows you to tab into external databases directly. This can come in handy when you work collaboratively on the same data or want to work with extensive datasets without having to download them. However, for an introduction to R, we will not use this feature of RStudio for now.

\includegraphics{images/chapter_04_img/04_environment_history_etc/03_rstudio_connections.png}

The last pane is called \emph{Tutorial}. Here you can find additional materials to learn \emph{R} and RStudio. If you search for more great content to learn R, this serves as a great starting point.

\includegraphics{images/chapter_04_img/04_environment_history_etc/04_rstudio_tutorial.png}

\hypertarget{the-files-plots-packages-help-viewer-window}{%
\section{The Files / Plots / Packages / Help / Viewer window}\label{the-files-plots-packages-help-viewer-window}}

The last window consists of five essential panes. The first one is the \emph{Files} pane. As the name indicates, it lists all the files and folders in your root directory. A root directory is the default directory where RStudio saves your files, for example, your analysis. However, you can easily change this directory to something else (see also CHAPTER X) or use R Project files (see CHAPTER X) to carry out your research. Thus, the \emph{Files} pane is an easy way to load data into RStudio and create folders to keep your research project well organised.

\includegraphics{images/chapter_04_img/05_files_plots_etc/01_rstudio_files.png}

Since the Console cannot reproduce data visualisations, RStudio offers a way to do this very easily. It is through the Plots pane. This pane is exclusively designed to show you any plots you have created using R. Here is a simple example that you can try. Type into your console \texttt{boxplot(mtcars\$hp)}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Here we create a nice boxplot using a dataset called \textquotesingle{}mtcars\textquotesingle{}}
\FunctionTok{boxplot}\NormalTok{(mtcars}\SpecialCharTok{$}\NormalTok{hp)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_for_non_programmers_files/figure-latex/Simple boxplot-1} \end{center}

Although this is a short piece of coding, it performs quite a lot of steps:

\begin{itemize}
\item
  it uses a function called \texttt{boxplot()} to draw a boxplot of
\item
  a variable called \texttt{hp} (for horsepower), which is located in
\item
  a dataset named \texttt{mtcars},
\item
  and it renders the graph in your \emph{Plots} pane
\end{itemize}

This is how the plot should look like in your RStudio \emph{Plots} pane.

\includegraphics{images/chapter_04_img/05_files_plots_etc/02_rstudio_plots.png}

If you wish to delete the plot, you can click on the \texttt{red\ circle\ with\ a\ white\ x} symbol. This will delete the currently visible plot. If you wish to remove all plots from this pane, you can use the \texttt{broom}. There is also an option to export your plot and move back and forth between different plots.

Do not worry about the coding at this point. It will all make sense in the following chapters.

The next pane is called \emph{Packages}. Packages are additional tools you can import and use when performing your analysis. A frequent analogy people use to explain packages is your phone and the apps you install. Each package you download is equivalent to an app on your phone. It can enhance different aspects of working in \emph{R}, such as creating animated plots, using unique machine learning algorithms, or simply making your life easier by doing multiple computations with just one single line of code. You will learn more about \emph{R packages} in Chapter \ref{r-packages}.

\includegraphics{images/chapter_04_img/05_files_plots_etc/03_rstudio_packages.png}

If you are in dire need of help, RStudio provides you with a \emph{Help} pane. You can search for specific topics, for example how certain computations work. The \emph{Help} pane also has documentation on different datasets that are included in \emph{R}, RStudio or \emph{R packages} you have installed. If you want a more comprehensive overview of how you can find help, have a look at CRAN's \href{https://www.r-project.org/help.html}{`Getting Help with R'} webpage.

\includegraphics{images/chapter_04_img/05_files_plots_etc/04_rstudio_help.png}

So, for example, if you want to know what the \texttt{mtcars} dataset is, you can either use the search window in the \emph{Help} pane or, much easier, use a \texttt{?} in the console to search for it:

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Type a \textquotesingle{}?\textquotesingle{} and immediately add the name to bring up helpful information.}

\NormalTok{?mtcars}
\end{Highlighting}
\end{Shaded}

This will open the \emph{Help} pane and give you more information about this dataset:

\includegraphics{images/chapter_04_img/05_files_plots_etc/04_rstudio_help_mtcars.png}

There are many different ways of how you can find help with your coding beyond RStudio and this book. My top three platforms to find solutions to my programming problems are:

\begin{itemize}
\item
  \href{https://www.google.com}{Google}
\item
  \href{https://stackoverflow.com}{stackoverflow.com}
\item
  \href{https://twitter.com/home}{Twitter} (with \href{https://twitter.com/hashtag/rstats}{\#RStats})
\end{itemize}

Lastly, we have the \emph{Viewer} pane. Not every data visualisation we create in R is a static image. You can create dynamic data visualisations or even websites with R. This type of content is displayed in the Viewer pane rather than in the Plots pane. Often these visualisations are based on HTML and other web-based programming languages. As such, it is easy to open them in your browser as well. However, in this book, we mainly focus on two-dimensional static plots, which are the ones you likely need most of the time, either for your assignments, thesis, or publication.

\includegraphics{images/chapter_04_img/05_files_plots_etc/05_rstudio_viewer.png}

\hypertarget{customise-your-user-interface}{%
\section{Customise your user interface}\label{customise-your-user-interface}}

As a last remark in this chapter, I would like to make you aware that you can modify each window. There are three basic adjustments you can make:

\begin{itemize}
\item
  Hide panes by clicking on the window symbol in the top right corner of each window,
\item
  Resize panes by dragging the border of a window horizontally or vertically, or
\item
  Add and remove panes by going to \texttt{RStudio\ \textgreater{}\ Preferences\ \textgreater{}\ Pane\ Layout}, or use the keyboard shortcut \texttt{‚åò\ +\ ,} if you are on a Mac. There is, unfortunately no default shortcut for PC users.
\end{itemize}

If you want a fully customised experience you can also alter the colour scheme of the RStudio itself (\texttt{RStudio\ \textgreater{}\ Preferences\ \textgreater{}\ Appearance}) and if the themes offered are not enough for you, you can create a custom theme \href{https://tmtheme-editor.herokuapp.com/\#!/editor/theme/Monokai}{here}

\hypertarget{r-basics-the-very-fundamentals}{%
\chapter{R Basics: The very fundamentals}\label{r-basics-the-very-fundamentals}}

After a likely tedious installation of R and RStudio, as well as a somewhat detailed introduction to the RStudio interface, you are finally ready to `do' things. By `doing', I mean coding. The term `coding' in itself can instil fear in some of you, but you only need one skill to do it: Writing. As mentioned earlier, learning coding or programming means learning a new language. However, once you have the basic grammar down, you already can communicate quite a bit. In this section, we will explore the fundamentals of R. These build the foundation for everything that follows. After that, we dive right into some analysis.

\hypertarget{basic-computations-in-r}{%
\section{Basic computations in R}\label{basic-computations-in-r}}

The most basic computation you can do in R is arithmetic operations. In other words, addition, subtraction, multiplication, division, exponentiation and extraction of roots. In other words, R can be used like your pocket calculator, or more likely the one you have on your phone. For example, in Chapter \ref{the-console-window} we already performed an addition. Thus, it might not come as a surprise how their equivalents work in R. Let's take a look at the following examples:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Addition}
\DecValTok{10} \SpecialCharTok{+} \DecValTok{5}
\DocumentationTok{\#\# [1] 15}

\CommentTok{\# Subtraction}
\DecValTok{10} \SpecialCharTok{{-}} \DecValTok{5}
\DocumentationTok{\#\# [1] 5}

\CommentTok{\# Multiplication}
\DecValTok{10} \SpecialCharTok{*} \DecValTok{5}
\DocumentationTok{\#\# [1] 50}

\CommentTok{\# Division}
\DecValTok{10} \SpecialCharTok{/} \DecValTok{5}
\DocumentationTok{\#\# [1] 2}

\CommentTok{\# Exponentiation}
\DecValTok{10} \SpecialCharTok{\^{}} \DecValTok{2}
\DocumentationTok{\#\# [1] 100}

\CommentTok{\# Square root}
\FunctionTok{sqrt}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\DocumentationTok{\#\# [1] 3.162278}
\end{Highlighting}
\end{Shaded}

They all look fairly straightforward except for the extraction of roots. As you probably know, extracting the root would typically mean we use the symbol \(\sqrt{}\) on your calculator. To compute the square root in R, we have to use a function instead to perform the computation. So we first put the name of the function \texttt{sqrt} and then the value \texttt{10} within parenthesis \texttt{()}. This results in the following code: \texttt{sqrt(10)}. If we were to write this down in our report, we would write \(\sqrt[2]{10}\).

Functions are an essential part of R and programming in general. You will learn more about them in this chapter.Besides arithmetic operations, there are also logical queries you can perform. Logical queries always return either the value TRUE or FALSE. Here are some examples which make this clearer:

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\#1 Is it TRUE or FALSE?}
\DecValTok{1} \SpecialCharTok{==} \DecValTok{1}
\DocumentationTok{\#\# [1] TRUE}

\CommentTok{\#2 Is 45 bigger than 55?}
\DecValTok{45} \SpecialCharTok{\textgreater{}} \DecValTok{55}
\DocumentationTok{\#\# [1] FALSE}

\CommentTok{\#3 Is 1982 bigger or equal to 1982?}
\DecValTok{1982} \SpecialCharTok{\textgreater{}=} \DecValTok{1982}
\DocumentationTok{\#\# [1] TRUE}

\CommentTok{\#4 Are these two words NOT the same?}
\StringTok{"Friends"} \SpecialCharTok{!=} \StringTok{"friends"}
\DocumentationTok{\#\# [1] TRUE}

\CommentTok{\#5 Are these sentences the same?}
\StringTok{"I love statistics"} \SpecialCharTok{==} \StringTok{"I love statist√≠cs"}
\DocumentationTok{\#\# [1] FALSE}
\end{Highlighting}
\end{Shaded}

Reflecting on these examples, you might notice three important things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  I used \texttt{==} instead of \texttt{=},
\item
  I can compare non-numerical values, i.e.~text, which is also known as \texttt{character} values, with each other,
\item
  The devil is in the details (considering \#5).
\end{enumerate}

One of the most common mistakes of R novices is the confusion around the \texttt{==} and \texttt{=} notation. While \texttt{==} represents \texttt{equal\ to}, \texttt{=} is used to assign a value to an object (for more details on assignments see Chapter \ref{the-files-plots-packages-help-viewer-window}). However, in practice, most R programmers tend to avoid \texttt{=} since it can easily lead to confusion with \texttt{==}. As such, you can strike this one out of your R vocabulary for now.

There are many different logical operations you can perform. Table \ref{tab:logical-operators-r} lists the most frequently used logical operators for your reference. These will become important once we select only certain parts of our data for analysis, e.g.~only \texttt{female} participants.

\begin{longtable}[]{@{}ll@{}}
\caption{\label{tab:logical-operators-r} Logical Operators in R}\tabularnewline
\toprule
Operator & Description \\
\midrule
\endfirsthead
\toprule
Operator & Description \\
\midrule
\endhead
== & is equal to \\
\textgreater= & is bigger or equal to \\
\textless= & is smaller of equal to \\
!= & is not equal to \\
a \textbar{} b & a or b \\
a \& b & a and b \\
!a & is not a \\
\bottomrule
\end{longtable}

\hypertarget{assigning-values-to-objects}{%
\section{Assigning values to objects: `\textless-'}\label{assigning-values-to-objects}}

Another common task you will perform is assigning values to an object. An object can be many different things:

\begin{itemize}
\item
  a dataset,
\item
  the results of a computation,
\item
  a plot,
\item
  a series of numbers,
\item
  a list of names,
\item
  a function,
\item
  etc.
\end{itemize}

In short, an object is an umbrella term for many different things which form part of your data analysis. For example, objects are handy when storing results that you want to process further in later analytical steps. Let's have a look at an example.

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# I have a friend called "Fiona"}
\NormalTok{friends }\OtherTok{\textless{}{-}} \StringTok{"Fiona"}
\end{Highlighting}
\end{Shaded}

In this example, I created an object called \texttt{friends} and added \texttt{"Fiona"} to it. Remember, because \texttt{"Fiona"} represents a \texttt{string}, we need \texttt{""}. So, if you wanted to read this line of code, you would say, `\texttt{friends} gets the value \texttt{"Fiona"}'. Alternatively, you could also say `\texttt{"Fiona"} is assigned to \texttt{friends}'.

If you look into your environment pane, you will find the object we just created. You can see it carries the value \texttt{"Fiona"}. We can also print values of an object in the console by simply typing the name of the object \texttt{friends} and hit \texttt{Return\ ‚Üµ}.

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# Who are my friends?}
\NormalTok{friends}
\DocumentationTok{\#\# [1] "Fiona"}
\end{Highlighting}
\end{Shaded}

Sadly, it seems I only have one friend. Luckily we can add some more, not the least to make me feel less lonely. To create objects with multiple values, we can use the function \texttt{c()}, which stands for `concatenate'. The \citet{concatenate-2021} define this word as follows:

\begin{quote}
`\textbf{\emph{concatenate}}',

to put things together as a connected series
\end{quote}

Let's concatenate some more friends into our \texttt{friends} object.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Adding some more friends to my life}
\NormalTok{friends }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Fiona"}\NormalTok{, }\StringTok{"Ida"}\NormalTok{, }\StringTok{"Lukas"}\NormalTok{, }\StringTok{"Georg"}\NormalTok{, }\StringTok{"Daniel"}\NormalTok{, }\StringTok{"Pavel"}\NormalTok{, }\StringTok{"Tigger"}\NormalTok{)}

\CommentTok{\# Here are all my friends}
\NormalTok{friends}
\DocumentationTok{\#\# [1] "Fiona"  "Ida"    "Lukas"  "Georg"  "Daniel" "Pavel"  "Tigger"}
\end{Highlighting}
\end{Shaded}

To concatenate values into a single object, we need to use a comma \texttt{,} to separate each value. Otherwise, R will report an error back.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{friends }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Fiona"} \StringTok{"Ida"}\NormalTok{)}
\DocumentationTok{\#\# Error: \textless{}text\textgreater{}:1:22: unexpected string constant}
\DocumentationTok{\#\# 1: friends \textless{}{-} c("Fiona" "Ida"}
\DocumentationTok{\#\#                          \^{}}
\end{Highlighting}
\end{Shaded}

R's error messages tend to be very useful and give meaningful clues to what went wrong. In this case, we can see that something `unexpected' happen, and it shows where our mistake is.

You can also concatenate numbers, and if you add \texttt{()} around it, you can automatically print the content of the object to the console. Thus, \texttt{(milestones\_of\_my\_life\ \textless{}-\ c(1982,\ 2006,\ 2011,\ 2018,\ 2020))} is the same as \texttt{milestones\_of\_my\_life\ \textless{}-\ c(1982,\ 2006,\ 2011,\ 2018,\ 2020)} followed by \texttt{milestones\_of\_my\_life}. The following examples illustrate this.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Important years in my life}
\NormalTok{milestones\_of\_my\_life }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1982}\NormalTok{, }\DecValTok{2006}\NormalTok{, }\DecValTok{2011}\NormalTok{, }\DecValTok{2018}\NormalTok{, }\DecValTok{2020}\NormalTok{)}
\NormalTok{milestones\_of\_my\_life}
\DocumentationTok{\#\# [1] 1982 2006 2011 2018 2020}

\CommentTok{\# The same as above, but we don\textquotesingle{}t need the second line of code}
\NormalTok{(milestones\_of\_my\_life }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1982}\NormalTok{, }\DecValTok{2006}\NormalTok{, }\DecValTok{2011}\NormalTok{, }\DecValTok{2018}\NormalTok{, }\DecValTok{2020}\NormalTok{))}
\DocumentationTok{\#\# [1] 1982 2006 2011 2018 2020}
\end{Highlighting}
\end{Shaded}

Finally, we can also concatenate numbers and character values into one object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(names\_and\_years }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Fiona"}\NormalTok{, }\DecValTok{1988}\NormalTok{, }\StringTok{"Daniel"}\NormalTok{, }\DecValTok{1982}\NormalTok{))}
\DocumentationTok{\#\# [1] "Fiona"  "1988"   "Daniel" "1982"}
\end{Highlighting}
\end{Shaded}

This last example is not necessarily something I would recommend to do, because it likely leads to undesirable outcomes. If you look into your environment pane you currently have three objects: \texttt{friends}, \texttt{milestones\_of\_my\_life}, and \texttt{names\_and\_years}.

\includegraphics{images/chapter_05_img/01_basic_computation_environment_objects.png}

The \texttt{friends} object shows that all the values inside the object are classified as \texttt{chr}, which denominates \texttt{character}. In this case, this is correct because it only includes the names of my friends. On the other hand, the object \texttt{milestones\_of\_my\_life} only includes \texttt{numeric} values, and therefore it says \texttt{num} in the environment pane. However, for the object \texttt{names\_and\_years} we know we want to have \texttt{numeric} and \texttt{character} values included. Still, R recognises them as \texttt{character} values only because values inside objects are meant to be of the same type.

Consequently, mixing different types of data (as explained in Chapter @ref()) into one object is likely a bad idea. This is especially true if you want to use the numeric values for computation. In short: ensure your objects are all of the same data type.

There is an exception to this rule. `Of course', you might say. There is one object that can have values of different types: \texttt{list}. As the name indicates, a \texttt{list} object holds several items. These items are usually other objects. In the spirit of `\href{https://www.imdb.com/title/tt1375666/?ref_=ext_shr_lnk}{Inception}', you can have lists inside lists, which contain more objects.

Let's create a list called \texttt{x\_files} using the \texttt{list} function and place all our objects inside.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This creates our list of objects}
\NormalTok{x\_files }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(friends,}
\NormalTok{               milestones\_of\_my\_life,}
\NormalTok{               names\_and\_years)}

\CommentTok{\# Let\textquotesingle{}s have a look what is hidden inside the x\_files}
\NormalTok{x\_files}
\DocumentationTok{\#\# [[1]]}
\DocumentationTok{\#\# [1] "Fiona"  "Ida"    "Lukas"  "Georg"  "Daniel" "Pavel"  "Tigger"}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# [[2]]}
\DocumentationTok{\#\# [1] 1982 2006 2011 2018 2020}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# [[3]]}
\DocumentationTok{\#\# [1] "Fiona"  "1988"   "Daniel" "1982"}
\end{Highlighting}
\end{Shaded}

You will notice in this example that I do not use \texttt{""} for each value in the list. This is because \texttt{friends} is not a character I put into the list, but an object. When we refer to objects, we do not need quotation marks.

We will encounter \texttt{list} objects quite frequently when we perform our analysis. Some functions return the results in the format of lists. This can be very helpful because otherwise our environment pane will be littered with objects. We would not necessarily know how they relate to each other, or worse, to which analysis they belong. Looking at the list item in the environment page (Figure \ref{fig:img-x-files}), you can see that the object \texttt{x\_files} is classified as a \texttt{List\ of\ 3,} and if you click on the blue icon, you can inspect the different objects inside.

\textbackslash begin\{figure\}

\{\centering \includegraphics[width=38.67in]{images/chapter_05_img/02_basic_computation_environment_lists}

\}

\textbackslash caption\{The environment pane showing our objects and our list \texttt{x\_files}\}\label{fig:img-x-files}
\textbackslash end\{figure\}

In Chapter \ref{basic-computations-in-r}, I mentioned that we should avoid using the \texttt{=} operator and explained that it is used to assign values to objects. You can, if you want, use \texttt{=} instead of \texttt{\textless{}-}. They fulfil the same purpose. However, as mentioned before, it is not wise to do so. Here is an example that shows that, in principle, it is possible.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# DO}
\NormalTok{(avengers1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Iron Man"}\NormalTok{, }\StringTok{"Captain America"}\NormalTok{, }\StringTok{"Black Widow"}\NormalTok{, }\StringTok{"Vision"}\NormalTok{))}
\DocumentationTok{\#\# [1] "Iron Man"        "Captain America" "Black Widow"     "Vision"}

\CommentTok{\# DON\textquotesingle{}T}
\NormalTok{(}\AttributeTok{avengers2 =} \FunctionTok{c}\NormalTok{(}\StringTok{"Iron Man"}\NormalTok{, }\StringTok{"Captain America"}\NormalTok{, }\StringTok{"Black Widow"}\NormalTok{, }\StringTok{"Vision"}\NormalTok{))}
\DocumentationTok{\#\# [1] "Iron Man"        "Captain America" "Black Widow"     "Vision"}
\end{Highlighting}
\end{Shaded}

On a final note, naming your objects is limited. You cannot chose any name. First, every name needs to start with a letter. Second, you can only use letters, numbers \texttt{\_} and \texttt{.} as valid components of the names for your objects \citep[see also][Chapter 4.2.]{wickham2016r}. I recommend to establish a naming convention that you adhere to. Personally I prefer to only user lower letters and \texttt{\_} to separate/connect words. You want to keep names informative, succinct and precise. Here are some examples of what some might consider good and bad choices for names.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Good choices}
\NormalTok{income\_per\_annum}
\NormalTok{open\_to\_exp          }\CommentTok{\# for \textquotesingle{}openness to new experiences\textquotesingle{}}
\NormalTok{soc\_int              }\CommentTok{\# for \textquotesingle{}social integration\textquotesingle{}}
 
\CommentTok{\# Bad choices}
\NormalTok{IncomePerAnnum}
\NormalTok{measurement\_of\_boredom\_of\_watching\_youtube}
\NormalTok{Sleep.per\_monthsIn.hours}
\end{Highlighting}
\end{Shaded}

Ultimately, you need to be able to effectively work with your data and output. Ideally, this should be true for others as well who want or need to work with your R project as well, e.g.~your co-investigator or supervisor. The same is true for your column names in datasets (see Chapter @ref()). Some more information about coding style (i.e.~the style of writing coding) can be found in Chapter \ref{coding-etiquette}.

\hypertarget{functions}{%
\section{Functions}\label{functions}}

I used the term `function' multiple times, but I never thoroughly explained what they are and why we need them. In simple terms, functions are objects. They contain lines of code that someone has written for us or we have written ourselves. One could say they are code snippets ready to use. Someone else might see them as shortcuts for our programming. Functions increase the speed with which we perform our analysis and write our computations and make our code more readable. Consider computing the \texttt{mean} of values stored in the object \texttt{pocket\_money}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First we create an object that stores our desired values}
\NormalTok{pocket\_money }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{13}\NormalTok{, }\DecValTok{21}\NormalTok{, }\DecValTok{34}\NormalTok{, }\DecValTok{55}\NormalTok{, }\DecValTok{89}\NormalTok{)}

\CommentTok{\#1 Manually compute the mean}
\NormalTok{sum }\OtherTok{\textless{}{-}} \DecValTok{0} \SpecialCharTok{+} \DecValTok{1} \SpecialCharTok{+} \DecValTok{1} \SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{+} \DecValTok{3} \SpecialCharTok{+} \DecValTok{5} \SpecialCharTok{+} \DecValTok{8} \SpecialCharTok{+} \DecValTok{13} \SpecialCharTok{+} \DecValTok{21} \SpecialCharTok{+} \DecValTok{34} \SpecialCharTok{+} \DecValTok{55} \SpecialCharTok{+} \DecValTok{89}
\NormalTok{sum }\SpecialCharTok{/} \DecValTok{12} \CommentTok{\# There are 12 items in the object}
\DocumentationTok{\#\# [1] 19.33333}

\CommentTok{\#2 Use a function to compute the mean}
\FunctionTok{mean}\NormalTok{(pocket\_money)}
\DocumentationTok{\#\# [1] 19.33333}

\CommentTok{\#3 Let\textquotesingle{}s make sure \#1 and \#2 are actually the same}
\NormalTok{sum }\SpecialCharTok{/} \DecValTok{12} \SpecialCharTok{==} \FunctionTok{mean}\NormalTok{(pocket\_money)}
\DocumentationTok{\#\# [1] TRUE}
\end{Highlighting}
\end{Shaded}

If we manually compute the mean, we first calculate the sum of all values in the object \texttt{pocket\_money}\footnote{If you find the order of numbers suspicious, it is because it represents the famous \href{https://en.wikipedia.org/wiki/Fibonacci_number}{Fibonacci sequence}.}. Then we divide it by the number of values in the object, which is \texttt{12}. This is the traditional way of computing the mean as we know it from primary school. However, by simply using the function \texttt{mean()}, we not only write considerably less code, but it is also much easier to understand as well because the word \texttt{mean} does precisely what we would expect. Which one do you find easier?

To further illustrate how functions look like, let's create one ourselves and call it \texttt{my\_mean}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_mean }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(numbers)\{}
\NormalTok{  sum }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(numbers)              }\CommentTok{\# Compute the sum of all values in \textquotesingle{}numbers\textquotesingle{}}
\NormalTok{  result }\OtherTok{\textless{}{-}}\NormalTok{ sum}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(numbers)    }\CommentTok{\# Divide the sum by the number of items in \textquotesingle{}numbers\textquotesingle{}}
  \FunctionTok{return}\NormalTok{(result)                   }\CommentTok{\# Return the result in the console}
\NormalTok{\}}

\FunctionTok{my\_mean}\NormalTok{(pocket\_money)}
\DocumentationTok{\#\# [1] 19.33333}
\end{Highlighting}
\end{Shaded}

Do not worry if half of this code does not make sense to you. Writing functions is an advanced R skill. However, it is good to know how functions look on the `inside'. You certainly can see the similarities between the code we have written before, but instead of using actual numbers, we work with placeholders like \texttt{numbers}. This way, we can use a function for different data and do not have to rewrite it every time.

All functions in R share the same structure. They have a \texttt{name} followed by \texttt{()}. Within these parentheses, we put \texttt{arguments}, which have specific \texttt{values}. For example, a function would look something like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{name\_of\_function}\NormalTok{(}\AttributeTok{argument\_1 =}\NormalTok{ value\_1,}
                 \AttributeTok{argument\_2 =}\NormalTok{ value\_2,}
                 \AttributeTok{argument\_3 =}\NormalTok{ value\_3)}
\end{Highlighting}
\end{Shaded}

How many arguments there are and what kind of values you can provide is very much dependent on the function you use. Thus, not every function takes every value. In the case of \texttt{mean()}, the function takes an object which holds a sequence of \texttt{numeric} values. It would make very little sense to compute the mean of our \texttt{friends} object, because it only contains names. R would return an error message:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(friends)}
\DocumentationTok{\#\# Warning in mean.default(friends): argument is not numeric or logical: returning}
\DocumentationTok{\#\# NA}
\DocumentationTok{\#\# [1] NA}
\end{Highlighting}
\end{Shaded}

\texttt{NA} refers to a value that is \emph{`not available'}. In this case, R tries to compute the mean, but the result is not available, because the values are not \texttt{numeric} but a \texttt{character}. In your dataset, you might find cells that are \texttt{NA}, which means there is data missing. Remember: If a function attempts a computation that includes even just a single value that is \texttt{NA}, R will return \texttt{NA}. However, there is a way to fix this. You will learn more about how to deal with \texttt{NA} values in Chapter @ref().

Sometimes you will also get a message from R that states \texttt{NaN}. \texttt{NaN} stands for \emph{`not a number'} and is returned when something is not possible to compute, for example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example 1}
\DecValTok{0}\SpecialCharTok{/}\DecValTok{0}
\DocumentationTok{\#\# [1] NaN}

\CommentTok{\# Example 2}
\FunctionTok{sqrt}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{9}\NormalTok{)}
\DocumentationTok{\#\# Warning in sqrt({-}9): NaNs produced}
\DocumentationTok{\#\# [1] NaN}
\end{Highlighting}
\end{Shaded}

\hypertarget{r-packages}{%
\section{R packages}\label{r-packages}}

R has many built-in functions that we can use right away. However, some of the most interesting ones are developed by different programmers, data scientists and enthusiasts. To add more functions to your repertoire, you can install R packages. R packages are a collection of functions that you can download and use for your own analysis. Throughout this book, you will learn about and use many different R packages to accomplish various tasks.

To give you another analogy,

\begin{itemize}
\item
  R is like a global supermarket,
\item
  RStudio is like my shopping cart,
\item
  and R packages are the products I can pick from the shelves.
\end{itemize}

Luckily, R packages are free to use, so I do not have to bring my credit card. For me, these additional functions, developed by some of the most outstanding scientists, is what keeps me addicted to performing my research in R.

R packages do not only include functions but often include datasets and documentation of what each function does. This way, you can easily try every function right away, even without your own dataset and read through what each function in the package does. Figure \ref{fig:img-r-package-documentation}

\begin{figure}

{\centering \includegraphics[width=28.08in]{images/chapter_05_img/r_package_documentation} 

}

\caption{The R package documentation for 'ggplot2'}\label{fig:img-r-package-documentation}
\end{figure}

However, how do you find those R packages? They are right at your fingertips. You have two options:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Use the function \texttt{install.packages()}
\item
  Use the packages pane in RStudio (see Chapter \ref{the-files-plots-packages-help-viewer-window})
\end{enumerate}

\hypertarget{installing-packages-using-a-function}{%
\subsection{\texorpdfstring{Installing packages using \texttt{install.packages()}}{Installing packages using install.packages()}}\label{installing-packages-using-a-function}}

The simplest and fastest way to install a package is calling the function \texttt{install.packages()}. You can either use it to install a single package or install a series of packages all at once using our trusty \texttt{c()} function. All you need to know is the name of the package. This approach works for all packages that are on CRAN (remember CRAN from Chapter \ref{installing-r}?).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install a single package}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}

\CommentTok{\# Install multiple packages at once}
\FunctionTok{install.packages}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{, }\StringTok{"naniar"}\NormalTok{, }\StringTok{"psych"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

If a package is not available from CRAN, chances are you can find them on \href{https://github.com}{GitHub}. GitHub is probably the world's largest global platform for programmers from all walks of life, and many of them develop fantastic R packages that make R programming not just easier but a lot more fun. As you continue to work in R, you should seriously consider creating your own account to keep backups of your R projects (see also Chapter \ref{next-steps-github}).

An essential companion for this book is \texttt{r4np}, which contains all datasets for this book and some useful functions to get you up and running in no time.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install the \textquotesingle{}r4np\textquotesingle{} pacakge from GitHub}
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"ddauber/r4np"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{installing-packages-via-rstudio}{%
\subsection{Installing packages via RStudio's package pane}\label{installing-packages-via-rstudio}}

RStudio offers a very convenient way of installing packages. In the packages pane, you cannot only see your installed packages, but you have two more buttons: \texttt{Install} and \texttt{Update}. The names are very self-explanatory. To install an R package you can follow the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Click on \texttt{Install}.
\item
  In most cases, you want to make sure you have \texttt{Repository\ (CRAN)} selected.

  \includegraphics{images/chapter_05_img/install_r_packages/01_install_r_packages.png}
\item
  Type in the name of the package you wish to install. RStudio offers an auto-complete feature to make it even easier to find the package you want.

  \includegraphics{images/chapter_05_img/install_r_packages/02_install_r_packages.png}
\item
  I recommend NOT to change the option which says \texttt{Install\ to\ library.} The default library settings will suffice.
\item
  Finally, I recommend to select \texttt{Install\ dependencies}, because some packages need other packages to function properly. This way, you do not have to do this manually.

  \includegraphics{images/chapter_05_img/install_r_packages/03_install_r_packages.png}
\end{enumerate}

The only real downside of using the packages pane is that you cannot install packages hosted on GitHub only. However, you can download them from there and install them directly from your computer using this option. This is particularly useful if you do not have an internet connection but you already downloaded the required packages onto a hard drive.

\hypertarget{using-r-packages}{%
\subsection{Using R Packages}\label{using-r-packages}}

Now that you have a nice collection of R packages, the next step would be to use them. While you only have to install R packages once, you have to `activate' them every time you start an new session in RStudio. This process is also called `loading an R package'. Once an R package is loaded, you can use all its functions. To load an R package, we have to use the function \texttt{library()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

The \texttt{tidyverse} package is a special kind of package. It contains multiple packages and loads them all at once. Almost all included packages (and more) you will use at some point when working through this book.

I know what you are thinking. Can you use \texttt{c()} to load all your packages at once? Unfortunately not. However, there is a way to do this, but it goes beyond the scope of this book to fully explain this (if you are curious, you can take a peek \href{https://stackoverflow.com/questions/8175912/load-multiple-packages-at-once}{here}).

Besides, it is not always advisable to load all functions of an entire package. One reason could be that two packages contain a function with the same name but with a different purpose. Two functions with the same name create a conflict between these two packages, and one of the functions would not be usable. Another reason could be that you only need to use the function once, and loading the whole package to use only one specific function seems excessive. Instead, you can explicitly call functions from packages without loading the package. For example, we might want to use the \texttt{vismis()} function from the \texttt{naniar} package to show where data is missing in our dataset \texttt{airquality}. Writing the code this way is also much quicker than loading the package and then calling the function if you don't use it repeatedly. Copy the code and try it yourself. Make sure you have \texttt{naniar} installed (see \protect\hyperlink{install-packages-tidyverse-nanair-psych}{above}). We will work with this package when we explore missing data in Chapter @ref().

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Here I use the dataset \textquotesingle{}airquality\textquotesingle{}, which comes with R}
\NormalTok{naniar}\SpecialCharTok{::}\FunctionTok{vis\_miss}\NormalTok{(airquality)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_for_non_programmers_files/figure-latex/Explicitly calling functions from R packages-1} \end{center}

\hypertarget{coding-etiquette}{%
\section{Coding etiquette}\label{coding-etiquette}}

Now you know everything to get started, but before we jump into our first project, I would like to briefly touch upon coding etiquette. This is not something that improves your analytical or coding skills directly, but is essential in building good habbits and making your life and those of others a little easier. Consider writing code like growing plants in your garden. You want to nurture the good plants, remove the weed and add labels that tell you which plant it is that you are growing. At the end of the day, you want your garden to be well-maintained. Treat you programming code the same way.

A script (see Chapter @ref()) with code should always have at least the following qualities:

\begin{itemize}
\item
  Only contains code that is necessary,
\item
  Is easy to read and understand,
\item
  Is self-contained.
\end{itemize}

With simple code this is easily achieved. However, what about more complex and longer code representing a whole set of analytical steps?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Very messy code}

\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(jtools)}
\NormalTok{model1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(covid\_cases\_per\_1m }\SpecialCharTok{\textasciitilde{}}\NormalTok{ idv, }\AttributeTok{data =}\NormalTok{ df)}
\FunctionTok{summ}\NormalTok{(model1, }\AttributeTok{scale =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{transform.response =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{vifs =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(covid\_cases\_per\_1m, idv, }\AttributeTok{colour =}\NormalTok{ europe, }\AttributeTok{label =}\NormalTok{ country))}\SpecialCharTok{+}
\FunctionTok{theme\_minimal}\NormalTok{()}\SpecialCharTok{+} \FunctionTok{geom\_label}\NormalTok{(}\AttributeTok{nudge\_y =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{()}
\NormalTok{mod\_model2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(cases\_per\_1m }\SpecialCharTok{\textasciitilde{}}\NormalTok{ idv }\SpecialCharTok{+}\NormalTok{ uai }\SpecialCharTok{+}\NormalTok{ idv}\SpecialCharTok{*}\NormalTok{europe }\SpecialCharTok{+}\NormalTok{ uai}\SpecialCharTok{*}\NormalTok{europe, }\AttributeTok{data =}\NormalTok{ df)}
\FunctionTok{summ}\NormalTok{(mod\_model2, }\AttributeTok{scale =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{transform.response =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{vifs =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{anova}\NormalTok{(mod\_model1, mod\_model2)}
\end{Highlighting}
\end{Shaded}

How about the following in comparison?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Nicely structured code}

\CommentTok{\# Load required R packages}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(jtools)}

\CommentTok{\# {-}{-}{-}{-} Modelling COVID{-}19 cases {-}{-}{-}{-}}

\DocumentationTok{\#\# Specify and run a regression}
\NormalTok{model1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(covid\_cases\_per\_1m }\SpecialCharTok{\textasciitilde{}}\NormalTok{ idv, }\AttributeTok{data =}\NormalTok{ df)}

\DocumentationTok{\#\# Retrieve the summary statistics of model1}
\FunctionTok{summ}\NormalTok{(model1, }\AttributeTok{scale =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{transform.response =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{vifs =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Does is matter whether a country lies in Europe?}

\DocumentationTok{\#\# Visualise rel. of covid cases, idv and being a European country}
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(covid\_cases\_per\_1m, idv, }\AttributeTok{colour =}\NormalTok{ europe, }\AttributeTok{label =}\NormalTok{ country))}\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{geom\_label}\NormalTok{(}\AttributeTok{nudge\_y =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}

\DocumentationTok{\#\# Specify and run a revised regression}
\NormalTok{mod\_model2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(cases\_per\_1m }\SpecialCharTok{\textasciitilde{}}\NormalTok{ idv }\SpecialCharTok{+}\NormalTok{ uai }\SpecialCharTok{+}\NormalTok{ idv}\SpecialCharTok{*}\NormalTok{europe }\SpecialCharTok{+}\NormalTok{ uai}\SpecialCharTok{*}\NormalTok{europe, }\AttributeTok{data =}\NormalTok{ df)}

\DocumentationTok{\#\# Retrieve the summary statistics of model2}
\FunctionTok{summ}\NormalTok{(mod\_model2, }\AttributeTok{scale =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{transform.response =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{vifs =} \ConstantTok{TRUE}\NormalTok{)}

\DocumentationTok{\#\# Test whether model2 is an improvement over model1}
\FunctionTok{anova}\NormalTok{(mod\_model1, mod\_model2)}
\end{Highlighting}
\end{Shaded}

I hope we can agree that the second example is much easier to read and understand even though you probably do not understand most of it yet. For once, I separated the different analytical steps from each other like paragraphs in a report. Apart from that, I added comments with \texttt{\#} to provide more context to my code for someone else who wants to understand my analysis. Admittedly, this example is a little excessive. Usually, you might have fewer comments. Commenting is an integral part of programming because it allows you to remember what you did. Ideally, you want to strike a good balance between commenting on and writing your code. How many comments you need will likely change throughout your R programming journey. Think of comments as headers for your programming script that give it structure..

We can use \texttt{\#} not only to write comments but also to tell R not to run particular code. This is very helpful if you want to keep some code but do not want to use it yet. There is also a handy keyboard shortcut you can use to `deactivate' multiple lines of code at once. Select whatever you want to `comment out' in your script and press \texttt{Ctrl+Shift+C} (PC) or \texttt{Cmd+Shift+C} (Mac).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# mean(pocket\_money) \# R will NOT run this code}
\FunctionTok{mean}\NormalTok{(pocket\_money)   }\CommentTok{\# R will run this code}
\end{Highlighting}
\end{Shaded}

RStudio helps a lot with keeping your coding tidy and properly formatted. However, there are some additional aspects worth considering. If you want to find out more about coding style, I highly recommend to read through the \href{https://style.tidyverse.org}{\emph{`The tidyverse style guide'}} \citep{wickham-2021}.\\

\hypertarget{exercises-chapter-5}{%
\section{Exercises}\label{exercises-chapter-5}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the result of \(\sqrt[2]{25-16}+2*8-6\)?
\item
  What does the console return if you execute the following code \texttt{"Five"\ ==\ 5}?
\item
  Create a list called \texttt{books} and include the following book titles in it:

  \begin{itemize}
  \item
    ``Harry Potter and the Deathly Hallows'',
  \item
    ``The Alchemist'',
  \item
    ``The Davinci Code'',
  \item
    ``R For Dummies''
  \end{itemize}
\item
  Copy and paste the function below into your RStudio console and run it. What does the function do when you use it?

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{x\_x }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(number1, number2)\{}
\NormalTok{  result1 }\OtherTok{\textless{}{-}}\NormalTok{ number1}\SpecialCharTok{*}\NormalTok{number2}
\NormalTok{  result2 }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(number1)}
\NormalTok{  result3 }\OtherTok{\textless{}{-}}\NormalTok{ number1}\SpecialCharTok{{-}}\NormalTok{number2}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{c}\NormalTok{(result1, result2, result3))}
\NormalTok{\} }
\end{Highlighting}
\end{Shaded}
\item
  What are the three steps to use a new R package that you found on CRAN?
\end{enumerate}

Check you answers: Solutions \ref{exercises-solutions-5}

\hypertarget{starting-your-r-projects}{%
\chapter{Starting your R projects}\label{starting-your-r-projects}}

Every project likely fills you with enthusiasm and excitement. And it should. You are about to find answers to your questions, and you hopefully come out more knowledgeable due to it. However, there are likely certain aspects of data analysis that you find less enjoyable. I can think of two:

\begin{itemize}
\item
  Keeping track of all the files my project generates
\item
  Data wrangling
\end{itemize}

While we cover data wrangling in great detail later (Chapter \ref{data-wrangling}), I would like to share some insights from my work that helped me stay organised and, consequently, less frustrated. The following applies to small and large research projects, which makes it very convenient no matter the situation. Of course, feel free to tweak my approach to whatever suits you. However, consistency is king.

\hypertarget{creating-an-r-project}{%
\section{Creating an R Project file}\label{creating-an-r-project}}

When working on a project, you likely create many different files for various purposes, especially R Scripts (see Chapter \ref{creating-an-r-script}). If you are not careful, this file is stored in your system's default location, which might not be where you want them to be. RStudio allows you to manage your entire project intuitively and conveniently through R Project files. Using R Project files comes with a couple of perks, for example:

\begin{itemize}
\item
  All the files that you generate are in the same place. Your data, your coding, your exported plots, your reports, etc., all are in one place together without you having to manage the files manually.
\item
  If you want to share your project, you can share the entire folder, and others can quickly reproduce your research or help fix problems. This is because all file paths are relative and not absolute.
\item
  You can, more easily, use GitHub for backups and so-called `version control', which allows you to track changes you have made to your code over time (see also Chapter \ref{next-steps-github}).
\end{itemize}

For now, the most important reason to use R Project files is the convenience of the organisation of files and the ability to share it easily with co-investigators, your supervisor, or your students.

To create an R Project, you need to perform the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Select \texttt{File\ \textgreater{}\ New\ Project\ldots{}} from the menu bar.

  \includegraphics{images/chapter_06_img/00_r_project/00_r_project_file_menu.png}
\item
  Select \texttt{New\ Directory} from the popup window.

  \includegraphics{images/chapter_06_img/00_r_project/01_r_project_new_directory.png}
\item
  Next, select \texttt{New\ Project}.

  \includegraphics{images/chapter_06_img/00_r_project/02_r_project_new_project.png}
\item
  Pick a meaningful name for your project folder, i.e.~the \texttt{Directory\ Name}. Ensure this project folder is created in the right place. You can change the \texttt{subdirectory} by clicking on \texttt{Browse\ldots{}}. Ideally the subdirectory is a place where you usually store your research projects.

  \includegraphics{images/chapter_06_img/00_r_project/03_r_project_specs.png}
\item
  You have the option to \texttt{Create\ a\ git\ repository}. This is only relevant if you already have a GitHub account and wish to use version control. For now, you can happily ignore it.
\item
  Lastly, tick \texttt{Open\ in\ new\ session}. This will open your R Project in a new RStudio window.

  \includegraphics{images/chapter_06_img/00_r_project/04_r_project_directory_name.png}
\item
  Once you are happy with your choices, you can click \texttt{Create\ Project}. This will open a new R Session, and you can start working on your project.

  \includegraphics{images/chapter_06_img/00_r_project/05_r_project_new_session.png}
\end{enumerate}

If you look carefully, you can see that your RStudio is now `branded' with your project name. At the top of the window, you see the project name, the files pane shows the root directory where all your files will be, and even the console shows on top the file path of your project. You could set all this up manually, but I would not recommend it, not the least because it is easy to work with R Projects.

\hypertarget{organising-your-projects}{%
\section{Organising your projects}\label{organising-your-projects}}

This section is not directly related to RStudio, R or data analysis in general. Instead, I want to convey to you that a good folder structure can go a long way. It is an excellent habit to start thinking about folder structures before you start working on your project. Placing your files into dedicated folders, rather than keeping them loosely in one container, will speed up your work and save you from the frustration of not finding the files you need. I have a template that I use regularly. You can either create it from scratch in RStudio or open your file browser and create the folders there. RStudio does not mind which way you do it. If you want to spend less time setting this up, you might want to use the function \texttt{create\_dr()} from the \texttt{r4np} package. It creates all the folders as shown in Figure \ref{fig:folder-structure}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install \textquotesingle{}r4np\textquotesingle{} from GitHub}
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"ddauber/r4np"}\NormalTok{)}

\CommentTok{\# Create the template structure}
\NormalTok{r4np}\SpecialCharTok{::}\FunctionTok{create\_dr}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

To create a folder, click on \texttt{New\ Folder} in the Files pane. I usually have at least the following folders for every project I am involved in:

\begin{itemize}
\item
  A folder for my raw data. I store `untouched' datasets in it. With `untouched', I mean they have not been processed in any way and are usually files I downloaded from my data collection tool, e.g.~online questionnaire platform.
\item
  A folder with `tidy' data. This is usually data I exported from R after cleaning it, i.e.~after data wrangling (see Chapter \ref{data-wrangling}).
\item
  A folder for my R scripts
\item
  A folder for my plots
\item
  A folder for reports
\end{itemize}

Thus, in RStudio, it would look something like this:

\begin{figure}

{\centering \includegraphics[width=38.67in]{images/chapter_06_img/01_organising_work/00_organising_work} 

}

\caption{An example of a scalable folder structure for your project}\label{fig:folder-structure}
\end{figure}

You probably noticed that my folders have numbers in front of them. I do this to ensure that all folders are in the order I want them to be, usually not the alphabetical order my computer suggests. I use two digits because I may have more than nine folders for a project, and folder ten would otherwise be listed as the third folder in this list. With this filing strategy in place, it will be easy to find whatever I need. Even others can easily understand what I stored where. It is simply `tidy', similar to how we want our data to be.

\hypertarget{creating-an-r-script}{%
\section{Creating an R Script}\label{creating-an-r-script}}

Code quickly becomes long and complex. Thus, it is not very convenient to write it in the console. So, instead, we can write code into an R Script. An R Script is a document that RStudio recognises as R programming code. Files that are not R Scripts, like \texttt{.txt}, \texttt{.rtf} or \texttt{.md}, can also be opened in RStudio, but any code written in it will not be automatically recognised.

When opening an R script or creating a new one, it will display in the source window (see Chapter \ref{the-source-window}). Some refer to this window as the `script editor'. An R Script starts as an empty file. Good coding etiquette (see Chapter \ref{coding-etiquette} demands that we use the first line to indicate what this file does by using a comment \texttt{\#}. Here is an example for our `TidyTuesday' R Project.

\includegraphics{images/chapter_06_img/02_r_script/00_r_script.png}

All examples in this book can easily be copied and pasted into your own R Script. However, for some code you will have to install the R package \texttt{r4np} (see \protect\hyperlink{install_r4np}{above}). Let's try it with the following code. The plot this code creates reveals which car manufacturer produces the most efficient cars.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{mpg }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(manufacturer, }\FunctionTok{desc}\NormalTok{(hwy), }\AttributeTok{FUN =}\NormalTok{ median),}
                   \AttributeTok{y =}\NormalTok{ hwy,}
                   \AttributeTok{fill =}\NormalTok{ manufacturer)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Manufacturer"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Highway miles per gallon"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_for_non_programmers_files/figure-latex/R Script copy and paste examples-1} \end{center}

You are probably wondering where your plot has gone. Copying the code will not automatically run it in your R Script. However, this is necessary to create the plot. If you tried pressing \texttt{Return\ ‚Üµ}, you would only add a new line. Instead, you need to select the code you want to run and press \texttt{Ctrl+Return\ ‚Üµ} (PC) or \texttt{Cmd+Return\ ‚Üµ} (Mac). You can also use the \texttt{Run} command at the top of your source window, but it is much more efficient to press the keyboard shortcut. Besides, you will remember this shortcut quickly, because we need to use it very frequently. If all worked out, you should see the following:

\includegraphics{images/chapter_06_img/02_r_script/01_r_script_example_plot.png}

As you can see, cars from Honda appear to drive furthest with the same amount of fuel (a gallon) compared to other vehicles. Thus, if you are looking for a very economical car, you now know where to find them.

The R Script editor has some conveniences for writing your code that are worth pointing out. You probably noticed that some of the code we have pasted is blue, and some other code is in green. These colours help to make your code more readable because they carry a specific meaning. In the default settings, green stands for any values in \texttt{""}, which usually stands for \texttt{character}s. This is also called `syntax highlighting'.

Moreover, code in R Scripts will be automatically indented to facilitate reading. If for whatever reason, the indentation does not happen, or you accidentally undo it, you can reindent a line with \texttt{Ctrl+I} (PC) or \texttt{Cmd+I} (Mac).

Lastly, the console and the R Script editor both feature code completion. This means that when you start typing a the name of function, R will provide suggestions. These are extremely helpful and make programming a lot faster. Once you found the function you were looking for, you press \texttt{Return\ ‚Üµ} to insert it. Here is an example of what happens when you have the package \texttt{tidyverse} loaded and type \texttt{ggpl}. Only functions that are loaded via packages or any object in your environment pane benefit from code completion.

\includegraphics{images/chapter_06_img/02_r_script/02_r script_code_completion.png}

Not only does RStudio show you all the available options, but it also tells you which package this function is from. In this case, all listed functions are from the \texttt{ggplot2} package. Furthermore, when you select one of the options but have not pressed \texttt{Return\ ‚Üµ} yet, you also get to see a yellow box, which provides you with a quick reference of all the arguments that this function accepts. So you do not have to memorise all the functions and their arguments.

\hypertarget{r-markdown-and-r-notebooks}{%
\section{Using R Markdown}\label{r-markdown-and-r-notebooks}}

There is too much to say about R Markdown, which is why I only will highlight that they exist and point out the one feature that might convince you to choose these formats over plain R Scripts: They look like a Word document (almost).

As the name indicates, R Markdown files are a combination of R Scripts and Markdown. Markdown is a way of writing and formatting text documents without needing software like MS Word. Instead, you write everything in plain text. Such plain text can be converted into many different document types such as HTML websites, PDF or Word documents. If you would like to see how it works, I recommend looking at the~\href{https://www.rstudio.com/resources/cheatsheets/}{R Markdown Cheatsheet}.

An R Markdown file works oppositely to an R Script. By default, an R Script considers everything as code and only through commenting \texttt{\#} we can include text to describe what the code does. This is what you have seen in all the coding examples so far. On the other hand, an R Markdown file considers everything as text, and we have to specify what is code. We can do so by inserting `code chunks'. Therefore, there is less of a need to use comments \texttt{\#} in R Markdown files because you can write about it. Another convenience of R Markdown files is that results from your analysis are immediately shown underneath the code chunk.

\includegraphics{images/chapter_06_img/03_r_markdown/01_r_markdown_plain.png}

If you switch your view to the \texttt{Visual\ Editor}, it almost looks like you are writing a report in MS Word.

\includegraphics{images/chapter_06_img/03_r_markdown/02_r_markdown_visual_editor_menu.png}

\includegraphics{images/chapter_06_img/03_r_markdown/03_r_markdown_visual_editor.png}

So, when should you use an R Script, and when should you use R Markdown. The rule-of-thumb is that if you intend to write a report, thesis or another form of publication, it might be better to work in an R Markdown file. If this does not apply, you might want to write an R Script. As mentioned above, R Markdown files emphasise text, while R Scripts primarily focus on code. In my projects, I often have a mixture of both. I use R Scripts to carry out data wrangling and my primary analysis and then use R Markdown files to present the findings, e.g.~creating plots, tables, etc. By the way, this book is written in R Markdown using the \texttt{bookdown} package.

No matter your choice, it will neither benefit nor disadvantage you in your R journey or when working through this book. The choice is all yours. You likely will come to appreciate both formats for what they offer.

\hypertarget{data-wrangling}{%
\chapter{Data Wrangling}\label{data-wrangling}}

You collected your data over months (and sometimes years), and all you want to know is whether your data makes sense and reveals something nobody would have ever expected. However, before we can truly go ahead with our analysis, it is essential to understand whether our data is `tidy'. Very often, the data we receive is everything else but clean, and we need to check whether our data is fit for analysis and ensure it is in a format that is easy to handle. For small datasets, this is usually a brief exercise. However, I found myself cleaning data for a month because the dataset was spread out into multiple spreadsheets (no pun intended) with different numbers of columns and odd column names. Thus, data cleaning or data wrangling is an essential first step in any data analysis. It is a step that cannot be skipped and has to be performed on every new dataset.

Luckily, R provides many useful functions to make our lives easier. You will be in for a treat if you are like me and used to do this in Excel. It is a lot simpler using R to achieve a clean dataset.

Here is an overview of the different steps we usually work through before starting with our primary analysis. This list is certainly not exhaustive:

\begin{itemize}
\item
  Importing data
\item
  Checking data types
\item
  Recoding and arranging factors, i.e.~categorical data.
\item
  Running missing data diagnostics
\item
  and other things
\end{itemize}

\hypertarget{import-your-data}{%
\section{Import your data}\label{import-your-data}}

The \texttt{r4np} package hosts several different datasets to work with, but at some point, you might want to apply your R knowledge to your own data. Therefore, an essential first step is to import your data into RStudio. There are three different methods, all of which are very handy:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Click on your data file in the Files pane and choose \texttt{Import\ Dataset}.
\item
  Use the \texttt{Import\ Dataset} button in the Environment pane.
\item
  Import your data calling one of the \texttt{readr} functions in the console or RScript.
\end{enumerate}

We will use the \texttt{readr} package to import our data. Using this package we can import a range of different file formats, including \texttt{.csv}, \texttt{.tsv}, \texttt{.txt}. If you want to import data from an \texttt{.xlsx} file, you need to use another package called \texttt{readxl}. The following sections will primarily focus on using \texttt{readr} via RStudio or directly in your Console or RScript.

\hypertarget{import-data-from-the-files-pane}{%
\subsection{Import data from the Files pane}\label{import-data-from-the-files-pane}}

This approach is by far the easiest. Let's assume you have a dataset called \texttt{gender\_age.csv} in your \texttt{00\_raw\_data\ folder}. If you wish to import it, you can do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Click on the name of the file
\item
  Select \texttt{Import\ Dataset}.

  \includegraphics{images/chapter_07_img/01_files_pane_import/01_files_pane_import.png}
\item
  A new window will open, and you can choose different options. You also see a little preview of how the data looks like. This is great if you are not sure whether you did it correctly.

  \includegraphics{images/chapter_07_img/01_files_pane_import/02_files_pane_import.png}
\item
  You can change how data should be imported, but the default should be fine in most cases. Here is a quick breakdown of the most important options:

  \begin{itemize}
  \item
    \texttt{Name} allows you to change the object name, i.e.~the name of the object this data will be assigned to. I often use \texttt{df\_raw} (\texttt{df} stand for \texttt{data\ frame}, which is how R calls such rectangular datasets).
  \item
    \texttt{Skip} is helpful if your data file starts with several empty rows at the top. You can remove them here.
  \item
    \texttt{First\ Row\ as\ Names} is ticked by default. In most Social Science projects, we tend to have the name of the variables as the first row in your dataset.
  \item
    \texttt{Trim\ Spaces} removes any unnecessary whitespace in your dataset. Leave it ticked.
  \item
    \texttt{Open\ Data\ Viewer} allows you to look at your imported dataset. I use it rarely, but it can be helpful at times.
  \item
    \texttt{Delimiter} defines how your columns are separate from each other in your file. If it is a \texttt{.csv} it would imply it is a `comma-separated value', i.e.~\texttt{,}. This setting can be changed for different files, depending on how your data is delimited. You can even use the option \texttt{Other\ldots{}} to specify a custom separation option.
  \item
    \texttt{NA} specifies how missing values in your data are acknowledged. By default, empty cells in your data will be recognised as missing data.
  \end{itemize}

  \includegraphics{images/chapter_07_img/01_files_pane_import/03_files_pane_import.png}
\item
  Once you are happy with your choices, you can click on \texttt{Import}.
\item
  You will find your dataset in the Environment pane.

  \includegraphics{images/chapter_07_img/01_files_pane_import/04_files_pane_import.png}
\end{enumerate}

In the Console, you can see that R also provides the \texttt{Column\ specification}, which we need later when inspecting `data types'. \texttt{readr} automatically imports all text-based columns as \texttt{chr}, i.e.~\texttt{character} values. However, this might not always be true. We will cover more of this aspect of data wrangling in Chapter \ref{change-data-types}.

\hypertarget{importing-data-from-the-environment-pane}{%
\subsection{Importing data from the Environment pane}\label{importing-data-from-the-environment-pane}}

The process of importing datasets from the Environment pane follows largely the one from the Files pane. Click on \texttt{Import\ Dataset\ \textgreater{}\ From\ Text\ (readr)\ldots{}}. The only main difference lies in having to find the file using the \texttt{Browse\ldots{}} button. The rest of the steps are the same as above.

You will have to use the Environment pane for importing data from specific file types, e.g.~\texttt{.txt}, because using the File pane would only open the file but not import the data for further processing.

\hypertarget{importing-data-using-functions}{%
\subsection{Importing data using functions directly}\label{importing-data-using-functions}}

If you organised your files well, it could be effortless and quick to use all the functions from \texttt{readr} directly. Here are two examples of how you can use \texttt{readr} to import your data. Make sure you have the \texttt{tidyverse} package loaded.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Import data from \textquotesingle{}.csv\textquotesingle{}}
\FunctionTok{read\_csv}\NormalTok{(}\StringTok{"00\_raw\_data/gender\_age.csv"}\NormalTok{)}

\CommentTok{\# Import data from any file text file by defining the separator yourself}
\FunctionTok{read\_delim}\NormalTok{(}\StringTok{"00\_raw\_data/gender\_age.txt"}\NormalTok{, }\AttributeTok{delim =} \StringTok{"|"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You might be wondering whether you can use \texttt{read\_delim()} to import \texttt{.csv} files too. The answer is `Yes, you can!'. In contrast to \texttt{read\_delim()}, \texttt{read\_csv()} sets the delimiter to \texttt{,} by default. This is mainly for convenience because \texttt{.csv} files are one of the most popular file formats used to store their data.

You might also be wondering what a `delimiter' is. When you record data in a plain-text file, it is easy to see where a new observation starts and ends because it is defined by a row in your file. However, we also need to tell our software where a new column starts, i.e.~where a cell begins and ends. Consider the following example. We have a file that holds our data which looks like this:

\begin{verbatim}
idagegender
124male
256female
333male
\end{verbatim}

The first row we probably can still decipher as \texttt{id}, \texttt{age}, \texttt{gender}. However, the next row makes it difficult to understand which value represents the \texttt{id} of a participant and which value reflects the \texttt{age} of that participant. Like us, computer software would find it hard too to decide on this ambiguous content. Thus, we need to use delimiters to make it very clear which value belongs to which column. For example, In a \texttt{.csv} file, the data would be separated by a \texttt{,}.

\begin{verbatim}
id,age,gender
1,24,male
2,56,female
3,33,male
\end{verbatim}

Considering our example from above, we could also use \texttt{\textbar{}} as a delimiter.

\begin{verbatim}
id|age|gender
1|24|male
2|56|female
3|33|male
\end{verbatim}

There is a lot more to \texttt{readr} than could be covered in this book. If you want to know more about this R package, I highly recommend looking at the \href{https://readr.tidyverse.org}{readr webpage}.

\hypertarget{inspecting-raw-data}{%
\section{Inspecting your data}\label{inspecting-raw-data}}

For the rest of this chapter, we will use the \texttt{wvs} dataset from the \texttt{r4np} package. However, we do not know much about this dataset, and therefore we cannot ask any research questions worth investigating. Therefore we need to look at what it contains. The first method of inspecting a dataset is to type the name of the object, i.e.~\texttt{wvs}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Ensure you loaded the \textquotesingle{}r4np\textquotesingle{} package first}
\FunctionTok{library}\NormalTok{(r4np)}

\CommentTok{\# Show the data in the console}
\NormalTok{wvs}
\DocumentationTok{\#\# \# A tibble: 69,578 x 7}
\DocumentationTok{\#\#    \textasciigrave{}Participant ID\textasciigrave{} \textasciigrave{}Country name\textasciigrave{} Gender   Age relationship\_status       }
\DocumentationTok{\#\#               \textless{}dbl\textgreater{} \textless{}chr\textgreater{}           \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}chr\textgreater{}                     }
\DocumentationTok{\#\#  1         20070001 Andorra             1    60 married                   }
\DocumentationTok{\#\#  2         20070002 Andorra             0    47 living together as married}
\DocumentationTok{\#\#  3         20070003 Andorra             0    48 separated                 }
\DocumentationTok{\#\#  4         20070004 Andorra             1    62 living together as married}
\DocumentationTok{\#\#  5         20070005 Andorra             0    49 living together as married}
\DocumentationTok{\#\#  6         20070006 Andorra             1    51 married                   }
\DocumentationTok{\#\#  7         20070007 Andorra             1    33 married                   }
\DocumentationTok{\#\#  8         20070008 Andorra             0    55 widowed                   }
\DocumentationTok{\#\#  9         20070009 Andorra             1    40 single                    }
\DocumentationTok{\#\# 10         20070010 Andorra             1    38 living together as married}
\DocumentationTok{\#\# \# ... with 69,568 more rows, and 2 more variables: Freedom.of.Choice \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   Satisfaction{-}with{-}life \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

The result is a series of rows and columns. The first information we receive is: \texttt{A\ tibble:\ 69,578\ x\ 9}. This indicates that our dataset has 69,578 observations (i.e.~rows) and 9 columns (i.e.~variables). This rectangular format is the one we encounter most frequently in Social Sciences (and probably beyond). If you ever worked in Microsoft Excel, this format will look familiar.

Even though it might be nice to look at a dataset in this way, it is not particularly useful. Depending on your monitor size, you might only see a small number of columns, and therefore we do not get to see a complete list of all variables. In short, we hardly ever will find much use in inspecting data this way. Luckily other functions can help us.

If you want to see each variable covered in the dataset and their data types, you can use the function \texttt{glimpse()} from the \texttt{dplyr} package (loaded as part of the \texttt{tidyverse} package).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(wvs)}
\DocumentationTok{\#\# Rows: 69,578}
\DocumentationTok{\#\# Columns: 7}
\DocumentationTok{\#\# $ \textasciigrave{}Participant ID\textasciigrave{}         \textless{}dbl\textgreater{} 20070001, 20070002, 20070003, 20070004, 20070\textasciitilde{}}
\DocumentationTok{\#\# $ \textasciigrave{}Country name\textasciigrave{}           \textless{}chr\textgreater{} "Andorra", "Andorra", "Andorra", "Andorra", "\textasciitilde{}}
\DocumentationTok{\#\# $ Gender                   \textless{}dbl\textgreater{} 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, \textasciitilde{}}
\DocumentationTok{\#\# $ Age                      \textless{}dbl\textgreater{} 60, 47, 48, 62, 49, 51, 33, 55, 40, 38, 54, 3\textasciitilde{}}
\DocumentationTok{\#\# $ relationship\_status      \textless{}chr\textgreater{} "married", "living together as married", "sep\textasciitilde{}}
\DocumentationTok{\#\# $ Freedom.of.Choice        \textless{}dbl\textgreater{} 10, 9, 9, 9, 8, 10, 10, 8, 8, 10, 9, 8, 10, 7\textasciitilde{}}
\DocumentationTok{\#\# $ \textasciigrave{}Satisfaction{-}with{-}life\textasciigrave{} \textless{}dbl\textgreater{} 10, 9, 9, 8, 7, 10, 5, 8, 8, 10, 8, 8, 10, 7,\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

The output of glimpse shows us the name of each column/variable after the \texttt{\$}, for example, \texttt{\textasciigrave{}Participant\ ID\textasciigrave{}}. The \texttt{\$} is used to lookup certain variables in our dataset. For example, if we want to inspect the column \texttt{relationship\_status} only, we could write the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wvs}\SpecialCharTok{$}\NormalTok{relationship\_status}
\DocumentationTok{\#\#     [1] "married"                    "living together as married"}
\DocumentationTok{\#\#     [3] "separated"                  "living together as married"}
\DocumentationTok{\#\#     [5] "living together as married" "married"                   }
\DocumentationTok{\#\#     [7] "married"                    "widowed"                   }
\NormalTok{....}
\end{Highlighting}
\end{Shaded}

After the variable name, we find the recognised datatype for each column in \texttt{\textless{}...\textgreater{}}, for example \texttt{\textless{}chr\textgreater{}}. We will return to data types in Chapter \ref{change-data-types}. Lastly, we get samples of the data included. This output is much more helpful.

I use \texttt{glimpse()} very frequently for different purposes, for example:

\begin{itemize}
\item
  to understand what variables are included in a dataset,
\item
  to check the correctness of data types,
\item
  to inspect variable names for typos or unconventional names,
\item
  to look up variable names.
\end{itemize}

There is one more way to inspect your data and receive more information about it by using a specialised R package. The \texttt{skimr} package is excellent in `skimming' your dataset. It provides not only information about variable names and data types but also provides some descriptive statistics. If you installed the \texttt{r4np} package and called the function \texttt{install\_r4np()}, you will have \texttt{skimr} installed already.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{skimr}\SpecialCharTok{::}\FunctionTok{skim}\NormalTok{(wvs)}
\end{Highlighting}
\end{Shaded}

The output in the Console should look like this:

\includegraphics{images/chapter_07_img/02_skimr_output/skimr_output.png}

As you can tell, there is a lot more information in this output. Many descriptive statistics that could be useful are already displayed. \texttt{skim()} provides a summary of the dataset and then automatically sorts the variables by data type. Depending on the data type, you also receive different descriptive statistics. As a bonus, the function also provides a histogram for numeric variables. However, there is one main problem: Some of the numeric variables are not numeric: \texttt{Participant\ ID} and \texttt{Gender}. Thus, we will have to correct the data types in a moment.

Inspecting your data in this way can be helpful to get a better understanding of what your data includes and spot problems with it. In addition, if you receive data from someone else, these methods are an excellent way to familiarise yourself with the dataset relatively quickly. Since I prepared this particular dataset for this book, I also made sure to provide documentation for it. You can access it by using \texttt{?wvs} in the Console. This will open the documentation in the Help pane. Such documentation is available for every dataset we use in this book.

\hypertarget{colnames-cleaning}{%
\section{\texorpdfstring{Cleaning your column names: Call the \texttt{janitor}}{Cleaning your column names: Call the janitor}}\label{colnames-cleaning}}

If you have an eagle eye, you might have noticed that most of the variable names in \texttt{wvs} are not consistent or easy to read/use.

\hypertarget{messy_column_names}{%
\label{messy_column_names}}%
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Whitespace and inconsistent capitalisation}
\NormalTok{Participant ID        }
\NormalTok{Country name          }
\NormalTok{Gender                }
\NormalTok{Age                   }

\CommentTok{\# Difficult to read}
\NormalTok{YearOfBirth           }
\NormalTok{Freedom.of.Choice     }
\NormalTok{Satisfaction}\SpecialCharTok{{-}}\NormalTok{with}\SpecialCharTok{{-}}\NormalTok{life}
\end{Highlighting}
\end{Shaded}

From Chapter \ref{coding-etiquette}, you will remember that being consistent in writing your code and naming your objects is essential. The same applies, of course, to variable names. R will not break using the existing names, but it will save you a lot of frustration if we take a minute to clean the names and make them more consistent.

You are probably thinking: ``This is easy. I just open the dataset in Excel and change all the column names.'' Indeed, it would be a viable and easy option, but it is not very efficient, especially with larger datasets with many more variables. Instead, we can make use of the \texttt{janitor} package. By definition, \texttt{janitor} is a package that helps to clean up whatever needs cleaning. In our case, we want to tidy our column names. We can use the function \texttt{clean\_names()} to achieve this. We store the result in a new object called \texttt{wvs} to keep those changes. The object will also show up in our Environment pane.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wvs }\OtherTok{\textless{}{-}}\NormalTok{ janitor}\SpecialCharTok{::}\FunctionTok{clean\_names}\NormalTok{(wvs)}
\FunctionTok{glimpse}\NormalTok{(wvs)}
\DocumentationTok{\#\# Rows: 69,578}
\DocumentationTok{\#\# Columns: 7}
\DocumentationTok{\#\# $ participant\_id         \textless{}dbl\textgreater{} 20070001, 20070002, 20070003, 20070004, 2007000\textasciitilde{}}
\DocumentationTok{\#\# $ country\_name           \textless{}chr\textgreater{} "Andorra", "Andorra", "Andorra", "Andorra", "An\textasciitilde{}}
\DocumentationTok{\#\# $ gender                 \textless{}dbl\textgreater{} 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\textasciitilde{}}
\DocumentationTok{\#\# $ age                    \textless{}dbl\textgreater{} 60, 47, 48, 62, 49, 51, 33, 55, 40, 38, 54, 39,\textasciitilde{}}
\DocumentationTok{\#\# $ relationship\_status    \textless{}chr\textgreater{} "married", "living together as married", "separ\textasciitilde{}}
\DocumentationTok{\#\# $ freedom\_of\_choice      \textless{}dbl\textgreater{} 10, 9, 9, 9, 8, 10, 10, 8, 8, 10, 9, 8, 10, 7, \textasciitilde{}}
\DocumentationTok{\#\# $ satisfaction\_with\_life \textless{}dbl\textgreater{} 10, 9, 9, 8, 7, 10, 5, 8, 8, 10, 8, 8, 10, 7, 1\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

Now that \texttt{janitor} has done its magic, we suddenly have easy to read variable names that are consistent with the `Tidyverse style guide' \citep{wickham-2021}.

If for whatever reason, the variable names are still not looking the way you want, you can use the function \texttt{rename()} from the \texttt{dplyr} package to manually assign new variable names.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wvs }\OtherTok{\textless{}{-}}\NormalTok{ wvs }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{rename}\NormalTok{(}\AttributeTok{satisfaction =}\NormalTok{ satisfaction\_with\_life,}
                      \AttributeTok{country =}\NormalTok{ country\_name)}
\FunctionTok{glimpse}\NormalTok{(wvs)}
\DocumentationTok{\#\# Rows: 69,578}
\DocumentationTok{\#\# Columns: 7}
\DocumentationTok{\#\# $ participant\_id      \textless{}dbl\textgreater{} 20070001, 20070002, 20070003, 20070004, 20070005, \textasciitilde{}}
\DocumentationTok{\#\# $ country             \textless{}chr\textgreater{} "Andorra", "Andorra", "Andorra", "Andorra", "Andor\textasciitilde{}}
\DocumentationTok{\#\# $ gender              \textless{}dbl\textgreater{} 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,\textasciitilde{}}
\DocumentationTok{\#\# $ age                 \textless{}dbl\textgreater{} 60, 47, 48, 62, 49, 51, 33, 55, 40, 38, 54, 39, 44\textasciitilde{}}
\DocumentationTok{\#\# $ relationship\_status \textless{}chr\textgreater{} "married", "living together as married", "separate\textasciitilde{}}
\DocumentationTok{\#\# $ freedom\_of\_choice   \textless{}dbl\textgreater{} 10, 9, 9, 9, 8, 10, 10, 8, 8, 10, 9, 8, 10, 7, 10,\textasciitilde{}}
\DocumentationTok{\#\# $ satisfaction        \textless{}dbl\textgreater{} 10, 9, 9, 8, 7, 10, 5, 8, 8, 10, 8, 8, 10, 7, 10, \textasciitilde{}}
\end{Highlighting}
\end{Shaded}

You are probably wondering what \texttt{\%\textgreater{}\%} stands for. This symbol is called a \emph{`piping operator'} , and it allows us to chain multiple functions together by considering the output of the previous function. So, do not confuse \texttt{\textless{}-} with \texttt{\%\textgreater{}\%}. Each operator serves a different purpose. The \texttt{\%\textgreater{}\%} has become synonymous with the \texttt{tidyverse} approach to R programming and is the chosen approach for this book. Many functions from the \texttt{tidyverse} are designed to be chained together.

If we wanted to spell out what we just did, we could say:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{wvs\ \textless{}-}: We assigned whatever happened to the right of the assignment operator to the object \texttt{wvs}.
\item
  \texttt{wvs\ \%\textgreater{}\%}: We defined the dataset we want to use with the functions defined after the \texttt{\%\textgreater{}\%}.
\item
  \texttt{rename(satisfaction\ =\ satisfcation\_with\_life)}: We define a new name \texttt{satisfaction} for the column \texttt{satisfaction\_with\_life}. Notice that the order is \texttt{new\_name\ =\ old\_name}. Here we also use \texttt{=}. A rare occasion where it makes sense to do so.
\end{enumerate}

Just for clarification, the following two lines of code accomplish the same task. The only difference lies that with \texttt{\%\textgreater{}\%} we could chain another function right after it. So, you could say, it is a matter of taste which approach you prefer. However, in later chapters, it will become apparent why using \texttt{\%\textgreater{}\%} is very advantageous.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Renaming a column using \textquotesingle{}\%\textgreater{}\%\textquotesingle{}}
\NormalTok{wvs }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{rename}\NormalTok{(}\AttributeTok{satisfaction\_new =}\NormalTok{ satisfaction)}

\CommentTok{\# Renaming a column without \textquotesingle{}\%\textgreater{}\%\textquotesingle{}}
\FunctionTok{rename}\NormalTok{(wvs, }\AttributeTok{satisfaction\_new =}\NormalTok{ satisfaction)}
\end{Highlighting}
\end{Shaded}

Since you will be using the pipe operator very frequently, it is a good idea to remember the keyboard shortcut for it: \texttt{Ctrl+Shift+M} for PC and \texttt{Cmd+Shift+M} for Mac.

\hypertarget{change-data-types}{%
\section{Data types: What are they and how can you change them}\label{change-data-types}}

When we inspected our data, I mentioned that some variables do not have the correct data type. You might be familiar with different data types by classifying them as:

\begin{itemize}
\item
  \emph{Nominal data}, which is categorical data of no particular order,
\item
  \emph{Ordinal data}, which is categorical data with a defined order, and
\item
  \emph{Quantitative data}, which is data that usually is represented by numeric values.
\end{itemize}

In R we have a slightly different distinction:

\begin{itemize}
\item
  \texttt{character} / \texttt{\textless{}chr\textgreater{}}: Textual data, for example the text of a tweet.
\item
  \texttt{factor} / \texttt{\textless{}fct\textgreater{}}: Categorical data with a finite number of categories with no particular order.
\item
  \texttt{ordered} / \texttt{\textless{}ord\textgreater{}}: Categorical data with a finite number of categories with a particular order.
\item
  \texttt{double} / \texttt{\textless{}dbl\textgreater{}}: Numerical data with decimal places.
\item
  \texttt{integer} / \texttt{\textless{}int\textgreater{}}: Numerical data with whole numbers only (i.e.~no decimals).
\item
  \texttt{logical} / \texttt{\textless{}lgl\textgreater{}}: Logical data, which only consists of values `TRUE' and `FALSE'.
\item
  \texttt{date} / \texttt{date}: Data which consists dates, e.g.~`2021-08-05'.
\item
  \texttt{date-time} / \texttt{dttm}: Data which consists dates and times, e.g.~`2021-08-05 16:29:25 BST'.
\end{itemize}

For a complete list of data types, I recommend looking at \href{https://tibble.tidyverse.org/articles/types.html}{`Column Data Types'} \citep{data-types-2021}.

R has a more fine-grained categorisation of data types. The most important distinction, though, lies between \texttt{\textless{}chr\textgreater{}}, \texttt{\textless{}fct\textgreater{}}/\texttt{\textless{}ord\textgreater{}} and \texttt{\textless{}dbl\textgreater{}} for most datasets in the Social Sciences. Still, it is good to know what the abbreviations in your \texttt{tibble} mean and how they might affect your analysis.

Now that we have a solid understanding of different data types, we can look at our dataset and see whether \texttt{readr} classified our variables correctly.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(wvs)}
\DocumentationTok{\#\# Rows: 69,578}
\DocumentationTok{\#\# Columns: 7}
\DocumentationTok{\#\# $ participant\_id      \textless{}dbl\textgreater{} 20070001, 20070002, 20070003, 20070004, 20070005, \textasciitilde{}}
\DocumentationTok{\#\# $ country             \textless{}chr\textgreater{} "Andorra", "Andorra", "Andorra", "Andorra", "Andor\textasciitilde{}}
\DocumentationTok{\#\# $ gender              \textless{}dbl\textgreater{} 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,\textasciitilde{}}
\DocumentationTok{\#\# $ age                 \textless{}dbl\textgreater{} 60, 47, 48, 62, 49, 51, 33, 55, 40, 38, 54, 39, 44\textasciitilde{}}
\DocumentationTok{\#\# $ relationship\_status \textless{}chr\textgreater{} "married", "living together as married", "separate\textasciitilde{}}
\DocumentationTok{\#\# $ freedom\_of\_choice   \textless{}dbl\textgreater{} 10, 9, 9, 9, 8, 10, 10, 8, 8, 10, 9, 8, 10, 7, 10,\textasciitilde{}}
\DocumentationTok{\#\# $ satisfaction        \textless{}dbl\textgreater{} 10, 9, 9, 8, 7, 10, 5, 8, 8, 10, 8, 8, 10, 7, 10, \textasciitilde{}}
\end{Highlighting}
\end{Shaded}

\texttt{readr} did a great job in identifying all the numeric variables. However, by default, \texttt{readr} imports all variables that include text as \texttt{\textless{}chr\textgreater{}}. It appears, in our dataset, this is not entirely correct. The variables \texttt{countr\_name}, \texttt{gender} and \texttt{relationship\_status} specify a finite number of categories. Therefore they should be classified as a \texttt{factor}. The variable \texttt{participant\_id} is represented by numbers, but its meaning is also rather categorical. We would not use the ID numbers of participants to perform additions or multiplications. This would make no sense. Therefore, it might be wise to turn them into a \texttt{factor}, even though we likely will not use it in our analysis and would make no difference. However, I am a stickler for those kinds of things, and I would include in it.

To perform the conversion, we need to use two new functions from \texttt{dplyr}:

\begin{itemize}
\item
  \texttt{mutate()}: Changes, i.e.~`mutates', a variable.
\item
  \texttt{as\_factor()}: Converts data from one type into a \texttt{factor}.
\end{itemize}

If we want to convert all variables in one go, we can put them into the same function, separated by a \texttt{,}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wvs }\OtherTok{\textless{}{-}}\NormalTok{ wvs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{country =} \FunctionTok{as\_factor}\NormalTok{(country),}
         \AttributeTok{gender =} \FunctionTok{as\_factor}\NormalTok{(gender),}
         \AttributeTok{relationship\_status =} \FunctionTok{as\_factor}\NormalTok{(relationship\_status),}
         \AttributeTok{participant\_id =} \FunctionTok{as\_factor}\NormalTok{(participant\_id)}
\NormalTok{  )}

\FunctionTok{glimpse}\NormalTok{(wvs)}
\DocumentationTok{\#\# Rows: 69,578}
\DocumentationTok{\#\# Columns: 7}
\DocumentationTok{\#\# $ participant\_id      \textless{}fct\textgreater{} 20070001, 20070002, 20070003, 20070004, 20070005, \textasciitilde{}}
\DocumentationTok{\#\# $ country             \textless{}fct\textgreater{} "Andorra", "Andorra", "Andorra", "Andorra", "Andor\textasciitilde{}}
\DocumentationTok{\#\# $ gender              \textless{}fct\textgreater{} 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,\textasciitilde{}}
\DocumentationTok{\#\# $ age                 \textless{}dbl\textgreater{} 60, 47, 48, 62, 49, 51, 33, 55, 40, 38, 54, 39, 44\textasciitilde{}}
\DocumentationTok{\#\# $ relationship\_status \textless{}fct\textgreater{} married, living together as married, separated, li\textasciitilde{}}
\DocumentationTok{\#\# $ freedom\_of\_choice   \textless{}dbl\textgreater{} 10, 9, 9, 9, 8, 10, 10, 8, 8, 10, 9, 8, 10, 7, 10,\textasciitilde{}}
\DocumentationTok{\#\# $ satisfaction        \textless{}dbl\textgreater{} 10, 9, 9, 8, 7, 10, 5, 8, 8, 10, 8, 8, 10, 7, 10, \textasciitilde{}}
\end{Highlighting}
\end{Shaded}

The output in the console shows that we successfully performed the transformation and our data types are as we intended them to be. Mission accomplished.

\hypertarget{handling-factors}{%
\section{Handling factors}\label{handling-factors}}

\hypertarget{recoding-factors}{%
\subsection{Recoding factors}\label{recoding-factors}}

Another common problem we have to tackle when working with data is their representation in the dataset. For example, \texttt{gender} could be measured as \texttt{male} and \texttt{female}\footnote{A sophisticated research project would likely not rely on a dichotomous approach to gender, but appreciate the diversity in this regard.} or as \texttt{0} and \texttt{1}. R does not mind which way you represent your data, but some other software does. Therefore, when we import data from somewhere else, the values of a variable might not look the way we want. The practicality of having your data represented accurately as what they are, becomes apparent when you intend to create tables and plots.

For example, we might be interested in knowing how many participants in the \texttt{wvs} were \texttt{male} and \texttt{female}. The function \texttt{count()} from \texttt{dplyr} does precisely that.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wvs }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{count}\NormalTok{(gender)}
\DocumentationTok{\#\# \# A tibble: 3 x 2}
\DocumentationTok{\#\#   gender     n}
\DocumentationTok{\#\#   \textless{}fct\textgreater{}  \textless{}int\textgreater{}}
\DocumentationTok{\#\# 1 0      33049}
\DocumentationTok{\#\# 2 1      36478}
\DocumentationTok{\#\# 3 \textless{}NA\textgreater{}      51}
\end{Highlighting}
\end{Shaded}

Now we know how many people were \texttt{male} and \texttt{female} and how many did not disclose their \texttt{gender}. Or do we? The issue here is that you would have to know what the \texttt{0} and \texttt{1} stand for. Surely you would have a coding manual that gives you the answer, but it seems a bit of a complication. For \texttt{gender}, this might still be easy to remember, but can you recall the ID numbers for 48 countries?

It certainly would be easier to replace the \texttt{0}s and \texttt{1}s with their corresponding labels. This can be achieved with a simple function called \texttt{fct\_recode()} from \texttt{forcats}. However, since we `mutate' a variable into something else, we also have to use the \texttt{mutate()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wvs }\OtherTok{\textless{}{-}}\NormalTok{ wvs }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{gender =} \FunctionTok{fct\_recode}\NormalTok{(gender, }\StringTok{"male"} \OtherTok{=} \StringTok{"0"}\NormalTok{, }\StringTok{"female"} \OtherTok{=} \StringTok{"1"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

If you have been following along very carefully, you might spot one oddity in this code: \texttt{"0"} and \texttt{"1"}. You likely recall that in Chapter \ref{r-basics-the-very-fundamentals}, I mentioned that we use \texttt{""} for \texttt{character} values but not for numbers. So what happens if we run the code and remove \texttt{""}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wvs }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{gender =} \FunctionTok{fct\_recode}\NormalTok{(gender, }\StringTok{"male"} \OtherTok{=} \DecValTok{0}\NormalTok{, }\StringTok{"female"} \OtherTok{=} \DecValTok{1}\NormalTok{))}
\DocumentationTok{\#\# Error: Problem with \textasciigrave{}mutate()\textasciigrave{} column \textasciigrave{}gender\textasciigrave{}.}
\DocumentationTok{\#\# i \textasciigrave{}gender = fct\_recode(gender, male = 0, female = 1)\textasciigrave{}.}
\DocumentationTok{\#\# x Each input to fct\_recode must be a single named string. Problems at positions: 1, 2}
\end{Highlighting}
\end{Shaded}

The error message is easy to understand: \texttt{fct\_recode()} only expects \texttt{strings} as input and not numbers. R recognises \texttt{0} and \texttt{1} as numbers, but \texttt{fct\_recode()} converts a \texttt{factor} value into another \texttt{factor} value. To refer to a factor level (i.e.~one of the categories in our factor), we have to use \texttt{""}. In other words, data types matter and are often a source of problems with your code. Thus, always pay close attention to it.

If we rerun our analysis and generate a frequency table for \texttt{gender}, we now get a much more readable output.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wvs }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{count}\NormalTok{(gender)}
\DocumentationTok{\#\# \# A tibble: 3 x 2}
\DocumentationTok{\#\#   gender     n}
\DocumentationTok{\#\#   \textless{}fct\textgreater{}  \textless{}int\textgreater{}}
\DocumentationTok{\#\# 1 male   33049}
\DocumentationTok{\#\# 2 female 36478}
\DocumentationTok{\#\# 3 \textless{}NA\textgreater{}      51}
\end{Highlighting}
\end{Shaded}

Another benefit of going through the trouble of recoding your factors is the readability of your plots. For example, we could quickly generate a bar plot based on the above table and have appropriate labels instead of \texttt{0} and \texttt{1}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wvs }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{count}\NormalTok{(gender) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(gender, n)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_col}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_for_non_programmers_files/figure-latex/Barplot for gender-1} \end{center}

Plots are an excellent way to explore your data and understand relationships between variables. More about this when we start to perform analytical steps on our data (see Chapter \ref{descriptive-statistics} and beyond).

Another use case for recoding factors could be for purely cosmetic reasons. For example, when looking through our dataset, we might notice that some country names are very long and do not look great in data visualisations or tables. Thus, we could consider shortening them.

First, we need to find out which country names are particularly long. There are 48 countries in this dataset, so it could take some time to look through them all. Instead, we could use the function \texttt{filter()} from \texttt{dplyr} to pick only countries with a long name. However, this poses another problem: How can we tell the filter function to pick only country names with a certain length? Ideally, we would want a function that does the counting for us. As you probably anticipated, there is a package called \texttt{stringr}, which also belongs to the \texttt{tidyverse,} and has a function that counts the number of characters that represent a value in our dataset: \texttt{str\_length()}. This function takes any \texttt{character} variable and returns the length of it. This also works with \texttt{factors} because this function can `coerce' it into a \texttt{character}, i.e.~it just ignores that it is a \texttt{factor} and looks at it as if it was a regular \texttt{character} variable. Good news for us, because now we can put the puzzle pieces together.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wvs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(stringr}\SpecialCharTok{::}\FunctionTok{str\_length}\NormalTok{(country) }\SpecialCharTok{\textgreater{}=} \DecValTok{15}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(country)}
\DocumentationTok{\#\# \# A tibble: 3 x 2}
\DocumentationTok{\#\#   country                             n}
\DocumentationTok{\#\#   \textless{}fct\textgreater{}                           \textless{}int\textgreater{}}
\DocumentationTok{\#\# 1 Bolivia, Plurinational State of  2067}
\DocumentationTok{\#\# 2 Iran, Islamic Republic of        1499}
\DocumentationTok{\#\# 3 Korea, Republic of               1245}
\end{Highlighting}
\end{Shaded}

I use the value \texttt{15} arbitrarily after some trial and error. You can change the value and see which other countries would show up with a lower threshold. However, this number seems to do the trick and returns three countries that seem to have longer labels. All we have to do is replace these categories with new ones the same way we recoded \texttt{gender}. You probably can guess already what we have to do to achieve this.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wvs }\OtherTok{\textless{}{-}}\NormalTok{ wvs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{country =} \FunctionTok{fct\_recode}\NormalTok{(country,}
                                   \StringTok{"Bolivia"} \OtherTok{=} \StringTok{"Bolivia, Plurinational State of"}\NormalTok{,}
                                   \StringTok{"Iran"} \OtherTok{=} \StringTok{"Iran, Islamic Republic of"}\NormalTok{,}
                                   \StringTok{"Korea"} \OtherTok{=} \StringTok{"Korea, Republic of"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{reordering-factor-levels}{%
\subsection{Reordering factor levels}\label{reordering-factor-levels}}

\texttt{TODO:\ CONTINUE\ FROM\ HERE}

\begin{itemize}
\tightlist
\item
  Consider whether this should happen here or later. Probably later, actually when we talk about descriptive statistics. This is not really data cleaning at this point. Too much stuff already. Move to descriptive statistics section.
\end{itemize}

\hypertarget{dealing-with-missing-data}{%
\section{Dealing with missing data}\label{dealing-with-missing-data}}

There is hardly any Social Sciences project where researchers do not have to deal with missing data. Participants are sometimes unwilling to complete a questionnaire or miss the second round of data collection entirely, e.g., longitudinal studies. It is not the purpose of this chapter to delve into all aspects of analysing missing data but provide a solid starting point. There are mainly three steps involved in dealing with missing data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Mapping missing data
\item
  Identifying patterns of missing data
\item
  Replacing or removing missing data
\end{enumerate}

\hypertarget{mapping-missing-data}{%
\subsection{Mapping missing data}\label{mapping-missing-data}}

Every study that intends to be rigorous will have to identify how much data is missing. In R, this can be achieved in multiple ways, but using a specialised package like \texttt{naniar} does help us to do this very quickly and systematically. First, we have to load the \texttt{naniar} package, and then we use the function \texttt{vis\_miss()} to visualise how much and where exactly data is missing.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(naniar)}
\FunctionTok{vis\_miss}\NormalTok{(wvs)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{r_for_non_programmers_files/figure-latex/mapping-missing-data-1} 

}

\caption{Mapping missing data with naniar}\label{fig:mapping-missing-data}
\end{figure}

\texttt{naniar} plays along nicely with the \texttt{tidyverse} approach of programming. As such, it would also be possible to write \texttt{wvs\ \%\textgreater{}\%\ vis\_miss()}.

As we can see, 99,7\% of our dataset is complete, and we are only missing 0.3\%. The dark lines (actually blocks) refer to missing data points. On the x-axis, we can see all our variables, and on the y-axis, we see our observations. This is the same layout as our rectangular dataset: Rows are observations, and columns are variables. Overall, this dataset appears relatively complete (luckily). In addition, we can see the percentage of missing data per variable. \texttt{freedom\_of\_choice} is the variable with the most missing data, i.e.~0.79\%. Still, the amount of missing data is not very large.

When working with larger datasets, it might also be helpful to rank variables by their degree of missing data to see where the most significant problems lie.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gg\_miss\_var}\NormalTok{(wvs)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics{r_for_non_programmers_files/figure-latex/missing-data-per-variable-1} 

}

\caption{Missing data per variable}\label{fig:missing-data-per-variable}
\end{figure}

It is noticeable that \texttt{freedom\_of\_choice} has the most missing data points, while \texttt{participant\_id}, and \texttt{country\_name} have no missing values. If you prefer to see the actual numbers instead, we can use a series of functions that start with \texttt{miss\_} (for a complete list of all functions, see the \href{https://naniar.njtierney.com/reference/index.html}{reference page of naniar}). For example, to retrieve the numeric values which are reflected in the plot above, we can write the following:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Summarise the missingness in each variable}
\FunctionTok{miss\_var\_summary}\NormalTok{(wvs)}
\DocumentationTok{\#\# \# A tibble: 7 x 3}
\DocumentationTok{\#\#   variable            n\_miss pct\_miss}
\DocumentationTok{\#\#   \textless{}chr\textgreater{}                \textless{}int\textgreater{}    \textless{}dbl\textgreater{}}
\DocumentationTok{\#\# 1 freedom\_of\_choice      547   0.786 }
\DocumentationTok{\#\# 2 relationship\_status    335   0.481 }
\DocumentationTok{\#\# 3 age                    318   0.457 }
\DocumentationTok{\#\# 4 satisfaction           239   0.343 }
\DocumentationTok{\#\# 5 gender                  51   0.0733}
\DocumentationTok{\#\# 6 participant\_id           0   0     }
\DocumentationTok{\#\# 7 country                  0   0}
\end{Highlighting}
\end{Shaded}

I tend to prefer data visualisations over numerical results for mapping missing data, especially in larger datasets with many variables. This also has the benefit that patterns of missing data can be more easily identified as well.

\hypertarget{patterns-of-missing-data}{%
\subsection{Identifying patterns of missing data}\label{patterns-of-missing-data}}

If you find that your data `suffers' from missing data, it is essential to answer another question: Is data missing systematically? This is quite an important diagnostic step since systematically missing data would imply that if we remove these observations from our dataset, we likely produce the wrong results.

We can distinguish missing data based on how it is missing, i.e.

\begin{itemize}
\item
  missing completely at random (MCAR),
\item
  missing at random (MAR), and
\item
  missing not at random (MNAR). \citep{rubin-1976}
\end{itemize}

\hypertarget{missing-completetly-at-random-mcar}{%
\subsubsection{Missing completely at random (MCAR)}\label{missing-completetly-at-random-mcar}}

Missing completely at random (MCAR) means that neither observed nor missing data can systematically explain why data is missing. It is a pure coincidence how data is missing, and there is no underlying pattern.

The \texttt{naniar} package comes with the very popular Little's MCAR test \citep{little-1988}, which provides insights into whether our data is missing completely at random. Thus, we can call the function \texttt{mcar\_test()} and inspect the result.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wvs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{participant\_id) }\SpecialCharTok{\%\textgreater{}\%}     \CommentTok{\# Remove variables which do not reflect a response}
  \FunctionTok{mcar\_test}\NormalTok{()}
\DocumentationTok{\#\# \# A tibble: 1 x 4}
\DocumentationTok{\#\#   statistic    df p.value missing.patterns}
\DocumentationTok{\#\#       \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}            \textless{}int\textgreater{}}
\DocumentationTok{\#\# 1      411.    67       0               19}
\end{Highlighting}
\end{Shaded}

When you run such a test, you have ensure that variables that are not part of the data collection are removed. In our case, the \texttt{participant\_id} is generated by the researcher and does not represent an actual response by the participants. As such, we need to remove it using \texttt{select()} before we can run the test. A \texttt{-} inverts the meaning of \texttt{select()}. While \texttt{select(participant\_id)} would do what it says, i.e.~include it as the only variable in the test, \texttt{select(-participant\_id)} results in selecting everything but this variable in our test. You will find it is sometimes easier to remove a variable with \texttt{select()} rather than listing all the variables you want to keep.

Since the \texttt{p.value} of the test is so small that it got rounded down to \texttt{0}, i.e.~\(p<0.0001\), we have to assume that our data is not missing completely at random. If we found that \(p>0.05\), we would have confirmation that data are missing completely at random.

\hypertarget{missing-at-random-mar}{%
\subsubsection{Missing at random (MAR)}\label{missing-at-random-mar}}

Missing at random (MAR) refers to a situation where the observed data can explain missing data, but not the missing data. \citet{dong-et-al-2013} (p.~2) provide a good example when this is the case:

\begin{quote}
\emph{Let's suppose that students who scored low on the pre-test are more likely to drop out of the course, hence, their scores on the post-test are missing. If we assume that the probability of missing the post-test depends only on scores on the pre-test, then the missing mechanism on the post-test is MAR. In other words, for students who have the same pre-test score, the probability of {[}them{]} missing the post-test is random.}
\end{quote}

Thus,the main difference between MCAR and MAR data lies in the fact that we can observe some patterns of missing data if data is MAR. These patterns are only based on data we have, i.e.~observed data. We also assume that no unobserved variables can explain these or other patterns.

Accordingly, we first look into variables with no missing data and see whether they can explain our missing data in other variables. For example, we could investigate whether missing data in \texttt{freedom\_of\_choice} is attributed to specific countries.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wvs }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(country) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(freedom\_of\_choice)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(n))                      }\CommentTok{\# Rearranging scores for easier reading}
\DocumentationTok{\#\# \# A tibble: 32 x 2}
\DocumentationTok{\#\# \# Groups:   country [32]}
\DocumentationTok{\#\#    country         n}
\DocumentationTok{\#\#    \textless{}fct\textgreater{}       \textless{}int\textgreater{}}
\DocumentationTok{\#\#  1 Japan          47}
\DocumentationTok{\#\#  2 Brazil         44}
\DocumentationTok{\#\#  3 New Zealand    44}
\DocumentationTok{\#\#  4 Russia         43}
\DocumentationTok{\#\#  5 Bolivia        40}
\DocumentationTok{\#\#  6 Romania        29}
\DocumentationTok{\#\#  7 Kazakhstan     27}
\DocumentationTok{\#\#  8 Turkey         24}
\DocumentationTok{\#\#  9 Egypt          23}
\DocumentationTok{\#\# 10 Serbia         20}
\DocumentationTok{\#\# \# ... with 22 more rows}
\end{Highlighting}
\end{Shaded}

It seems four countries have exceptionally high numbers of missing data for \texttt{freedom\_of\_choice}: Japan, Brazil, New Zealand, Russia and Bolivia. Why this is the case lies beyond this dataset and is something only the researchers themselves could explain. Collecting data in different countries is particularly challenging, and one is quickly faced with different unfavourable conditions. Furthermore, the missing data is not completely random because we have some first evidence that the location of data collection might have affected its completeness.

Another way of understanding patterns of missing data can be achieved by looking at relationships between missing values, for example, the co-occurrence of missing values across different variables. This can be achieved by using upset plots. An upset plot consists of three parts: Set size, intersection size and a Venn diagram which defines the intersections.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gg\_miss\_upset}\NormalTok{(wvs)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_for_non_programmers_files/figure-latex/Upset plot-1} \end{center}

The most frequent combination of missing data in our dataset occurs when only \texttt{freedom\_of\_choice} is missing (the first column), but nothing else. Similar results can be found for \texttt{relationships\_status} and \texttt{age}. The first combination of missing data is defined by two variables: \texttt{satisfaction} and \texttt{freedom\_of\_choice}. In total, 107 participants had \texttt{satisfaction} and \texttt{freedom\_of\_choice} missing but nothing else.

The `set size' shown in the upset plot refers to the number of missing values for each variable in the diagram. This corresponds to what we have found when looking at Figure \ref{fig:missing-data-per-variable}).

Our analysis also suggests that values are not completely randomly missing but that we have data to help explain why they are missing.

\hypertarget{missing-not-at-random-mnar}{%
\subsubsection{Missing not at random (MNAR)}\label{missing-not-at-random-mnar}}

Lastly, missing not at random (MNAR) implies that data is missing systematically and that other variables or reasons exist that explain why data is missing. Still, they are not fully known to us. In questionnaire-based research, an easily overlooked reason that can explain missing data is the `page-drop-off' phenomenon. In such cases, participants stop completing a questionnaire once they advance to another page. Figure \ref{fig:mnar-example} shows this very clearly for a large scale project where an online questionnaire was used. After almost every page break in the questionnaire, some participants decided to discontinue. Finding these types of patterns is difficult when only working with numeric values. Thus, it is always advisable to visualise your data as well. Such missing data is linked to the design of the data collection tool.

\begin{figure}

{\centering \includegraphics[width=26.67in]{images/chapter_07_img/03_missing_data/00_mnar_example} 

}

\caption{MNAR pattern in a dataset due to 'page-drop-offs'}\label{fig:mnar-example}
\end{figure}

Defining whether a dataset is MNAR or not is mainly achieved by ruling out MCAR and MAR assumptions. It is not possible to test whether missing data is MNAR, unless we have more information about the underlying population available \citep{van2020rebutting}.

We have sufficient evidence that our data is MAR as was shown \protect\hyperlink{missing-at-random-mar}{above}, because we managed to identify some relationships between unobserved and observed data. In practice, it is very rare to find datasets that are truly MCAR \citep{van-buuren-2018}. Therefore we might consider `imputation' as a possible strategy to solve our missing data problem. More about imputation in the next Chapter.

If you are looking for more inspiration of how you could visualise and identify patterns of missingness in your data, you might find the `\href{https://naniar.njtierney.com/articles/naniar-visualisation.html}{Gallery}' of the \texttt{naniar} website particularly useful.

\hypertarget{replacing-removing-missing-data}{%
\subsection{Replacing or removing missing data}\label{replacing-removing-missing-data}}

Once you determined which pattern of missing data applies to your dataset, it is time to evaluate how we want to deal with those missing values. Generally, you can either keep the missing values as they are, replace them or remove them entirely.

\citet{jakobsen-et-al-2017} provide a rule of thumb of 5\% where researchers can consider missing data as negligible, i.e.~we can ignore the missing values because they won't affect our analysis in a significant way. They also argue that if the proportion of missing data exceeds 40\%, we should also only work with our observed data. However, with such a large amount of missing data, it is questionable whether we can rely on our analysis as much as we want to. If the missing data lies somewhere in-between this range, we need to consider the missing data pattern at hand.

If data is MCAR, we could remove missing data. This process is called `listwise deletion', i.e.~you remove all data from a participant with missing data. As just mentioned, removing missing values is only suitable if you have relatively few missing values in your dataset (see also \citet{schafer-1999}), as is the case with the \texttt{wvs} dataset. There are additional problems with deleting observations listwise, many of which are summarised by \citet{van-buuren-2018} in his \href{https://stefvanbuuren.name/fimd/sec-simplesolutions.html}{introduction}. Usually, we try to avoid removing data as much as possible. If you wanted to perform listwise deletion, it can be done with a single function call: \texttt{na.omit()} from the built-in \texttt{stats} package. Here is an example of how we can apply this function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# No more missing data in this plot}
\NormalTok{wvs }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{na.omit}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{vis\_miss}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_for_non_programmers_files/figure-latex/Example of removing all missing data points-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]

\CommentTok{\# How much observations are left after we removed all missing data?}
\NormalTok{wvs }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{na.omit}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{count}\NormalTok{()}
\DocumentationTok{\#\# \# A tibble: 1 x 1}
\DocumentationTok{\#\#       n}
\DocumentationTok{\#\#   \textless{}int\textgreater{}}
\DocumentationTok{\#\# 1 68312}
\end{Highlighting}
\end{Shaded}

If you wanted to remove the missing data without the plot, you could use \texttt{wvs\_no\_na\ \textless{}-\ na.omit(wvs)}. However, I always recommend `saving' the result in a new object to ensure I keep my original data available. This can be helpful when trying to compare how this decision affects my analysis, i.e.~I can run the analysis with and without missing data removed. For the \texttt{wvs} dataset, this does not seem to be the best option.

Based on our analysis of the \texttt{wvs} dataset (by no means complete!), we could assume that data is MAR. In such cases, it is possible to `impute' the missing values, i.e.~replacing the missing data with computed scores. This is possible because we can model the missing data based on variables we have. We cannot model missing data based on variables we have not measured in our study for obvious reasons.

You might be thinking: ``Why would it make data better if we `make up' numbers? Is this not cheating?'' Imputation of missing values is a science in itself. There is plenty to read about the process of handling missing data, which would reach far beyond the scope of this book. However, the seminal work of \citet{van-buuren-2018}, \citet{dong-et-al-2013} and \citet{jakobsen-et-al-2017} are excellent starting points. In short: Simply removing missing data can lead to biases in your data, e.g.~if we removed missing data in our dataset, we would mainly exclude participants from Japan, Brazil, New Zealand, Russia and Bolivia (as we found out \protect\hyperlink{missing-data-by-country-table}{earlier}). While imputation is not perfect, scholars have shown that it can produce more reliable results than not using imputation at all (\texttt{add\ some\ references\ here}), assuming the data meets the requirements for such imputation.

Even though our dataset has only a minimal amount of missing data (relative to the entire size), we can still use imputation. There are many different ways to approach this task, one of which is `multiple imputation'. As highlighted by \citet{van2020rebutting}, multiple imputation has not yet reached the same popularity as listwise deletion, despite its benefits. The main reason for this lies in the complexity of using this technique. Therefore I included an example of how to use multiple imputation separately in Chapter @ref(). There are many more approaches to imputation, and going through them all in detail would be impossible and distract from the book's main purpose. Still, I would like to share some interesting packages with you that use different imputation methods:

\begin{itemize}
\item
  \href{https://cran.r-project.org/web/packages/Amelia/index.html}{mice} (Multivariate Imputation via Chained Equations)
\item
  \href{https://cran.r-project.org/web/packages/Amelia/index.html}{Amelia} (Uses a bootstrapped-based algorithm)
\item
  \href{https://cran.r-project.org/web/packages/missForest/index.html}{missForest} (Uses a random forest algorithm)
\item
  \href{https://cran.r-project.org/web/packages/Hmisc/index.html}{Hmisc} (Uses Additive Regression, Bootstrapping, and Predictive Mean Matching)
\item
  \href{https://cran.r-project.org/web/packages/mi/index.html}{mi} (Uses posterior predictive distribution, predictive mean matching, mean-imputation, median-imputation, or conditional mean-imputation)
\end{itemize}

Besides multiple imputation, there is also the option for single imputation. The \texttt{simputation} package offers a great variety of imputation functions, one of which also fits our data quite well \texttt{impute\_knn()}. This function makes use of a clustering technique called `K-nearest neighbour'. In this case, the function will look for observations closest to the one with missing data and take the value of that observation. In other words, it looks for similar participants who answered the questionnaire in a very similar way. The great convenience of this approach is that it can handle all kinds of data types simultaneously, which is not true for all imputation techniques.

If we apply this function to our dataset, we have to write the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wvs\_nona }\OtherTok{\textless{}{-}}\NormalTok{ wvs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{participant\_id) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}                  \CommentTok{\# Transforms our tibble into a data frame}
\NormalTok{  simputation}\SpecialCharTok{::}\FunctionTok{impute\_knn}\NormalTok{(. }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .)}
\end{Highlighting}
\end{Shaded}

The function \texttt{impute\_knn(.\ \textasciitilde{}\ .)} might look like a combination of text with an emoji \texttt{(.\ \textasciitilde{}\ .)}. This imputation function requires us to specify a model to impute the data. Since we want to impute all missing values in the dataset and use all variables available, we put \texttt{.} on both sides of the equation, separated by a \texttt{\textasciitilde{}}. The \texttt{.} reflects that we do not specify a specific variable but instead tell the function to use all variables that are not mentioned. In our case, we did not mention any variables at all, and therefore it chooses all of them. For example, if we wanted to impute only \texttt{freedom\_of\_choice}, we would have to put \texttt{impute\_knn(freedom\_of\_choice\ \textasciitilde{}\ .)}. We will elaborate more on how to specify models when we cover regression models (see Chapter \ref{regression}).

As you will have noticed, we also had to convert our \texttt{tibble} into a \texttt{data\ frame} using \texttt{as.data.frame()}. As mentioned in Chapter \ref{functions}, some functions require a specific data type or format. The \texttt{simputation} package works with \texttt{dplyr}, but it prefers \texttt{data\ frame}s. Based on my experiences, the wrong data type or format is one of the most frequent causes of errors that novice R programmers report. So, keep an eye on it and read the documentation carefully.

Be aware that imputation of any kind can take a long time. For example, my MacBook Pro took about 4.22 seconds to complete \texttt{impute\_knn()} with the \texttt{wvs} dataset. If we used multiple imputation, this would have taken considerably longer, i.e.~several minutes and more.

We now should have a dataset that is free of any missing values. To ensure this is the case we can create the missing data matrix that we made at the beginning of this chapter (see Figure \ref{fig:mapping-missing-data}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vis\_miss}\NormalTok{(wvs\_nona)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{r_for_non_programmers_files/figure-latex/Mapping missing data of imputeted dataset-1} \end{center}

\hypertarget{main-takeaways-regarding-dealing-with-missing-data}{%
\subsection{Main takeaways regarding dealing with missing data}\label{main-takeaways-regarding-dealing-with-missing-data}}

Handling missing data is hardly ever a simple process. Do not feel discouraged if you get lost in the myriad of options. While there is some guidance on how and when to use specific strategies to deal with missing values in your dataset, the most crucial point to remember is: Be transparent about what you did. As long as you can explain why you did something and how you did it, everyone can follow your thought process and help improve your analysis. However, ignoring the fact that data is missing and not acknowledging it is more than just unwise.

\hypertarget{latent-constructs}{%
\section{Latent constructs and their reliability}\label{latent-constructs}}

Social Scientists commonly face the challenge that we want to measure something that cannot be measured directly. For example, `happiness' is a feeling that does not naturally occur as a number we can observe and measure. The opposite is true for `temperature' which is naturally measured in numbers. At the same time, `happiness' is much more complex of a variable than `temperature', because `happiness' can unfold in various ways and be caused by different triggers (e.g.~a joke, an unexpected present, tasty food, etc.). To account for this, we often work with `latent variables'. These are defined as variables that are not directly measured but are inferred from other variables. In practice, we often use multiple questions in a questionnaire to measure one latent variable, usually by computing the mean of those questions.

The \texttt{gep} dataset from the \texttt{r4np} package includes data about students' social integration experience (\texttt{si}) and communication skills development (\texttt{cs}). Data were obtained using the \href{https://warwick.ac.uk/gep}{Global Education Profiler (GEP)}. I extracted 6 different questions (also called `items') from the questionnaire, which measure \texttt{si}, and 6 items that measure \texttt{cs}. This dataset only consists of a randomly chosen set of responses, i.e.~300 out of over 12,000.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(gep)}
\DocumentationTok{\#\# Rows: 300}
\DocumentationTok{\#\# Columns: 12}
\DocumentationTok{\#\# $ age                           \textless{}dbl\textgreater{} 22, 26, 21, 23, 25, 27, 24, 23, 21, 24, \textasciitilde{}}
\DocumentationTok{\#\# $ gender                        \textless{}chr\textgreater{} "Female", "Female", "Female", "Female", \textasciitilde{}}
\DocumentationTok{\#\# $ level\_of\_study                \textless{}chr\textgreater{} "UG", "PGT", "UG", "UG", "PGT", "PGT", "\textasciitilde{}}
\DocumentationTok{\#\# $ si\_socialise\_with\_people\_exp  \textless{}dbl\textgreater{} 6, 3, 4, 3, 4, 2, 5, 1, 6, 6, 3, 3, 4, 5\textasciitilde{}}
\DocumentationTok{\#\# $ si\_supportive\_friends\_exp     \textless{}dbl\textgreater{} 4, 4, 5, 4, 4, 2, 5, 1, 6, 6, 3, 3, 4, 6\textasciitilde{}}
\DocumentationTok{\#\# $ si\_time\_socialising\_exp       \textless{}dbl\textgreater{} 5, 2, 4, 4, 3, 2, 6, 3, 6, 6, 3, 2, 4, 6\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_explain\_ideas\_imp          \textless{}dbl\textgreater{} 6, 5, 5, 5, 5, 5, 4, 5, 6, 6, 5, 5, 4, 6\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_find\_clarification\_imp     \textless{}dbl\textgreater{} 4, 5, 5, 6, 6, 5, 4, 6, 6, 6, 5, 5, 4, 5\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_learn\_different\_styles\_imp \textless{}dbl\textgreater{} 6, 5, 6, 4, 4, 4, 4, 6, 6, 6, 4, 5, 5, 5\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_explain\_ideas\_exp          \textless{}dbl\textgreater{} 6, 5, 2, 5, 6, 5, 6, 6, 6, 6, 4, 4, 3, 6\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_find\_clarification\_exp     \textless{}dbl\textgreater{} 6, 5, 4, 6, 6, 5, 6, 6, 6, 6, 2, 5, 4, 5\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_learn\_different\_styles\_exp \textless{}dbl\textgreater{} 6, 4, 5, 4, 6, 3, 5, 6, 6, 6, 2, 4, 2, 5\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

For example, if we wanted to know how each student scored with regards to social integration (\texttt{si}), we have to

\begin{itemize}
\item
  compute the mean (\texttt{mean())} of all related items (i.e.~all variables starting with \texttt{si\_}),
\item
  for each row (\texttt{rowwise()}) because each row presents one participant.
\end{itemize}

The same is true for communication skills (\texttt{cs\_imp} and \texttt{cs\_exp}). We can compute all three variables in one go. For each variable, we compute \texttt{mean()} and use \texttt{c()} to list all the variables that we want to include in the mean:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute the scores for the latent variable \textquotesingle{}si\textquotesingle{} and \textquotesingle{}cs\textquotesingle{}}
\NormalTok{gep }\OtherTok{\textless{}{-}}\NormalTok{ gep }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{si =} \FunctionTok{mean}\NormalTok{(}\FunctionTok{c}\NormalTok{(si\_socialise\_with\_people\_exp,}
\NormalTok{                     si\_supportive\_friends\_exp,}
\NormalTok{                     si\_time\_socialising\_exp}
\NormalTok{                     )}
\NormalTok{                   ),}
         \AttributeTok{cs\_imp =} \FunctionTok{mean}\NormalTok{(}\FunctionTok{c}\NormalTok{(cs\_explain\_ideas\_imp,}
\NormalTok{                         cs\_find\_clarification\_imp,}
\NormalTok{                         cs\_learn\_different\_styles\_imp}
\NormalTok{                         )}
\NormalTok{                       ),}
         \AttributeTok{cs\_exp =} \FunctionTok{mean}\NormalTok{(}\FunctionTok{c}\NormalTok{(cs\_explain\_ideas\_exp,}
\NormalTok{                         cs\_find\_clarification\_exp,}
\NormalTok{                         cs\_learn\_different\_styles\_exp}
\NormalTok{                         )}
\NormalTok{                       )}
\NormalTok{         )}

\FunctionTok{glimpse}\NormalTok{(gep)}
\DocumentationTok{\#\# Rows: 300}
\DocumentationTok{\#\# Columns: 15}
\DocumentationTok{\#\# Rowwise: }
\DocumentationTok{\#\# $ age                           \textless{}dbl\textgreater{} 22, 26, 21, 23, 25, 27, 24, 23, 21, 24, \textasciitilde{}}
\DocumentationTok{\#\# $ gender                        \textless{}chr\textgreater{} "Female", "Female", "Female", "Female", \textasciitilde{}}
\DocumentationTok{\#\# $ level\_of\_study                \textless{}chr\textgreater{} "UG", "PGT", "UG", "UG", "PGT", "PGT", "\textasciitilde{}}
\DocumentationTok{\#\# $ si\_socialise\_with\_people\_exp  \textless{}dbl\textgreater{} 6, 3, 4, 3, 4, 2, 5, 1, 6, 6, 3, 3, 4, 5\textasciitilde{}}
\DocumentationTok{\#\# $ si\_supportive\_friends\_exp     \textless{}dbl\textgreater{} 4, 4, 5, 4, 4, 2, 5, 1, 6, 6, 3, 3, 4, 6\textasciitilde{}}
\DocumentationTok{\#\# $ si\_time\_socialising\_exp       \textless{}dbl\textgreater{} 5, 2, 4, 4, 3, 2, 6, 3, 6, 6, 3, 2, 4, 6\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_explain\_ideas\_imp          \textless{}dbl\textgreater{} 6, 5, 5, 5, 5, 5, 4, 5, 6, 6, 5, 5, 4, 6\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_find\_clarification\_imp     \textless{}dbl\textgreater{} 4, 5, 5, 6, 6, 5, 4, 6, 6, 6, 5, 5, 4, 5\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_learn\_different\_styles\_imp \textless{}dbl\textgreater{} 6, 5, 6, 4, 4, 4, 4, 6, 6, 6, 4, 5, 5, 5\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_explain\_ideas\_exp          \textless{}dbl\textgreater{} 6, 5, 2, 5, 6, 5, 6, 6, 6, 6, 4, 4, 3, 6\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_find\_clarification\_exp     \textless{}dbl\textgreater{} 6, 5, 4, 6, 6, 5, 6, 6, 6, 6, 2, 5, 4, 5\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_learn\_different\_styles\_exp \textless{}dbl\textgreater{} 6, 4, 5, 4, 6, 3, 5, 6, 6, 6, 2, 4, 2, 5\textasciitilde{}}
\DocumentationTok{\#\# $ si                            \textless{}dbl\textgreater{} 5.000000, 3.000000, 4.333333, 3.666667, \textasciitilde{}}
\DocumentationTok{\#\# $ cs\_imp                        \textless{}dbl\textgreater{} 5.333333, 5.000000, 5.333333, 5.000000, \textasciitilde{}}
\DocumentationTok{\#\# $ cs\_exp                        \textless{}dbl\textgreater{} 6.000000, 4.666667, 3.666667, 5.000000, \textasciitilde{}}
\end{Highlighting}
\end{Shaded}

Compared to dealing with missing data, this is a fairly straightforward task. However, there is a caveat. Before we can compute the mean across all these variables, we need to know and understand whether all these scores reliably contribute to one single latent variable. If not, we would be in trouble and make a significant mistake.

By far, the most common approach to assessing reliability (or more accurately `internal consistency') of latent variables is Cronbach's \(\alpha\). This indicator looks at how strongly a set of items (i.e.~questions in your questionnaire) are related to each other. For example, the stronger the relationship of all items starting with \texttt{si\_} to each other, the more likely we achieve a higher Cronbach's \(\alpha\). The psych package has a suitable function to compute it for us.

Instead of listing all the items by hand, I use the function \texttt{starts\_with()} to pick only the variables whose names start with \texttt{si\_}. It certainly pays off to think about your variable names more thoroughly in advance to benefit from such shortcuts (see also Chapter \ref{colnames-cleaning}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gep }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"si\_"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{()}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Reliability analysis   }
\DocumentationTok{\#\# Call: psych::alpha(x = .)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#   raw\_alpha std.alpha G6(smc) average\_r S/N   ase mean  sd median\_r}
\DocumentationTok{\#\#       0.85      0.85     0.8      0.66 5.8 0.015  3.5 1.3     0.65}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#  lower alpha upper     95\% confidence boundaries}
\DocumentationTok{\#\# 0.82 0.85 0.88 }
\DocumentationTok{\#\# }
\DocumentationTok{\#\#  Reliability if an item is dropped:}
\DocumentationTok{\#\#                              raw\_alpha std.alpha G6(smc) average\_r S/N alpha se}
\DocumentationTok{\#\# si\_socialise\_with\_people\_exp      0.82      0.82    0.70      0.70 4.6    0.021}
\DocumentationTok{\#\# si\_supportive\_friends\_exp         0.77      0.77    0.63      0.63 3.4    0.027}
\DocumentationTok{\#\# si\_time\_socialising\_exp           0.79      0.79    0.65      0.65 3.7    0.025}
\DocumentationTok{\#\#                              var.r med.r}
\DocumentationTok{\#\# si\_socialise\_with\_people\_exp    NA  0.70}
\DocumentationTok{\#\# si\_supportive\_friends\_exp       NA  0.63}
\DocumentationTok{\#\# si\_time\_socialising\_exp         NA  0.65}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#  Item statistics }
\DocumentationTok{\#\#                                n raw.r std.r r.cor r.drop mean  sd}
\DocumentationTok{\#\# si\_socialise\_with\_people\_exp 300  0.86  0.86  0.75   0.69  3.7 1.4}
\DocumentationTok{\#\# si\_supportive\_friends\_exp    300  0.90  0.89  0.81   0.75  3.5 1.6}
\DocumentationTok{\#\# si\_time\_socialising\_exp      300  0.88  0.88  0.79   0.73  3.2 1.5}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Non missing response frequency for each item}
\DocumentationTok{\#\#                                 1    2    3    4    5    6 miss}
\DocumentationTok{\#\# si\_socialise\_with\_people\_exp 0.06 0.17 0.22 0.24 0.19 0.12    0}
\DocumentationTok{\#\# si\_supportive\_friends\_exp    0.13 0.18 0.20 0.18 0.18 0.13    0}
\DocumentationTok{\#\# si\_time\_socialising\_exp      0.15 0.21 0.22 0.20 0.12 0.10    0}
\end{Highlighting}
\end{Shaded}

The function \texttt{alpha()} returns a lot of information. The most important part, though, is shown at the very beginning:

\begin{verbatim}
##  raw_alpha std.alpha G6(smc) average_r  S/N  ase mean   sd median_r
##       0.85      0.85     0.8      0.66 5.76 0.01 3.46 1.33     0.65
\end{verbatim}

In most publications, researchers would primarily report the \texttt{raw\_alpha} value. This is fine, but it is not a bad idea to include at least \texttt{std.alpha} and \texttt{G6(smc)}.

In terms of interpretation, Cronbrach's \(\alpha\) scores can range from 0 to 1. The closer the score to 1, the higher we would judge its reliability. \citet{nunally-1967} originally provided the following classification for Cronbach's \(\alpha\):

\begin{itemize}
\item
  between 0.6 and 0.5 can be sufficient during the early stages of development,
\item
  0.8 or higher is sufficient for most basic research,
\item
  0.9 or higher is suitable for applied research, where the questionnaires are used to make critical decisions, e.g.~clinical studies, university admission tests, etc., with a `desired standard' of 0.95.
\end{itemize}

However, a few years later, \citet{nunally-1978} revisited his original categorisation and considered 0.7 or higher as a suitable benchmark in more exploratory-type research. This gave grounds for researchers to pick and choose the `right' publication for them \citep{henson2001understanding}. Consequently, depending on your research field, the expected reliability score might lean more towards 0.7 or 0.8. Still, the higher the score, the better it is.

Our dataset shows that \texttt{si} scores a solid \(\alpha = 0.85\), which is excellent. We should repeat this step for \texttt{cs\_imp} and \texttt{cs\_exp} as well. However, we have to adjust \texttt{select()}, because we only want variables included that start with \texttt{cs\_} and end with the facet of the respective variable, i.e.~\texttt{\_imp} or \texttt{\_exp}. We also created two variables that start with \texttt{cs\_}, which we also have to remove. Let me demonstrate the `evolution' of how the \texttt{select()} function works to achieve what we want.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Select only variables which start with \textquotesingle{}cs\_\textquotesingle{}}
\NormalTok{gep }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"cs\_"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{glimpse}\NormalTok{()}
\DocumentationTok{\#\# Rows: 300}
\DocumentationTok{\#\# Columns: 8}
\DocumentationTok{\#\# Rowwise: }
\DocumentationTok{\#\# $ cs\_explain\_ideas\_imp          \textless{}dbl\textgreater{} 6, 5, 5, 5, 5, 5, 4, 5, 6, 6, 5, 5, 4, 6\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_find\_clarification\_imp     \textless{}dbl\textgreater{} 4, 5, 5, 6, 6, 5, 4, 6, 6, 6, 5, 5, 4, 5\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_learn\_different\_styles\_imp \textless{}dbl\textgreater{} 6, 5, 6, 4, 4, 4, 4, 6, 6, 6, 4, 5, 5, 5\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_explain\_ideas\_exp          \textless{}dbl\textgreater{} 6, 5, 2, 5, 6, 5, 6, 6, 6, 6, 4, 4, 3, 6\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_find\_clarification\_exp     \textless{}dbl\textgreater{} 6, 5, 4, 6, 6, 5, 6, 6, 6, 6, 2, 5, 4, 5\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_learn\_different\_styles\_exp \textless{}dbl\textgreater{} 6, 4, 5, 4, 6, 3, 5, 6, 6, 6, 2, 4, 2, 5\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_imp                        \textless{}dbl\textgreater{} 5.333333, 5.000000, 5.333333, 5.000000, \textasciitilde{}}
\DocumentationTok{\#\# $ cs\_exp                        \textless{}dbl\textgreater{} 6.000000, 4.666667, 3.666667, 5.000000, \textasciitilde{}}

\CommentTok{\# Do the above but include only variables that also end with \textquotesingle{}\_imp\textquotesingle{}}
\NormalTok{gep }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"cs\_"}\NormalTok{) }\SpecialCharTok{\&} \FunctionTok{ends\_with}\NormalTok{(}\StringTok{"\_imp"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\DocumentationTok{\#\# Rows: 300}
\DocumentationTok{\#\# Columns: 4}
\DocumentationTok{\#\# Rowwise: }
\DocumentationTok{\#\# $ cs\_explain\_ideas\_imp          \textless{}dbl\textgreater{} 6, 5, 5, 5, 5, 5, 4, 5, 6, 6, 5, 5, 4, 6\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_find\_clarification\_imp     \textless{}dbl\textgreater{} 4, 5, 5, 6, 6, 5, 4, 6, 6, 6, 5, 5, 4, 5\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_learn\_different\_styles\_imp \textless{}dbl\textgreater{} 6, 5, 6, 4, 4, 4, 4, 6, 6, 6, 4, 5, 5, 5\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_imp                        \textless{}dbl\textgreater{} 5.333333, 5.000000, 5.333333, 5.000000, \textasciitilde{}}

\CommentTok{\# Remove \textquotesingle{}cs\_imp\textquotesingle{}, because it is the computed latent variable}
\NormalTok{gep }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"cs\_"}\NormalTok{) }\SpecialCharTok{\&} \FunctionTok{ends\_with}\NormalTok{(}\StringTok{"\_imp"}\NormalTok{), }\SpecialCharTok{{-}}\NormalTok{cs\_imp) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}
\DocumentationTok{\#\# Rows: 300}
\DocumentationTok{\#\# Columns: 3}
\DocumentationTok{\#\# Rowwise: }
\DocumentationTok{\#\# $ cs\_explain\_ideas\_imp          \textless{}dbl\textgreater{} 6, 5, 5, 5, 5, 5, 4, 5, 6, 6, 5, 5, 4, 6\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_find\_clarification\_imp     \textless{}dbl\textgreater{} 4, 5, 5, 6, 6, 5, 4, 6, 6, 6, 5, 5, 4, 5\textasciitilde{}}
\DocumentationTok{\#\# $ cs\_learn\_different\_styles\_imp \textless{}dbl\textgreater{} 6, 5, 6, 4, 4, 4, 4, 6, 6, 6, 4, 5, 5, 5\textasciitilde{}}
\end{Highlighting}
\end{Shaded}

Since we want to compute the Cronbach's \(\alpha\) for both variables, we write the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gep }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"cs\_"}\NormalTok{) }\SpecialCharTok{\&} \FunctionTok{ends\_with}\NormalTok{(}\StringTok{"\_imp"}\NormalTok{), }\SpecialCharTok{{-}}\NormalTok{cs\_imp) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{()}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Reliability analysis   }
\DocumentationTok{\#\# Call: psych::alpha(x = .)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#   raw\_alpha std.alpha G6(smc) average\_r S/N   ase mean   sd median\_r}
\DocumentationTok{\#\#       0.86      0.87    0.82      0.68 6.5 0.014    5 0.98     0.65}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#  lower alpha upper     95\% confidence boundaries}
\DocumentationTok{\#\# 0.84 0.86 0.89 }
\DocumentationTok{\#\# }
\DocumentationTok{\#\#  Reliability if an item is dropped:}
\DocumentationTok{\#\#                               raw\_alpha std.alpha G6(smc) average\_r S/N}
\DocumentationTok{\#\# cs\_explain\_ideas\_imp               0.78      0.78    0.65      0.65 3.6}
\DocumentationTok{\#\# cs\_find\_clarification\_imp          0.76      0.76    0.62      0.62 3.2}
\DocumentationTok{\#\# cs\_learn\_different\_styles\_imp      0.88      0.88    0.79      0.79 7.4}
\DocumentationTok{\#\#                               alpha se var.r med.r}
\DocumentationTok{\#\# cs\_explain\_ideas\_imp             0.025    NA  0.65}
\DocumentationTok{\#\# cs\_find\_clarification\_imp        0.028    NA  0.62}
\DocumentationTok{\#\# cs\_learn\_different\_styles\_imp    0.014    NA  0.79}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#  Item statistics }
\DocumentationTok{\#\#                                 n raw.r std.r r.cor r.drop mean  sd}
\DocumentationTok{\#\# cs\_explain\_ideas\_imp          300  0.90  0.90  0.85   0.77  5.2 1.0}
\DocumentationTok{\#\# cs\_find\_clarification\_imp     300  0.91  0.91  0.87   0.79  5.1 1.1}
\DocumentationTok{\#\# cs\_learn\_different\_styles\_imp 300  0.86  0.85  0.71   0.67  4.8 1.2}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Non missing response frequency for each item}
\DocumentationTok{\#\#                                  1    2    3    4    5    6 miss}
\DocumentationTok{\#\# cs\_explain\_ideas\_imp          0.01 0.01 0.06 0.14 0.27 0.51    0}
\DocumentationTok{\#\# cs\_find\_clarification\_imp     0.01 0.02 0.05 0.14 0.30 0.47    0}
\DocumentationTok{\#\# cs\_learn\_different\_styles\_imp 0.01 0.03 0.09 0.19 0.31 0.36    0}

\NormalTok{gep }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"cs\_"}\NormalTok{) }\SpecialCharTok{\&} \FunctionTok{ends\_with}\NormalTok{(}\StringTok{"\_exp"}\NormalTok{), }\SpecialCharTok{{-}}\NormalTok{cs\_exp) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  psych}\SpecialCharTok{::}\FunctionTok{alpha}\NormalTok{()}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Reliability analysis   }
\DocumentationTok{\#\# Call: psych::alpha(x = .)}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#   raw\_alpha std.alpha G6(smc) average\_r S/N   ase mean  sd median\_r}
\DocumentationTok{\#\#       0.81      0.82    0.76       0.6 4.4 0.019  4.2 1.1     0.58}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#  lower alpha upper     95\% confidence boundaries}
\DocumentationTok{\#\# 0.78 0.81 0.85 }
\DocumentationTok{\#\# }
\DocumentationTok{\#\#  Reliability if an item is dropped:}
\DocumentationTok{\#\#                               raw\_alpha std.alpha G6(smc) average\_r S/N}
\DocumentationTok{\#\# cs\_explain\_ideas\_exp               0.69      0.69    0.53      0.53 2.3}
\DocumentationTok{\#\# cs\_find\_clarification\_exp          0.73      0.74    0.58      0.58 2.8}
\DocumentationTok{\#\# cs\_learn\_different\_styles\_exp      0.81      0.81    0.68      0.68 4.2}
\DocumentationTok{\#\#                               alpha se var.r med.r}
\DocumentationTok{\#\# cs\_explain\_ideas\_exp             0.035    NA  0.53}
\DocumentationTok{\#\# cs\_find\_clarification\_exp        0.031    NA  0.58}
\DocumentationTok{\#\# cs\_learn\_different\_styles\_exp    0.022    NA  0.68}
\DocumentationTok{\#\# }
\DocumentationTok{\#\#  Item statistics }
\DocumentationTok{\#\#                                 n raw.r std.r r.cor r.drop mean  sd}
\DocumentationTok{\#\# cs\_explain\_ideas\_exp          300  0.88  0.88  0.80   0.71  4.2 1.3}
\DocumentationTok{\#\# cs\_find\_clarification\_exp     300  0.85  0.86  0.76   0.68  4.5 1.2}
\DocumentationTok{\#\# cs\_learn\_different\_styles\_exp 300  0.84  0.82  0.67   0.61  4.0 1.4}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# Non missing response frequency for each item}
\DocumentationTok{\#\#                                  1    2    3    4    5    6 miss}
\DocumentationTok{\#\# cs\_explain\_ideas\_exp          0.04 0.09 0.11 0.33 0.26 0.17    0}
\DocumentationTok{\#\# cs\_find\_clarification\_exp     0.02 0.05 0.13 0.27 0.32 0.21    0}
\DocumentationTok{\#\# cs\_learn\_different\_styles\_exp 0.06 0.09 0.21 0.24 0.22 0.17    0}
\end{Highlighting}
\end{Shaded}

Similarly, to \texttt{si}, \texttt{cs\_imp} and \texttt{cs\_exp} show very good internal consistency scores too: \(\alpha_{cs\_imp} = 0.86\) and \(\alpha_{cs\_exp} = 0.81\). Based on these results we could be confident to use our latent variables for further analysis.

However, while Cronbach's \(\alpha\) is very popular due to its simplicity, there is plenty of criticism (add references). Therefore, it is often not enough to report the Cronbach's \(\alpha\), but undertake additional steps. Depending on the stage of development of your measurement instrument (i.e.~your questionnaire), you likely have to perform one of the following before computing the \(\alpha\) scores:

\begin{itemize}
\item
  Exploratory factor analysis (EFA): Generally used to identify latent variables in a set of questionnaire items.
\item
  Confirmatory factor analysis (CFA): To confirm whether a set of items truly reflect a latent variable.
\end{itemize}

An example of exploratory factor analysis is presented in Chapter @ref(). Since the \texttt{gep} data is based on an established measurement tool, we perform a CFA. To perform a CFA, we use the popular \texttt{lavaan} (\underline{La}tent \underline{Va}riable \underline{An}alysis) package. The steps of running a CFA in R include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Define which variables are supposed to measure a specific latent variable (i.e.~creating a model)
\item
  Run the CFA to see whether our model fits the data we collected
\item
  Interpret the results based on various indicators.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lavaan)}
\DocumentationTok{\#\# This is lavaan 0.6{-}9}
\DocumentationTok{\#\# lavaan is FREE software! Please report any bugs.}

\CommentTok{\#1: Define the model which explains how items relate to latent variables}
\NormalTok{model }\OtherTok{\textless{}{-}} \StringTok{\textquotesingle{}}
\StringTok{social\_integration =\textasciitilde{}}
\StringTok{si\_socialise\_with\_people\_exp +}
\StringTok{si\_supportive\_friends\_exp +}
\StringTok{si\_time\_socialising\_exp}

\StringTok{comm\_skills\_imp =\textasciitilde{}}
\StringTok{cs\_explain\_ideas\_imp +}
\StringTok{cs\_find\_clarification\_imp +}
\StringTok{cs\_learn\_different\_styles\_imp}

\StringTok{comm\_skills\_exp =\textasciitilde{}}
\StringTok{cs\_explain\_ideas\_exp +}
\StringTok{cs\_find\_clarification\_exp +}
\StringTok{cs\_learn\_different\_styles\_exp}
\StringTok{\textquotesingle{}}

\CommentTok{\#2: Run the CFA to see how well this model fits our data}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{cfa}\NormalTok{(model, }\AttributeTok{data =}\NormalTok{ gep)}

\CommentTok{\#3a: Extract the performance indicators}
\NormalTok{fit\_indices }\OtherTok{\textless{}{-}} \FunctionTok{fitmeasures}\NormalTok{(fit)}

\CommentTok{\#3b: We tidy the results with the \textquotesingle{}broom\textquotesingle{} package and pick only those indices we are most interested in}
\NormalTok{broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{(fit\_indices) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(names }\SpecialCharTok{==} \StringTok{"cfi"} \SpecialCharTok{|} 
\NormalTok{         names }\SpecialCharTok{==} \StringTok{"srmr"} \SpecialCharTok{|}
\NormalTok{         names }\SpecialCharTok{==} \StringTok{"rmsea"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{x =} \FunctionTok{round}\NormalTok{(x, }\DecValTok{3}\NormalTok{))         }\CommentTok{\# Round the results to 3 decimal places}
\DocumentationTok{\#\# \# A tibble: 3 x 2}
\DocumentationTok{\#\#   names x         }
\DocumentationTok{\#\#   \textless{}chr\textgreater{} \textless{}lvn.vctr\textgreater{}}
\DocumentationTok{\#\# 1 cfi   0.967     }
\DocumentationTok{\#\# 2 rmsea 0.081     }
\DocumentationTok{\#\# 3 srmr  0.037}
\end{Highlighting}
\end{Shaded}

The \texttt{broom} package is useful to clean output from all kinds of models, such as a CFA model. It allows us to convert the raw output into a \texttt{tibble}, which we can further manipulate using the functions we already know. If you want to know where \texttt{names} and \texttt{x} came from we have to inspect the output from \texttt{broom::tidy(fit\_indices)}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{(fit\_indices)}
\DocumentationTok{\#\# \# A tibble: 42 x 2}
\DocumentationTok{\#\#    names           x           }
\DocumentationTok{\#\#    \textless{}chr\textgreater{}           \textless{}lvn.vctr\textgreater{}  }
\DocumentationTok{\#\#  1 npar            2.100000e+01}
\DocumentationTok{\#\#  2 fmin            1.177732e{-}01}
\DocumentationTok{\#\#  3 chisq           7.066390e+01}
\DocumentationTok{\#\#  4 df              2.400000e+01}
\DocumentationTok{\#\#  5 pvalue          1.733810e{-}06}
\DocumentationTok{\#\#  6 baseline.chisq  1.429729e+03}
\DocumentationTok{\#\#  7 baseline.df     3.600000e+01}
\DocumentationTok{\#\#  8 baseline.pvalue 0.000000e+00}
\DocumentationTok{\#\#  9 cfi             9.665187e{-}01}
\DocumentationTok{\#\# 10 tli             9.497780e{-}01}
\DocumentationTok{\#\# \# ... with 32 more rows}
\end{Highlighting}
\end{Shaded}

These column names were generated when we called the function \texttt{tidy()}. I often find myself working through chains of analytical steps iteratively to see what the intermediary steps produce. This also makes it easier to spot any mistake early on. Therefore, I recommend slowly building up your \texttt{dplyr} chains of function calls, especially when you just started learning R and the \texttt{tidyverse} approach of data analysis.

The results of our CFA appear fairly promising:

\begin{itemize}
\item
  The \texttt{cfi} (Comparative Fit Index) lies above 0.95 and
\item
  the \texttt{srmr} (Standardised, Root Mean Square Residual) lies well below 0.08.
\item
  The \texttt{rmsea} (Root Mean Square Error of Approximation) appears slightly higher than desirable, i.e.~below 0.6. \citep{hu-bentler-1999cutoff}
\end{itemize}

Overall, however, the model seems to suggest a good fit with our data. Combined with the computed Cronbach's \(\alpha\), we can be reasonably confident in our latent variables and perform further analytical steps, for example, as shown in Chapter \ref{regression}.

\hypertarget{conclusion-data-wrangling}{%
\section{Once you finished with data wrangling}\label{conclusion-data-wrangling}}

Once you finished your data cleaning, I recommend writing (i.e.~exporting) your cleaned data out of R into your `01\_tidy\_data' folder. You can then use this dataset to continue with your analysis. This way, you do not have to run all your data wrangling code every time you open your R project. To write your tidy dataset onto your hard drive of choice, we can use \texttt{readr} in the same as we did at the \protect\hyperlink{importing-data-using-functions}{beginning}. However, instead of \texttt{read\_csv()} we have to use another function that writes the file into a folder. The function \texttt{write\_csv()} first takes the object we want to save and then the folder and file name. We only made changes to the \texttt{wvs} dataset, so we should save it to our hard drive.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write\_csv}\NormalTok{(wvs, }\StringTok{"01\_tidy\_data/wvs\_tidy.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This chapter has been a reasonably long one. Nonetheless, it only covered the basics of what can and should be done when preparing data for analysis. These steps should not be rushed or even skipped. It is essential to have the data cleaned appropriately. This process helps you familiarise yourself with the dataset in great depth, and it makes you aware of limitations or even problems of your data. In the spirit of \href{https://osf.io}{Open Science}, using R also helps to document the steps you undertook to get from a raw dataset to a tidy one. This is also beneficial if you intend to publish your work later. Reproducibility has become an essential aspect of transparent and impactful research and should be a guiding principle of any empirical research.

Now that we have learned the basics of data wrangling (and there might be more waiting for you in the coming chapters), we can finally start our data analysis.

\hypertarget{descriptive-statistics}{%
\chapter{Descriptive Statistics}\label{descriptive-statistics}}

\hypertarget{correlations}{%
\chapter{Correlations}\label{correlations}}

\hypertarget{comparing-groups}{%
\chapter{Comparing groups}\label{comparing-groups}}

\hypertarget{regression}{%
\chapter{Regression: Creating models to predict future observations}\label{regression}}

\hypertarget{mixed-methods-research-analysing-qualitative-in-r}{%
\chapter{Mixed-methods research: Analysing qualitative in R}\label{mixed-methods-research-analysing-qualitative-in-r}}

\hypertarget{next-steps}{%
\chapter{Where to go from here: The next steps in your R journey}\label{next-steps}}

\hypertarget{next-steps-github}{%
\section{GitHub: A Gateway to even more ingenious R packages}\label{next-steps-github}}

\hypertarget{next-steps-books}{%
\section{Books to read and expand your knowledge}\label{next-steps-books}}

\hypertarget{next-steps-online-readings}{%
\section{Engage in regular online readings about R}\label{next-steps-online-readings}}

\begin{itemize}
\tightlist
\item
  Tidyverse Blog
\item
  R-blogger
\end{itemize}

\hypertarget{next-steps-twitter}{%
\section{Join the Twitter community and hone your skills}\label{next-steps-twitter}}

\begin{itemize}
\tightlist
\item
  \#RStats
\item
  TidyTuesday
\end{itemize}

\hypertarget{exercises-solutions}{%
\chapter{Exercises: Solutions}\label{exercises-solutions}}

\hypertarget{exercises-solutions-5}{%
\section{Solutions for \ref{exercises-chapter-5}}\label{exercises-solutions-5}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#1 {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\FunctionTok{sqrt}\NormalTok{(}\DecValTok{25{-}16}\NormalTok{)}\SpecialCharTok{+}\DecValTok{2}\SpecialCharTok{*}\DecValTok{8{-}6}
\DocumentationTok{\#\# [1] 13}

\CommentTok{\#2 {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\StringTok{"Five"} \SpecialCharTok{==} \DecValTok{5}
\DocumentationTok{\#\# [1] FALSE}

\CommentTok{\#3 {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{books }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\StringTok{"Harry Potter and the Deathly Hallows"}\NormalTok{,}
              \StringTok{"The Alchemist"}\NormalTok{,}
              \StringTok{"The Davinci Code"}\NormalTok{,}
              \StringTok{"R For Dummies"}\NormalTok{)}

\NormalTok{books}
\DocumentationTok{\#\# [[1]]}
\DocumentationTok{\#\# [1] "Harry Potter and the Deathly Hallows"}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# [[2]]}
\DocumentationTok{\#\# [1] "The Alchemist"}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# [[3]]}
\DocumentationTok{\#\# [1] "The Davinci Code"}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# [[4]]}
\DocumentationTok{\#\# [1] "R For Dummies"}

\CommentTok{\# 4 {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{x\_x }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(number1, number2)\{       }\CommentTok{\# Creates a function that takes 2 arguments}
\NormalTok{  result1 }\OtherTok{\textless{}{-}}\NormalTok{ number1}\SpecialCharTok{*}\NormalTok{number2             }\CommentTok{\# Result1 multiplies the two arguments}
\NormalTok{  result2 }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(number1)               }\CommentTok{\# Result2 computes the squareroot of the 1st argument}
\NormalTok{  result3 }\OtherTok{\textless{}{-}}\NormalTok{ number1}\SpecialCharTok{{-}}\NormalTok{number2             }\CommentTok{\# Result3 subtracts the 2nd argument from the 1st}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{c}\NormalTok{(result1, result2, result3))   }\CommentTok{\# Prints all three results into the console}
\NormalTok{\} }

\FunctionTok{x\_x}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\DocumentationTok{\#\# [1]  6.000000  1.414214 {-}1.000000}

\CommentTok{\# 5 {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}

\CommentTok{\# First, install the package.}

\CommentTok{\# Second, load the package with \textquotesingle{}library()\textquotesingle{}}
\CommentTok{\#         or use \textquotesingle{}::\textquotesingle{} to load only a particular function once.}

\CommentTok{\# Third, call the function you are interested in.}
\end{Highlighting}
\end{Shaded}


  \bibliography{book.bib,packages.bib}

\end{document}

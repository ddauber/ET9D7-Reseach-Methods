<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>12 Regression: Creating models to predict future observations | R for Non-Programmers: A Guide for Social Scientists</title>
  <meta name="description" content="This books is intended for Social Scientists. No matter whether you have years of experiences, this book might come in handy if you never use R or any other programming language before." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="12 Regression: Creating models to predict future observations | R for Non-Programmers: A Guide for Social Scientists" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This books is intended for Social Scientists. No matter whether you have years of experiences, this book might come in handy if you never use R or any other programming language before." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="12 Regression: Creating models to predict future observations | R for Non-Programmers: A Guide for Social Scientists" />
  
  <meta name="twitter:description" content="This books is intended for Social Scientists. No matter whether you have years of experiences, this book might come in handy if you never use R or any other programming language before." />
  

<meta name="author" content="Daniel Dauber" />


<meta name="date" content="2021-09-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="comparing-groups.html"/>
<link rel="next" href="mixed-methods-research-analysing-qualitative-in-r.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R for Non-Programmers</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome üëã</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments üôè</a></li>
<li class="chapter" data-level="1" data-path="readme-before-you-get-started.html"><a href="readme-before-you-get-started.html"><i class="fa fa-check"></i><b>1</b> <code>Readme.</code> Before you get started</a>
<ul>
<li class="chapter" data-level="1.1" data-path="readme-before-you-get-started.html"><a href="readme-before-you-get-started.html#a-starting-point-and-reference-book"><i class="fa fa-check"></i><b>1.1</b> A starting point and reference book</a></li>
<li class="chapter" data-level="1.2" data-path="readme-before-you-get-started.html"><a href="readme-before-you-get-started.html#download-the-companion-r-package"><i class="fa fa-check"></i><b>1.2</b> Download the companion R package</a></li>
<li class="chapter" data-level="1.3" data-path="readme-before-you-get-started.html"><a href="readme-before-you-get-started.html#a-tidyverse-approach-with-some-basic-r"><i class="fa fa-check"></i><b>1.3</b> A ‚Äòtidyverse‚Äô approach with some basic R</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="why-learn-a-programming-language-as-a-non-programmer.html"><a href="why-learn-a-programming-language-as-a-non-programmer.html"><i class="fa fa-check"></i><b>2</b> Why learn a programming language as a non-programmer?</a>
<ul>
<li class="chapter" data-level="2.1" data-path="why-learn-a-programming-language-as-a-non-programmer.html"><a href="why-learn-a-programming-language-as-a-non-programmer.html#learning-new-tools-to-analyse-your-data-is-always-essential"><i class="fa fa-check"></i><b>2.1</b> Learning new tools to analyse your data is always essential</a></li>
<li class="chapter" data-level="2.2" data-path="why-learn-a-programming-language-as-a-non-programmer.html"><a href="why-learn-a-programming-language-as-a-non-programmer.html#programming-languages-enhance-your-conceptual-thinking"><i class="fa fa-check"></i><b>2.2</b> Programming languages enhance your conceptual thinking</a></li>
<li class="chapter" data-level="2.3" data-path="why-learn-a-programming-language-as-a-non-programmer.html"><a href="why-learn-a-programming-language-as-a-non-programmer.html#programming-languages-allow-you-to-look-at-your-data-from-a-different-angle"><i class="fa fa-check"></i><b>2.3</b> Programming languages allow you to look at your data from a different angle</a></li>
<li class="chapter" data-level="2.4" data-path="why-learn-a-programming-language-as-a-non-programmer.html"><a href="why-learn-a-programming-language-as-a-non-programmer.html#learning-any-programming-language-will-help-you-learn-other-programming-languages."><i class="fa fa-check"></i><b>2.4</b> Learning any programming language will help you learn other programming languages.</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="setting-up-r-and-rstudio.html"><a href="setting-up-r-and-rstudio.html"><i class="fa fa-check"></i><b>3</b> Setting up R and RStudio</a>
<ul>
<li class="chapter" data-level="3.1" data-path="setting-up-r-and-rstudio.html"><a href="setting-up-r-and-rstudio.html#installing-r"><i class="fa fa-check"></i><b>3.1</b> Installing R</a></li>
<li class="chapter" data-level="3.2" data-path="setting-up-r-and-rstudio.html"><a href="setting-up-r-and-rstudio.html#installing-rstudio"><i class="fa fa-check"></i><b>3.2</b> Installing RStudio</a></li>
<li class="chapter" data-level="3.3" data-path="setting-up-r-and-rstudio.html"><a href="setting-up-r-and-rstudio.html#when-you-first-start-rstudio"><i class="fa fa-check"></i><b>3.3</b> When you first start RStudio</a></li>
<li class="chapter" data-level="3.4" data-path="setting-up-r-and-rstudio.html"><a href="setting-up-r-and-rstudio.html#updating-r-and-rstudio"><i class="fa fa-check"></i><b>3.4</b> Updating R and RStudio: Living at the pulse of innovation</a></li>
<li class="chapter" data-level="3.5" data-path="setting-up-r-and-rstudio.html"><a href="setting-up-r-and-rstudio.html#rstudio-cloud"><i class="fa fa-check"></i><b>3.5</b> RStudio Cloud</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-rstudio-interface.html"><a href="the-rstudio-interface.html"><i class="fa fa-check"></i><b>4</b> The RStudio Interface</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-rstudio-interface.html"><a href="the-rstudio-interface.html#the-console-window"><i class="fa fa-check"></i><b>4.1</b> The Console window</a></li>
<li class="chapter" data-level="4.2" data-path="the-rstudio-interface.html"><a href="the-rstudio-interface.html#the-source-window"><i class="fa fa-check"></i><b>4.2</b> The Source window</a></li>
<li class="chapter" data-level="4.3" data-path="the-rstudio-interface.html"><a href="the-rstudio-interface.html#the-environment-history-connections-tutorial-window"><i class="fa fa-check"></i><b>4.3</b> The Environment / History / Connections / Tutorial window</a></li>
<li class="chapter" data-level="4.4" data-path="the-rstudio-interface.html"><a href="the-rstudio-interface.html#the-files-plots-packages-help-viewer-window"><i class="fa fa-check"></i><b>4.4</b> The Files / Plots / Packages / Help / Viewer window</a></li>
<li class="chapter" data-level="4.5" data-path="the-rstudio-interface.html"><a href="the-rstudio-interface.html#customise-your-user-interface"><i class="fa fa-check"></i><b>4.5</b> Customise your user interface</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="r-basics-the-very-fundamentals.html"><a href="r-basics-the-very-fundamentals.html"><i class="fa fa-check"></i><b>5</b> R Basics: The very fundamentals</a>
<ul>
<li class="chapter" data-level="5.1" data-path="r-basics-the-very-fundamentals.html"><a href="r-basics-the-very-fundamentals.html#basic-computations-in-r"><i class="fa fa-check"></i><b>5.1</b> Basic computations in R</a></li>
<li class="chapter" data-level="5.2" data-path="r-basics-the-very-fundamentals.html"><a href="r-basics-the-very-fundamentals.html#assigning-values-to-objects"><i class="fa fa-check"></i><b>5.2</b> Assigning values to objects: ‚Äò&lt;-‚Äô</a></li>
<li class="chapter" data-level="5.3" data-path="r-basics-the-very-fundamentals.html"><a href="r-basics-the-very-fundamentals.html#functions"><i class="fa fa-check"></i><b>5.3</b> Functions</a></li>
<li class="chapter" data-level="5.4" data-path="r-basics-the-very-fundamentals.html"><a href="r-basics-the-very-fundamentals.html#r-packages"><i class="fa fa-check"></i><b>5.4</b> R packages</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="r-basics-the-very-fundamentals.html"><a href="r-basics-the-very-fundamentals.html#installing-packages-using-a-function"><i class="fa fa-check"></i><b>5.4.1</b> Installing packages using <code>install.packages()</code></a></li>
<li class="chapter" data-level="5.4.2" data-path="r-basics-the-very-fundamentals.html"><a href="r-basics-the-very-fundamentals.html#installing-packages-via-rstudio"><i class="fa fa-check"></i><b>5.4.2</b> Installing packages via RStudio‚Äôs package pane</a></li>
<li class="chapter" data-level="5.4.3" data-path="r-basics-the-very-fundamentals.html"><a href="r-basics-the-very-fundamentals.html#using-r-packages"><i class="fa fa-check"></i><b>5.4.3</b> Using R Packages</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="r-basics-the-very-fundamentals.html"><a href="r-basics-the-very-fundamentals.html#coding-etiquette"><i class="fa fa-check"></i><b>5.5</b> Coding etiquette</a></li>
<li class="chapter" data-level="5.6" data-path="r-basics-the-very-fundamentals.html"><a href="r-basics-the-very-fundamentals.html#exercises-chapter-5"><i class="fa fa-check"></i><b>5.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="starting-your-r-projects.html"><a href="starting-your-r-projects.html"><i class="fa fa-check"></i><b>6</b> Starting your R projects</a>
<ul>
<li class="chapter" data-level="6.1" data-path="starting-your-r-projects.html"><a href="starting-your-r-projects.html#creating-an-r-project"><i class="fa fa-check"></i><b>6.1</b> Creating an R Project file</a></li>
<li class="chapter" data-level="6.2" data-path="starting-your-r-projects.html"><a href="starting-your-r-projects.html#organising-your-projects"><i class="fa fa-check"></i><b>6.2</b> Organising your projects</a></li>
<li class="chapter" data-level="6.3" data-path="starting-your-r-projects.html"><a href="starting-your-r-projects.html#creating-an-r-script"><i class="fa fa-check"></i><b>6.3</b> Creating an R Script</a></li>
<li class="chapter" data-level="6.4" data-path="starting-your-r-projects.html"><a href="starting-your-r-projects.html#r-markdown-and-r-notebooks"><i class="fa fa-check"></i><b>6.4</b> Using R Markdown</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i><b>7</b> Data Wrangling</a>
<ul>
<li class="chapter" data-level="7.1" data-path="data-wrangling.html"><a href="data-wrangling.html#import-your-data"><i class="fa fa-check"></i><b>7.1</b> Import your data</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="data-wrangling.html"><a href="data-wrangling.html#import-data-from-the-files-pane"><i class="fa fa-check"></i><b>7.1.1</b> Import data from the Files pane</a></li>
<li class="chapter" data-level="7.1.2" data-path="data-wrangling.html"><a href="data-wrangling.html#importing-data-from-the-environment-pane"><i class="fa fa-check"></i><b>7.1.2</b> Importing data from the Environment pane</a></li>
<li class="chapter" data-level="7.1.3" data-path="data-wrangling.html"><a href="data-wrangling.html#importing-data-using-functions"><i class="fa fa-check"></i><b>7.1.3</b> Importing data using functions directly</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="data-wrangling.html"><a href="data-wrangling.html#inspecting-raw-data"><i class="fa fa-check"></i><b>7.2</b> Inspecting your data</a></li>
<li class="chapter" data-level="7.3" data-path="data-wrangling.html"><a href="data-wrangling.html#colnames-cleaning"><i class="fa fa-check"></i><b>7.3</b> Cleaning your column names: Call the <code>janitor</code></a></li>
<li class="chapter" data-level="7.4" data-path="data-wrangling.html"><a href="data-wrangling.html#change-data-types"><i class="fa fa-check"></i><b>7.4</b> Data types: What are they and how can you change them</a></li>
<li class="chapter" data-level="7.5" data-path="data-wrangling.html"><a href="data-wrangling.html#handling-factors"><i class="fa fa-check"></i><b>7.5</b> Handling factors</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="data-wrangling.html"><a href="data-wrangling.html#recoding-factors"><i class="fa fa-check"></i><b>7.5.1</b> Recoding factors</a></li>
<li class="chapter" data-level="7.5.2" data-path="data-wrangling.html"><a href="data-wrangling.html#reordering-factor-levels"><i class="fa fa-check"></i><b>7.5.2</b> Reordering factor levels</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="data-wrangling.html"><a href="data-wrangling.html#dealing-with-missing-data"><i class="fa fa-check"></i><b>7.6</b> Dealing with missing data</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="data-wrangling.html"><a href="data-wrangling.html#mapping-missing-data"><i class="fa fa-check"></i><b>7.6.1</b> Mapping missing data</a></li>
<li class="chapter" data-level="7.6.2" data-path="data-wrangling.html"><a href="data-wrangling.html#patterns-of-missing-data"><i class="fa fa-check"></i><b>7.6.2</b> Identifying patterns of missing data</a></li>
<li class="chapter" data-level="7.6.3" data-path="data-wrangling.html"><a href="data-wrangling.html#replacing-removing-missing-data"><i class="fa fa-check"></i><b>7.6.3</b> Replacing or removing missing data</a></li>
<li class="chapter" data-level="7.6.4" data-path="data-wrangling.html"><a href="data-wrangling.html#main-takeaways-regarding-dealing-with-missing-data"><i class="fa fa-check"></i><b>7.6.4</b> Main takeaways regarding dealing with missing data</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="data-wrangling.html"><a href="data-wrangling.html#latent-constructs"><i class="fa fa-check"></i><b>7.7</b> Latent constructs and their reliability</a></li>
<li class="chapter" data-level="7.8" data-path="data-wrangling.html"><a href="data-wrangling.html#conclusion-data-wrangling"><i class="fa fa-check"></i><b>7.8</b> Once you finished with data wrangling</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html"><i class="fa fa-check"></i><b>8</b> Descriptive Statistics</a>
<ul>
<li class="chapter" data-level="8.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#plotting-in-r-with-ggplot2"><i class="fa fa-check"></i><b>8.1</b> Plotting in R with <code>ggplot2</code></a></li>
<li class="chapter" data-level="8.2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>8.2</b> Central tendency measures: Mean, Median, Mode</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#mean"><i class="fa fa-check"></i><b>8.2.1</b> Mean</a></li>
<li class="chapter" data-level="8.2.2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#median"><i class="fa fa-check"></i><b>8.2.2</b> Median</a></li>
<li class="chapter" data-level="8.2.3" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#mode"><i class="fa fa-check"></i><b>8.2.3</b> Mode</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#spread-of-data"><i class="fa fa-check"></i><b>8.3</b> Indicators and visualisations to examine the spread of data</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#boxplot-so-much-information-in-just-one-box"><i class="fa fa-check"></i><b>8.3.1</b> Boxplot: So much information in just one box</a></li>
<li class="chapter" data-level="8.3.2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#histogram-do-not-mistake-it-as-a-bar-plot"><i class="fa fa-check"></i><b>8.3.2</b> Histogram: Do not mistake it as a bar plot</a></li>
<li class="chapter" data-level="8.3.3" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#density-plots-your-smooth-histograms"><i class="fa fa-check"></i><b>8.3.3</b> Density plots: Your smooth histograms</a></li>
<li class="chapter" data-level="8.3.4" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#violin-plot-your-smooth-boxplot"><i class="fa fa-check"></i><b>8.3.4</b> Violin plot: Your smooth boxplot</a></li>
<li class="chapter" data-level="8.3.5" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#qq-plot"><i class="fa fa-check"></i><b>8.3.5</b> QQ plot: A ‚Äòcute‚Äô plot to check for normality in your data</a></li>
<li class="chapter" data-level="8.3.6" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#standard-deviation"><i class="fa fa-check"></i><b>8.3.6</b> Standard deviation: Your average deviation from the mean</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="sources-of-bias.html"><a href="sources-of-bias.html"><i class="fa fa-check"></i><b>9</b> Sources of bias: Outliers, normality and other ‚Äòconundrums‚Äô</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sources-of-bias.html"><a href="sources-of-bias.html#additivity-and-linearity"><i class="fa fa-check"></i><b>9.1</b> Linearity and additivity</a></li>
<li class="chapter" data-level="9.2" data-path="sources-of-bias.html"><a href="sources-of-bias.html#independence"><i class="fa fa-check"></i><b>9.2</b> Independence</a></li>
<li class="chapter" data-level="9.3" data-path="sources-of-bias.html"><a href="sources-of-bias.html#normality"><i class="fa fa-check"></i><b>9.3</b> Normality</a></li>
<li class="chapter" data-level="9.4" data-path="sources-of-bias.html"><a href="sources-of-bias.html#homogeneity-of-variance"><i class="fa fa-check"></i><b>9.4</b> Homogeneity of variance (homoscedasticity)</a></li>
<li class="chapter" data-level="9.5" data-path="sources-of-bias.html"><a href="sources-of-bias.html#dealing-with-outliers"><i class="fa fa-check"></i><b>9.5</b> Outliers and how to deal with them</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="sources-of-bias.html"><a href="sources-of-bias.html#ouliers-standard_deviation"><i class="fa fa-check"></i><b>9.5.1</b> Detecting outliers using the standard deviation</a></li>
<li class="chapter" data-level="9.5.2" data-path="sources-of-bias.html"><a href="sources-of-bias.html#outliers-iqr"><i class="fa fa-check"></i><b>9.5.2</b> Detecting outliers using the interquartile range</a></li>
<li class="chapter" data-level="9.5.3" data-path="sources-of-bias.html"><a href="sources-of-bias.html#removing-or-replacing-outliers"><i class="fa fa-check"></i><b>9.5.3</b> Removing or replacing outliers</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="correlations.html"><a href="correlations.html"><i class="fa fa-check"></i><b>10</b> Correlations</a>
<ul>
<li class="chapter" data-level="10.1" data-path="correlations.html"><a href="correlations.html#parametric-or-non-parametric"><i class="fa fa-check"></i><b>10.1</b> Parametric or non-parametric: That is the question</a></li>
<li class="chapter" data-level="10.2" data-path="correlations.html"><a href="correlations.html#plotting-correlations"><i class="fa fa-check"></i><b>10.2</b> Plotting correlations</a></li>
<li class="chapter" data-level="10.3" data-path="correlations.html"><a href="correlations.html#significance"><i class="fa fa-check"></i><b>10.3</b> Significance: A way to help you judge your findings</a></li>
<li class="chapter" data-level="10.4" data-path="correlations.html"><a href="correlations.html#limitations-of-correlations"><i class="fa fa-check"></i><b>10.4</b> Limitations of correlations</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="correlations.html"><a href="correlations.html#correlations-are-not-causal-relationships"><i class="fa fa-check"></i><b>10.4.1</b> Correlations are not causal relationships</a></li>
<li class="chapter" data-level="10.4.2" data-path="correlations.html"><a href="correlations.html#correlations-can-be-spurious"><i class="fa fa-check"></i><b>10.4.2</b> Correlations can be spurious</a></li>
<li class="chapter" data-level="10.4.3" data-path="correlations.html"><a href="correlations.html#simpsons-paradox"><i class="fa fa-check"></i><b>10.4.3</b> Simpson‚Äôs Paradox: When correlations betray you</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="comparing-groups.html"><a href="comparing-groups.html"><i class="fa fa-check"></i><b>11</b> Comparing groups</a>
<ul>
<li class="chapter" data-level="11.1" data-path="comparing-groups.html"><a href="comparing-groups.html#comparability-apples-vs-oranges"><i class="fa fa-check"></i><b>11.1</b> Comparability: Apples vs Oranges</a></li>
<li class="chapter" data-level="11.2" data-path="comparing-groups.html"><a href="comparing-groups.html#comparing-two-groups"><i class="fa fa-check"></i><b>11.2</b> Comparing two groups</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="comparing-groups.html"><a href="comparing-groups.html#two-unpaired-groups"><i class="fa fa-check"></i><b>11.2.1</b> Two Unpaired groups</a></li>
<li class="chapter" data-level="11.2.2" data-path="comparing-groups.html"><a href="comparing-groups.html#two-paired-groups"><i class="fa fa-check"></i><b>11.2.2</b> Two Paired groups</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="comparing-groups.html"><a href="comparing-groups.html#comparing-more-than-two-groups"><i class="fa fa-check"></i><b>11.3</b> Comparing more than two groups</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="comparing-groups.html"><a href="comparing-groups.html#multiple-unpaired-groups"><i class="fa fa-check"></i><b>11.3.1</b> Multiple unpaired groups</a></li>
<li class="chapter" data-level="11.3.2" data-path="comparing-groups.html"><a href="comparing-groups.html#multiple-paired-groups"><i class="fa fa-check"></i><b>11.3.2</b> Multiple paired groups</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="comparing-groups.html"><a href="comparing-groups.html#chi-squared-test"><i class="fa fa-check"></i><b>11.4</b> Comparing groups based on factors: Contingency tables</a></li>
<li class="chapter" data-level="11.5" data-path="comparing-groups.html"><a href="comparing-groups.html#cheatsheet-group-comparisons"><i class="fa fa-check"></i><b>11.5</b> A cheatsheet to guide your own group comparisons</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>12</b> Regression: Creating models to predict future observations</a>
<ul>
<li class="chapter" data-level="12.1" data-path="regression.html"><a href="regression.html#single-linear-regression"><i class="fa fa-check"></i><b>12.1</b> Single linear regression</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="regression.html"><a href="regression.html#fitting-a-regression-model-by-hand"><i class="fa fa-check"></i><b>12.1.1</b> Fitting a regression model by hand, i.e.¬†trial and error</a></li>
<li class="chapter" data-level="12.1.2" data-path="regression.html"><a href="regression.html#fitting-a-regression-model-computationally"><i class="fa fa-check"></i><b>12.1.2</b> Fitting a regression model computationally</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="regression.html"><a href="regression.html#multiple-regression"><i class="fa fa-check"></i><b>12.2</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="regression.html"><a href="regression.html#outliers-in-multiple-regressions"><i class="fa fa-check"></i><b>12.2.1</b> Outliers in multiple regressions</a></li>
<li class="chapter" data-level="12.2.2" data-path="regression.html"><a href="regression.html#standardised-beta-beta-vs.-unstandardised-beta-b"><i class="fa fa-check"></i><b>12.2.2</b> Standardised beta (<span class="math inline">\(\beta\)</span>) vs.¬†unstandardised beta (<span class="math inline">\(B\)</span>)</a></li>
<li class="chapter" data-level="12.2.3" data-path="regression.html"><a href="regression.html#multicollinearity-the-dilemma-of-highly-correlated-independent-variables"><i class="fa fa-check"></i><b>12.2.3</b> Multicollinearity: The dilemma of highly correlated independent variables</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="regression.html"><a href="regression.html#hierarchical-regression"><i class="fa fa-check"></i><b>12.3</b> Hierarchical regression</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="regression.html"><a href="regression.html#regressions-with-control-variables"><i class="fa fa-check"></i><b>12.3.1</b> Regressions with control variables</a></li>
<li class="chapter" data-level="12.3.2" data-path="regression.html"><a href="regression.html#moderated-regression"><i class="fa fa-check"></i><b>12.3.2</b> Moderated regression</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="regression.html"><a href="regression.html#other-regression-models-alternatives-to-ols"><i class="fa fa-check"></i><b>12.4</b> Other regression models: Alternatives to OLS</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="mixed-methods-research-analysing-qualitative-in-r.html"><a href="mixed-methods-research-analysing-qualitative-in-r.html"><i class="fa fa-check"></i><b>13</b> Mixed-methods research: Analysing qualitative in R</a></li>
<li class="chapter" data-level="14" data-path="next-steps.html"><a href="next-steps.html"><i class="fa fa-check"></i><b>14</b> Where to go from here: The next steps in your R journey</a>
<ul>
<li class="chapter" data-level="14.1" data-path="next-steps.html"><a href="next-steps.html#next-steps-github"><i class="fa fa-check"></i><b>14.1</b> GitHub: A Gateway to even more ingenious R packages</a></li>
<li class="chapter" data-level="14.2" data-path="next-steps.html"><a href="next-steps.html#next-steps-books"><i class="fa fa-check"></i><b>14.2</b> Books to read and expand your knowledge</a></li>
<li class="chapter" data-level="14.3" data-path="next-steps.html"><a href="next-steps.html#next-steps-online-readings"><i class="fa fa-check"></i><b>14.3</b> Engage in regular online readings about R</a></li>
<li class="chapter" data-level="14.4" data-path="next-steps.html"><a href="next-steps.html#next-steps-twitter"><i class="fa fa-check"></i><b>14.4</b> Join the Twitter community and hone your skills</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="case-studies-in-depth-analysis.html"><a href="case-studies-in-depth-analysis.html"><i class="fa fa-check"></i><b>15</b> Case studies: In-depth analysis</a>
<ul>
<li class="chapter" data-level="15.1" data-path="case-studies-in-depth-analysis.html"><a href="case-studies-in-depth-analysis.html#bootstrapped-regression-with-multi-level-control-variable-and-moderation"><i class="fa fa-check"></i><b>15.1</b> Bootstrapped regression with multi-level control variable and moderation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a>
<ul>
<li class="chapter" data-level="15.2" data-path="appendix.html"><a href="appendix.html#comparing-two-unpaired-groups"><i class="fa fa-check"></i><b>15.2</b> Comparing two unpaired groups</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="exercises-solutions.html"><a href="exercises-solutions.html"><i class="fa fa-check"></i><b>16</b> Exercises: Solutions</a>
<ul>
<li class="chapter" data-level="16.1" data-path="exercises-solutions.html"><a href="exercises-solutions.html#exercises-solutions-5"><i class="fa fa-check"></i><b>16.1</b> Solutions for 5.6</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://twitter.com/daniel_dauber" target="blank">Find me on twitter!</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R for Non-Programmers: A Guide for Social Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression" class="section level1" number="12">
<h1><span class="header-section-number">12</span> Regression: Creating models to predict future observations</h1>
<p>Regressions are an interesting area of data analysis since it enables us to make very specific predictions about the future incorporating different variables at the same time. As the name implies, regressions ‚Äòregress,‚Äô i.e.¬†they draw on past observations to make predictions about future observations. Thus, any analysis incorporating a regression makes the implicit assumption that the future can be best explained by the past.</p>
<p>I once heard someone refer to regressions as driving a car by looking at the rear-view mirror. As long as the road is straight, we will be able to successfully navigate the car. However, if there is a sudden turn, we might drive into the abyss. This makes it very clear when and how regressions are can be helpful.</p>
<p>Regressions are also a machine learning method, which falls under models with supervised learning. If you find machine learning fascinating, you might find the book ‚ÄúHands-on Machine Learning with R‚Äù <span class="citation">(<a href="#ref-boehmke2019hands" role="doc-biblioref">Boehmke and Greenwell 2019</a>)</span> very insightful and interesting.</p>
<p>In the following chapters we will cover three common types of regressions, which are also known as Ordinary Least Squares (OLS) regression models:</p>
<ul>
<li><p>Single linear regression</p></li>
<li><p>Multiple regression</p></li>
<li><p>Hierarchical regression</p></li>
</ul>
<p>These three types will allow you to perform any OLS regression you could think of. We can further distinguish two approaches to modelling via regressions:</p>
<ul>
<li><p><em>hypothesis testing</em>: A regression model is defined ex-ante.</p></li>
<li><p><em>machine learning</em>: A model is developed based on empirical data.</p></li>
</ul>
<p>In the following chapters we will slightly blur the lines between both approaches by making assumptions about relationships (hypothesising) and make informed decisions based on our data (exploring).</p>
<p>All our regressions will be performed using the <code>covid</code> dataset of the <code>r4np</code> package to investigate whether certain factors can predict COVID numbers in different countries. I felt, this book would not have been complete without covering this topic. After all, I wrote it during the pandemic and it likely will mark a dark chapter in human history.</p>
<div id="single-linear-regression" class="section level2" number="12.1">
<h2><span class="header-section-number">12.1</span> Single linear regression</h2>
<p>A single linear regression looks very similar to a correlation (see Chapter <a href="correlations.html#correlations">10</a>, but it is different in that it defines which variable affects another variable, i.e.¬†a single direction relationship. I used the terms dependent variable (DV) and independent variable (IV) previously when comparing groups (see Chapter <a href="comparing-groups.html#comparing-groups">11</a>, and we will use them here again. In group comparisons, the independent variable was usually a <code>factor</code>, but in regressions we can use data that is not a categorical variable, i.e.¬†<code>integer</code>, <code>double</code>, etc.</p>
<p>While I understand that mathematical equations can be confusing, with regressions, they are fairly simple to understand. Also, when writing our models in R, we will continuously use a <code>formula</code> to specify our regression. A single linear regression consists of one independent variable and one dependent variable:</p>
<div align="center">
<p><span class="math display">\[
DV = \beta_{0} + IV * \beta_{1} + error
\]</span></p>
</div>
<p>Beta (<span class="math inline">\(\beta\)</span>) represents the coefficient of the independent variable, i.e.¬†how much a change in IV causes a change in DV. For example, a one unit change in the IV might mean that the DV changes by two units of IV:</p>
<div id="single-linear-regression-example" align="center">
<p><span class="math display">\[
DV = \beta_0 + 2 * IV + error
\]</span></p>
</div>
<p>If we ignore <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(error\)</span> for a moment, we find that that if <span class="math inline">\(IV = 3\)</span>, our <span class="math inline">\(DV = 2*3 = 6\)</span>. Therefore, if <span class="math inline">\(IV = 5\)</span>, we find that <span class="math inline">\(DV = 10\)</span>, and so on. According to this model, DV will always be twice as large as IV.</p>
<p>You might be wondering what <span class="math inline">\(\beta_0\)</span> stands for. It indicates an offset for each value, also called the intercept. Thus, no matter which value we choose for IV, DV will always be <span class="math inline">\(\beta_0\)</span> different from IV. It is a constant in our model. This can be best explained by visualising a regression line. Pay particular attention to the to the expressions after <code>function(x)</code></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="regression.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># A: Two models with different beta(0)</span></span>
<span id="cb1-2"><a href="regression.html#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb1-3"><a href="regression.html#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> <span class="cf">function</span>(x) <span class="dv">2</span> <span class="sc">*</span> x, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb1-4"><a href="regression.html#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> <span class="cf">function</span>(x) <span class="dv">1</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> x, <span class="at">colour =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb1-5"><a href="regression.html#cb1-5" aria-hidden="true" tabindex="-1"></a>  see<span class="sc">::</span><span class="fu">theme_modern</span>()</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/beta%20zero%20and%20beta%20one%20explained-1.png" width="672" /></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="regression.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># B: Two models with the same beta(0), but different beta(1)</span></span>
<span id="cb2-2"><a href="regression.html#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb2-3"><a href="regression.html#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> <span class="cf">function</span>(x) <span class="dv">2</span> <span class="sc">*</span> x, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb2-4"><a href="regression.html#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> <span class="cf">function</span>(x) <span class="dv">3</span> <span class="sc">*</span> x, <span class="at">colour =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb2-5"><a href="regression.html#cb2-5" aria-hidden="true" tabindex="-1"></a>  see<span class="sc">::</span><span class="fu">theme_modern</span>()</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/beta%20zero%20and%20beta%20one%20explained-2.png" width="672" /></p>
<p>Plot B also shows what happens if we change <span class="math inline">\(\beta_1\)</span>, i.e.¬†the slope. The two models both have the same intercept (and therefore the same origin), but the blue line ascends quicker than the red one, because its <span class="math inline">\(\beta_1\)</span> is higher than the one for the red model.</p>
<p>Lastly, the <span class="math inline">\(error\)</span> component in the regression model refers to the deviation of data from this regression lines. Ideally, we want this value to be as small as possible.</p>
<div id="fitting-a-regression-model-by-hand" class="section level3" number="12.1.1">
<h3><span class="header-section-number">12.1.1</span> Fitting a regression model by hand, i.e.¬†trial and error</h3>
<p>If this sounds all awfully theoretical, let‚Äôs try to fit a regression model by hand. First we need to consider what our model should be able to predict. Let‚Äôs say that the number of COVID-19 cases predicts the number of deaths due to COVID-19. Intuitively we would assume this should be a linear relationship, because the more cases there are, the more likely we find more deaths caused by it.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="regression.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We only select most recent numbers, i.e. &quot;2021-08-26&quot;</span></span>
<span id="cb3-2"><a href="regression.html#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># and countries which have COVID cases</span></span>
<span id="cb3-3"><a href="regression.html#cb3-3" aria-hidden="true" tabindex="-1"></a>covid_sample <span class="ot">&lt;-</span> covid <span class="sc">%&gt;%</span></span>
<span id="cb3-4"><a href="regression.html#cb3-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(date_reported <span class="sc">==</span> <span class="st">&quot;2021-08-26&quot;</span> <span class="sc">&amp;</span></span>
<span id="cb3-5"><a href="regression.html#cb3-5" aria-hidden="true" tabindex="-1"></a>           cumulative_cases <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb3-6"><a href="regression.html#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="regression.html#cb3-7" aria-hidden="true" tabindex="-1"></a>covid_sample <span class="sc">%&gt;%</span></span>
<span id="cb3-8"><a href="regression.html#cb3-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> cumulative_cases,</span>
<span id="cb3-9"><a href="regression.html#cb3-9" aria-hidden="true" tabindex="-1"></a>             <span class="at">y =</span> cumulative_deaths)) <span class="sc">+</span></span>
<span id="cb3-10"><a href="regression.html#cb3-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/Finding%20a%20regression%20equation%20by%20hand%20p1-1.png" width="672" /></p>
<p>This data visualisation shows us not much. We can see that there are three countries, which appear to have considerably more cases than most other countries. Thus, all other countries are crammed together in the bottom left corner. To improve this visualisation without removing the outliers, we can rescale the x and y axis using the function <code>scale_x_continuous()</code> and <code>scale_y_continuous()</code>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="regression.html#cb4-1" aria-hidden="true" tabindex="-1"></a>covid_sample <span class="sc">%&gt;%</span></span>
<span id="cb4-2"><a href="regression.html#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> cumulative_cases,</span>
<span id="cb4-3"><a href="regression.html#cb4-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">y =</span> cumulative_deaths)) <span class="sc">+</span></span>
<span id="cb4-4"><a href="regression.html#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb4-5"><a href="regression.html#cb4-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">trans =</span> <span class="st">&quot;log&quot;</span>) <span class="sc">+</span></span>
<span id="cb4-6"><a href="regression.html#cb4-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">trans =</span> <span class="st">&quot;log&quot;</span>)</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/Finding%20a%20regression%20equation%20by%20hand%20p2-1.png" width="672" /></p>
<p>As we can see, the scatterplot is now easier to read and the dots are more spread out. This reveals that there is quite a strong relationship between <code>cumulative_cases</code> and <code>cumulative_deaths</code>. However, similar to before, we should avoid outliers when performing our analysis. For the sake of simplicity, in this section I will limit the number of countries included in our analysis, which also removes the requirement of using <code>scale_x_continuous()</code> and <code>scale_y_continuous()</code>.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="regression.html#cb5-1" aria-hidden="true" tabindex="-1"></a>covid_sample <span class="ot">&lt;-</span> covid_sample <span class="sc">%&gt;%</span></span>
<span id="cb5-2"><a href="regression.html#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(date_reported <span class="sc">==</span> <span class="st">&quot;2021-08-26&quot;</span> <span class="sc">&amp;</span></span>
<span id="cb5-3"><a href="regression.html#cb5-3" aria-hidden="true" tabindex="-1"></a>           cumulative_cases <span class="sc">&gt;=</span> <span class="dv">2500</span> <span class="sc">&amp;</span></span>
<span id="cb5-4"><a href="regression.html#cb5-4" aria-hidden="true" tabindex="-1"></a>           cumulative_cases <span class="sc">&lt;=</span> <span class="dv">150000</span> <span class="sc">&amp;</span></span>
<span id="cb5-5"><a href="regression.html#cb5-5" aria-hidden="true" tabindex="-1"></a>           cumulative_deaths <span class="sc">&lt;=</span> <span class="dv">3000</span>)</span>
<span id="cb5-6"><a href="regression.html#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="regression.html#cb5-7" aria-hidden="true" tabindex="-1"></a>plot <span class="ot">&lt;-</span> covid_sample <span class="sc">%&gt;%</span></span>
<span id="cb5-8"><a href="regression.html#cb5-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> cumulative_cases,</span>
<span id="cb5-9"><a href="regression.html#cb5-9" aria-hidden="true" tabindex="-1"></a>             <span class="at">y =</span> cumulative_deaths)) <span class="sc">+</span></span>
<span id="cb5-10"><a href="regression.html#cb5-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span>
<span id="cb5-11"><a href="regression.html#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="regression.html#cb5-12" aria-hidden="true" tabindex="-1"></a>plot</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/Finding%20a%20regression%20equation%20by%20hand%20p3-1.png" width="672" /></p>
<p>Through trial and error we can try to fit a linear line on top by adjusting the beta values. This is effectively what we hope to achieve with a regression: the best <span class="math inline">\(\beta\)</span> values which best explain our data. Let‚Äôs start with the basic assumption of <span class="math inline">\(y = x\)</span> without specific <span class="math inline">\(\beta\)</span>s, i.e.¬†they are zero.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="regression.html#cb6-1" aria-hidden="true" tabindex="-1"></a>plot <span class="sc">+</span></span>
<span id="cb6-2"><a href="regression.html#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> <span class="cf">function</span>(x) x, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/Finding%20a%20regression%20equation%20by%20hand%20p4-1.png" width="672" /></p>
<p>What we try to achieve is that the red line fits nicely inside the cloud of dots. Our very simple model provides a very poor fit to our data points, because all the dots are way below it. This makes sense, because <span class="math inline">\(y = x\)</span> would imply that every COVID-19 case leads to a death, i.e.¬†everyone with COVID did not survive. From our own experience we know that this is luckily not true. Ideally we want the line to be less steep, because our first model does not make much sense. We can do this by adding a <span class="math inline">\(\beta_1\)</span> to our equation. Maybe only 2% of people who got COVID-19 might not have recovered, i.e.¬†<span class="math inline">\(\beta_1 = 0.02\)</span>.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="regression.html#cb7-1" aria-hidden="true" tabindex="-1"></a>plot <span class="sc">+</span></span>
<span id="cb7-2"><a href="regression.html#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> <span class="cf">function</span>(x) <span class="fl">0.02</span><span class="sc">*</span>x, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/Finding%20a%20regression%20equation%20by%20hand%20p5-1.png" width="672" /></p>
<p>This time the line looks much more aligned with our observations. One could argue that it might have to move a little to the right as well, to cover the observations at the bottom a bit better. Therefore, we should add a <span class="math inline">\(\beta_0\)</span> to our equation, e.g.¬†<code>-50</code>.= to move it to the right and tweak the <span class="math inline">\(\beta_1\)</span> ever so slightly.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="regression.html#cb8-1" aria-hidden="true" tabindex="-1"></a>plot <span class="sc">+</span></span>
<span id="cb8-2"><a href="regression.html#cb8-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> <span class="cf">function</span>(x) <span class="sc">-</span><span class="dv">50</span> <span class="sc">+</span> <span class="fl">0.015</span><span class="sc">*</span>x, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/Finding%20a%20regression%20equation%20by%20hand%20p6-1.png" width="672" /></p>
<p>We finished creating our regression model. If we wanted to express it as a formula we would write <span class="math inline">\(DV = -5 + 0.015 * IV\)</span>. We could now use this model to predict how high COVID cases likely will be in other countries.</p>
</div>
<div id="fitting-a-regression-model-computationally" class="section level3" number="12.1.2">
<h3><span class="header-section-number">12.1.2</span> Fitting a regression model computationally</h3>
<p>Estimating a regression model by hand is not ideal and far from accurate. Instead, we would compute the <span class="math inline">\(\beta\)</span>s based on our observed data, i.e.¬†<code>cumulative_cases</code> and <code>cumulative_deaths</code>. We can use the function <code>lm()</code> to achieve this. I also rounded the all <code>numeric</code> values to two decimal places to make the output easier to read. We also use <code>tidy()</code> to retrieve a cleaner output from the computation.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="regression.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># classic r</span></span>
<span id="cb9-2"><a href="regression.html#cb9-2" aria-hidden="true" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> <span class="fu">lm</span>(cumulative_deaths <span class="sc">~</span> cumulative_cases, <span class="at">data =</span> covid_sample)</span>
<span id="cb9-3"><a href="regression.html#cb9-3" aria-hidden="true" tabindex="-1"></a>broom<span class="sc">::</span><span class="fu">tidy</span>(m0) <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">where</span>(is.numeric), round, <span class="dv">2</span>))</span></code></pre></div>
<pre><code>## # A tibble: 2 √ó 5
##   term             estimate std.error statistic p.value
##   &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)         88.1       70.5      1.25    0.21
## 2 cumulative_cases     0.01       0       10.7     0</code></pre>
<p>We first might notice that the <code>p.value</code> indicates that the relationship between <code>cumulative death</code> and <code>cumulative_cases</code> is significant. Thus, we can conclude that countries with more COVID cases also suffer higher numbers of people who do not successfully recover from it. However, you might be wondering where our <span class="math inline">\(\beta\)</span> scores are. They are found where it says <code>estimate</code>. The standard error (<code>std.error</code>) denotes the <code>error</code> we specified in the <a href="regression.html#single-linear-regression">previous equation</a>. We find that in the first row we get the <span class="math inline">\(\beta_0\)</span>, i.e.¬†the one for the intercept which is 88.10. This one is larger than what we estimated, i.e.¬†<code>-50</code>. However, <span class="math inline">\(\beta_1\)</span> is 0.01, which means we have done a very good job in guessing this estimate. Still, it becomes hopefully obvious that it is much easier to use the function <code>lm()</code> to estimate a model than ‚Äòeyeballing it.‚Äô</p>
<p>We can now visualise the computed model (in blue) and our guessed model (in red) in one plot and see the differences. The plot shows that we have not been too far off. However, it was relatively easy to fit a model onto the observed data in this case. Often, it is much more difficult, especially when more than two variables are involved.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="regression.html#cb11-1" aria-hidden="true" tabindex="-1"></a>plot <span class="sc">+</span></span>
<span id="cb11-2"><a href="regression.html#cb11-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> <span class="cf">function</span>(x) <span class="sc">-</span><span class="dv">50</span> <span class="sc">+</span> <span class="fl">0.015</span><span class="sc">*</span>x, <span class="at">colour =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb11-3"><a href="regression.html#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_function</span>(<span class="at">fun =</span> <span class="cf">function</span>(x) <span class="fl">88.1</span> <span class="sc">+</span> <span class="fl">0.01</span><span class="sc">*</span>x, <span class="at">colour =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/Showing%20computed%20and%20guessed%20regression-1.png" width="672" /></p>
<p>With our final model computed, we also need to check its quality in terms of predictive power based on how well it can actually explain our observed data. We have tested models before when we looked at confirmatory factor analyses for latent variables (see Chapter <a href="data-wrangling.html#latent-constructs">7.7</a>. This time we want to know how accurate our model is in explaining observed data and therefore how accurate it will be predicting future observations. The package <code>performance</code> offers a nice little shortcut to compute many different things at once:</p>
<ul>
<li><p><code>check_model()</code>: Checks for linearity, homogeneity, collinearity and outliers</p></li>
<li><p><code>model_performance()</code>: Tests the quality of our model.</p></li>
</ul>
<p>For now, we are mainly interested in the performance of our model. So, we can compute it the following way:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="regression.html#cb12-1" aria-hidden="true" tabindex="-1"></a>performance<span class="sc">::</span><span class="fu">model_performance</span>(m0)</span></code></pre></div>
<pre><code>## # Indices of model performance
## 
## AIC      |      BIC |    R2 | R2 (adj.) |    RMSE |   Sigma
## -----------------------------------------------------------
## 1472.626 | 1480.319 | 0.548 |     0.543 | 502.573 | 507.891</code></pre>
<p>There are quite a number of performance indicators and here is how to read them:</p>
<ul>
<li><p><code>AIC</code> stands for <em>Akaike Information Criterion</em> and the lower the score the better the model.</p></li>
<li><p><code>BIC</code> stands for <em>Bayesian Information Criterion</em> and the lower the score the better the model.</p></li>
<li><p><code>R2</code> stands for <em>R squared</em> (<span class="math inline">\(R^2\)</span>) and is also known as the coefficient of determination. It measures how much the independent variable can explain the variance in the dependent variable. In other words, the higher <span class="math inline">\(R^2\)</span> the better is our model, because more of the variance can be explained by our model. <span class="math inline">\(R^2\)</span> falls between 0-1, where 1 would imply that our model can explain 100% of the variance in our sample. <span class="math inline">\(R^2\)</span> is also considered a <em>goodness-of-fit measure</em>.</p></li>
<li><p><code>R2 (adj.)</code> stands for <em>adjusted R squared</em>. The adjusted version of <span class="math inline">\(R^2\)</span> becomes important if we have more than one predictor (i.e.¬†independent variable) in our regression. The adjustment of <span class="math inline">\(R^2\)</span> accounts for the number of independent variables in our model. Thus, it is possible for us to compare different models with each other, even though they might have different numbers of predictors. It is important to note that <span class="math inline">\(R^2\)</span> will always increase if we add more predictors.</p></li>
<li><p><code>RMSE</code> stands for <em>Root Mean Square Error</em> and is an indicate how small or large the prediction error of the model is. Conceptually, it aims to measure the average deviations of values from our model when we attempt predictions. The lower the score the better, i.e.¬†a score of 0 would imply that our model perfectly fits the data, which is likely never the case in the field of Social Sciences. The RMSE is particularly useful when trying to compare models.</p></li>
<li><p><code>Sigma</code> stands for the standard deviation of our residuals (the difference between predicted and empirically observed values) and is therefore a measure of prediction accuracy. <code>Sigma</code> is <em>‚Äòa measure of the average distance each observation falls from its prediction from the model‚Äô</em> <span class="citation">(<a href="#ref-gelman2020regression" role="doc-biblioref">Gelman, Hill, and Vehtari 2020, 168</a>)</span>.</p></li>
</ul>
<p>Many of these indices will become more relevant when we compare models. However, <span class="math inline">\(R^2\)</span> can also be meaningfully interpreted without a reference model. We know that the bigger <span class="math inline">\(R^2\)</span> the better. In our case it is <code>0.548</code> which is very good considering that our model consist of only one predictor only. It is not easy to interpret whether an particular <span class="math inline">\(R^2\)</span> value is good or bad. In our simple single linear regression, <span class="math inline">\(R^2\)</span> is literally ‚Äòr squared,‚Äô which we already know from correlations and their effect sizes (see Table <a href="#tab:effect-size-cohen"><strong>??</strong></a>. Thus, if we take the square root of <span class="math inline">\(R^2\)</span> we can retrieve the correlation coefficient, i.e.¬†<span class="math inline">\(r = \sqrt{R^2} = \sqrt{0.548} = 0.740\)</span>. According to <span class="citation"><a href="#ref-cohen1988statistical" role="doc-biblioref">J. Cohen</a> (<a href="#ref-cohen1988statistical" role="doc-biblioref">1988</a>)</span>, this would count as a large effect size.</p>
<p>However, for multiple regressions, the situation is slightly more complicated, but the interpretation of <span class="math inline">\(R^2\)</span> and its adjusted version remain largely the same.</p>
<p>Once you have a model and it is fairly accurate, you can start making predictions. This can be achieved by using our model object <code>m0</code> in combination with the function <code>predict()</code>. However, first we should define a set of values for our independent variable, i.e.¬†<code>cumulative_cases</code>, which we store in a tibble using the <code>tribble()</code> function.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="regression.html#cb14-1" aria-hidden="true" tabindex="-1"></a>df_predict <span class="ot">&lt;-</span> <span class="fu">tribble</span>(</span>
<span id="cb14-2"><a href="regression.html#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span class="sc">~</span>cumulative_cases,</span>
<span id="cb14-3"><a href="regression.html#cb14-3" aria-hidden="true" tabindex="-1"></a>                <span class="dv">100</span>,</span>
<span id="cb14-4"><a href="regression.html#cb14-4" aria-hidden="true" tabindex="-1"></a>               <span class="dv">1000</span>,</span>
<span id="cb14-5"><a href="regression.html#cb14-5" aria-hidden="true" tabindex="-1"></a>              <span class="dv">10000</span>,</span>
<span id="cb14-6"><a href="regression.html#cb14-6" aria-hidden="true" tabindex="-1"></a>             <span class="dv">100000</span></span>
<span id="cb14-7"><a href="regression.html#cb14-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-8"><a href="regression.html#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(m0, <span class="at">newdata =</span> df_predict)</span></code></pre></div>
<pre><code>##          1          2          3          4 
##   89.53289  102.45411  231.66629 1523.78810</code></pre>
<p>As a result we find out how many likely deaths from COVID have to be expected based on our model for each value in our dataset.</p>
<p>Single linear regressions are simple and a good way to introduce novice scholars to modeling social phenomena. However, hardly ever will find that a single variable can explain enough variance to be a useful model. Instead, we most likely can improve the majority of our single regression models by considering more variables in the form of multiple regressions.</p>
</div>
</div>
<div id="multiple-regression" class="section level2" number="12.2">
<h2><span class="header-section-number">12.2</span> Multiple regression</h2>
<p>Multiple regressions expand single linear regressions by allowing us to add more variables. Maybe less surprising, computing a multiple regression is similar to a single regression in R, because it requires the same function, i.e.¬†<code>lm()</code>. However, we add more IVs. Therefore, the equation we used before needs to be modified slightly by adding more independent variables each of which will have its own <span class="math inline">\(\beta\)</span> value:</p>
<div align="center">
<p><span class="math display">\[
DV = \beta_{0} + IV_{1} * \beta_{1} + IV_{2} * \beta_{2} + ... + IV_{n} * \beta_{n} + error
\]</span></p>
</div>
<p>In the last section we wanted to know how many people will likely not recover from COVID. However, it might be even more interesting to understand how we can predict new cases and prevent casualties from the outset. Since I live in the United Kingdom at the time of the pandemic, I am curious to know whether certain COVID measures helped to reduce the number of new cases there. To keep it more interesting, I will also add Germany to the mix since it has shown to be very effective in handling the pandemic relative to other European countries. Of course, feel free to pick different countries (maybe the one you live in?) to follow with my example. In Chapter <a href="#moderated-regressions"><strong>??</strong></a> it will become obvious why I chose two countries (#spoiler-alert).</p>
<p>First, we create a dataset that only contains information from the <code>United Kingdom</code> and <code>Germany</code><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>, which means we use <code>filter()</code>. We also create a column with unique identifiers, i.e.¬†an ID, because there is none. We can take the names of rows (which are numbers) and create a new column <code>rownames</code> by using the function <code>rownames_to_column</code>. This will come in handy at a later stage - no spoilers. We also should remove observations with missing data.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="regression.html#cb16-1" aria-hidden="true" tabindex="-1"></a>covid_uk_ger <span class="ot">&lt;-</span> covid <span class="sc">%&gt;%</span></span>
<span id="cb16-2"><a href="regression.html#cb16-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(iso3 <span class="sc">==</span> <span class="st">&quot;GBR&quot;</span> <span class="sc">|</span> iso3 <span class="sc">==</span> <span class="st">&quot;DEU&quot;</span>)</span></code></pre></div>
<p>In a next step, we might want to know how new cases are distributed over time. It is always a good idea to inspect the dependent variable to also get a feeling of how much variance there is in your data. Having a bigger range of data values is ideal because the regression model will be able to consider low and high values of the dependent variable instead of just high or low scores. Let‚Äôs plot the DV <code>new_cases</code> across time to see when and how much new COVID cases had to be reported.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="regression.html#cb17-1" aria-hidden="true" tabindex="-1"></a>covid_uk_ger <span class="sc">%&gt;%</span></span>
<span id="cb17-2"><a href="regression.html#cb17-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> date_reported,</span>
<span id="cb17-3"><a href="regression.html#cb17-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">y =</span> new_cases)) <span class="sc">+</span></span>
<span id="cb17-4"><a href="regression.html#cb17-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_col</span>()</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/Multiple%20regression%20plotting%20new%20cases-1.png" width="672" /></p>
<p>We can tell that there are different waves of new cases. As such, we should find that there are certain variables that should help explain when <code>new_cases</code> are high and when they are low. If you have hypotheses you want to test, you would already know which variables to include in your regression. However, in our case, we do not really have a hypothesis based on our prior reading or other studies. Thus, we pick variables of interest that we suspect could help us with modelling new COVID cases. We can be fairly certain that the number of new COVID cases should be lower if there are more safety measures in place - assuming that they are effective and adhered to, of course. The <code>covid</code> dataset includes such information evaluated by the <a href="https://covid19.who.int/info/" target="blank" title="WHO">WHO</a>, i.e.¬†<code>masks</code>, <code>travel</code>, <code>gatherings</code>, <code>schools</code> and <code>movements</code>. Remember, you can always find out what these variables stand for by typing <code>?covid</code> into the console. A higher value for these variables indicates that there were more safety measures. Scores can range from 0 (i.e.¬†no measures) to 100 (all WHO measures taken).</p>
<p>We simply add these variables by using the <code>+</code> symbol in the <code>lm()</code> function.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="regression.html#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create our model</span></span>
<span id="cb18-2"><a href="regression.html#cb18-2" aria-hidden="true" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> <span class="fu">lm</span>(new_cases <span class="sc">~</span> masks <span class="sc">+</span> movements <span class="sc">+</span> gatherings <span class="sc">+</span> schools <span class="sc">+</span> businesses <span class="sc">+</span> travel,</span>
<span id="cb18-3"><a href="regression.html#cb18-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">data =</span> covid_uk_ger)</span>
<span id="cb18-4"><a href="regression.html#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="regression.html#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect the model specifications</span></span>
<span id="cb18-6"><a href="regression.html#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co"># I always round the p.value since I</span></span>
<span id="cb18-7"><a href="regression.html#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co"># do not prefer the scientific notation</span></span>
<span id="cb18-8"><a href="regression.html#cb18-8" aria-hidden="true" tabindex="-1"></a>broom<span class="sc">::</span><span class="fu">tidy</span>(m0) <span class="sc">%&gt;%</span></span>
<span id="cb18-9"><a href="regression.html#cb18-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">p.value =</span> <span class="fu">round</span>(p.value, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>## # A tibble: 7 √ó 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)  -2585.     745.       -3.47   0.001
## 2 masks          -14.8      9.18     -1.61   0.108
## 3 movements      -54.7     15.5      -3.52   0    
## 4 gatherings     -26.0     19.8      -1.31   0.191
## 5 schools        394.      29.7      13.2    0    
## 6 businesses      93.7     13.8       6.79   0    
## 7 travel          49.5      9.30      5.32   0</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="regression.html#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the quality of our model</span></span>
<span id="cb20-2"><a href="regression.html#cb20-2" aria-hidden="true" tabindex="-1"></a>performance<span class="sc">::</span><span class="fu">model_performance</span>(m0)</span></code></pre></div>
<pre><code>## # Indices of model performance
## 
## AIC       |       BIC |    R2 | R2 (adj.) |      RMSE |     Sigma
## -----------------------------------------------------------------
## 25277.622 | 25318.262 | 0.233 |     0.229 | 10027.204 | 10056.877</code></pre>
<p>Overall (and purely subjectively judged), the model is not particularly great, because even though we added so many variables, the <span class="math inline">\(adjusted \ R^2\)</span> is not particularly high, i.e.¬†‚Äòonly‚Äô 0.229. As mentioned earlier, for multiple regression it is better to look at <span class="math inline">\(adjusted \ R^2\)</span>, because it adjusts for the number of variables in our model and makes comparison of multiple models easier. There are a couple more important insights gained from this analysis:</p>
<ul>
<li><p>Not variables appear to be significant. The predictors <code>masks</code> and <code>gatherings</code> are not significant, i.e.¬†<code>p.value &gt; 0.05</code>. Thus, it might be worth removing these variables to optimise the model.</p></li>
<li><p>The variable, <code>movements</code> seems to reduce <code>new_cases</code>, i.e.¬†it has a negative estimate (<span class="math inline">\(\beta\)</span>).</p></li>
<li><p>However, <code>schools</code>, <code>businesses</code>, <code>travel</code> have a positive effect on <code>new_cases</code>.</p></li>
</ul>
<p>Especially the last point might appear confusing. How can measures taken increase the number of new COVID cases? Should we avoid them? What we have not considered in our regression is the fact that measures might be put in place to reduce the number of new cases rather than to prevent them. Thus, it might not be the case that <code>schools</code>, <code>businesses</code> and <code>travel</code> predict higher <code>new_cases</code>, but rather the opposite, i.e.¬†due to higher <code>new_cases</code> the measures for <code>schools</code>, <code>businesses</code> and <code>travel</code> were tightened, which later on (with a time lag) led to lower <code>new_cases</code>. Thus, the relationships might be a bit more complicated, but to keep things simple, we accept the fact that with our data we face certain limitations (as is usually the case).</p>
<p>As a final step, we should remove the variables that are not significant and see how this affects our model.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="regression.html#cb22-1" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(new_cases <span class="sc">~</span> movements <span class="sc">+</span> schools <span class="sc">+</span> businesses <span class="sc">+</span> travel,</span>
<span id="cb22-2"><a href="regression.html#cb22-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">data =</span> covid_uk_ger)</span>
<span id="cb22-3"><a href="regression.html#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="regression.html#cb22-4" aria-hidden="true" tabindex="-1"></a>broom<span class="sc">::</span><span class="fu">tidy</span>(m1) <span class="sc">%&gt;%</span></span>
<span id="cb22-5"><a href="regression.html#cb22-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">p.value =</span> <span class="fu">round</span>(p.value, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>## # A tibble: 5 √ó 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)  -3024.     724.       -4.18       0
## 2 movements      -64.0     13.4      -4.79       0
## 3 schools        382.      28.8      13.3        0
## 4 businesses      90.5     12.9       7.01       0
## 5 travel          45.7      9.17      4.99       0</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="regression.html#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the quality of our model</span></span>
<span id="cb24-2"><a href="regression.html#cb24-2" aria-hidden="true" tabindex="-1"></a>performance<span class="sc">::</span><span class="fu">model_performance</span>(m1)</span></code></pre></div>
<pre><code>## # Indices of model performance
## 
## AIC       |       BIC |    R2 | R2 (adj.) |      RMSE |     Sigma
## -----------------------------------------------------------------
## 25279.519 | 25309.999 | 0.229 |     0.227 | 10052.123 | 10073.343</code></pre>
<p>If we compare our original model <code>m0</code> with our revised model <code>m1</code>, we can see that our <code>R2 (adj.)</code> barely changed. Thus, <code>m1</code> is a superior model, because it can explain (almost) the same amount of variance, but with less predictors. The model <code>m1</code> would also be called a <em>parsimonious model</em>, i.e.¬†a model that is simple, but has good predictive power.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="regression.html#cb26-1" aria-hidden="true" tabindex="-1"></a>performance<span class="sc">::</span><span class="fu">compare_performance</span>(m0, m1)</span></code></pre></div>
<pre><code>## # Comparison of Model Performance Indices
## 
## Name | Model |       AIC |       BIC |    R2 | R2 (adj.) |      RMSE |     Sigma
## --------------------------------------------------------------------------------
## m0   |    lm | 25277.622 | 25318.262 | 0.233 |     0.229 | 10027.204 | 10056.877
## m1   |    lm | 25279.519 | 25309.999 | 0.229 |     0.227 | 10052.123 | 10073.343</code></pre>
<p>However, there are a couple of things we overlooked when running this regression. If you are familiar with regressions already, you might have been folding the hands over your face and bursted into tears about the blasphemeous approach to linear regression modelling. Let me course-correct at this point.</p>
<p>Similar to other parametric approaches to conducting research, we need to test for sources of bias, linearity, normality and homogeneity of variance (or the unpronounceable word ‚Äòhomoscedasticity‚Äô). Since multiple regressions consider more than one variable we have to consider these criteria in light of other variables. As such, we have to draw on different tools to assess our data. There are certain pre- and post-tests we have to perform to fully assess and develop a multiple regression model:</p>
<ul>
<li><p><em>pre-test</em>: We need to consider whether there are any outliers and whether all assumptions of OLS regression models are met.</p></li>
<li><p><em>post-test</em>: Do our independent variables correlate very strongly with each other, i.e.¬†are there issues of multiple collinearity.</p></li>
</ul>
<p>We already covered aspects of linearity, normality and homogeneity of variance. However, outliers and collinearity have to be reconsidered for multiple regressions.</p>
<div id="outliers-in-multiple-regressions" class="section level3" number="12.2.1">
<h3><span class="header-section-number">12.2.1</span> Outliers in multiple regressions</h3>
<p>While it should be fairly clear by now why we need to handle outliers (remember Chapter <a href="sources-of-bias.html#dealing-with-outliers">9.5</a>), the approach we take is somewhat different when we need to consider multiple variables at once. Instead of identifying outliers for each variable independently, we have to consider the interplay of variables as well. In other words, we need to find out how an outlier in our independent variable affects the overall model rather than just on single other variable. In short, we need a different technique to assess outliers. By now, you might not be shocked to find out that there is more than one way of identifying outliers in regressions and many different ways to compute them in R. <span class="citation">(<a href="#ref-cohen2014applied" role="doc-biblioref">P. Cohen, West, and Aiken 2014</a>)</span> distinguishes between where one can find outliers in the model as summarised in Table <a href="regression.html#tab:outliers-in-multiple-regressions">12.1</a>. I offer a selection of possible ways of computing the relevant statistics, but this list is not exhaustive. For example, many of these statistics can also be found by using <code>influence.measures()</code>.</p>
<table>
<caption><span id="tab:outliers-in-multiple-regressions">Table 12.1: </span>Outlier detection in multiple regressions</caption>
<colgroup>
<col width="17%" />
<col width="26%" />
<col width="56%" />
</colgroup>
<thead>
<tr class="header">
<th>Outlier in?</th>
<th>Measures</th>
<th>function in R</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>Dependent variable</em></td>
<td><ul>
<li>Internally studentized residuals</li>
</ul></td>
<td><ul>
<li><code>rstandard()</code> or <code>fortify()</code></li>
</ul></td>
</tr>
<tr class="even">
<td><em>Dependent variable</em></td>
<td><ul>
<li>Externally studentized residuals</li>
</ul></td>
<td><ul>
<li><code>rstudent()</code></li>
</ul></td>
</tr>
<tr class="odd">
<td><em>Independent variable</em></td>
<td><ul>
<li>Leverage</li>
</ul></td>
<td><ul>
<li><code>hatvalues()</code> or <code>fortify()</code></li>
</ul></td>
</tr>
<tr class="even">
<td><em>Independent variable</em></td>
<td><ul>
<li>Mahalanobis distance</li>
</ul></td>
<td><ul>
<li><code>mahalanobis()</code><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> or <code>mahalanobis_distance()</code><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></li>
</ul></td>
</tr>
<tr class="odd">
<td><em>Entire model</em></td>
<td><p><em>Global measures of influence</em></p>
<ul>
<li><p>DFFITS,</p></li>
<li><p>Cook‚Äôs d</p></li>
</ul></td>
<td><p><em>Global measures of influence</em></p>
<ul>
<li><p><code>dffits()</code></p></li>
<li><p><code>cooks.distance()</code> or <code>fortify()</code></p></li>
</ul></td>
</tr>
<tr class="even">
<td><em>Entire model</em></td>
<td><p><em>Specific measures of influence:</em></p>
<ul>
<li>DFBETAS</li>
</ul></td>
<td><p><em>Specific measures of influence:</em></p>
<ul>
<li><code>dfbetas()</code></li>
</ul></td>
</tr>
</tbody>
</table>
<p>While it might be clear why we need to use different approaches to find outliers in different components of our model, this might be less clear when evaluating outliers that affect the entire model. We distinguish between <em>global measures of influence</em>, which identify how a single observation affects the quality of the entire model, and <em>specific measures of influence</em>, which identify how a single observation affects each individual independent variable, i.e.¬†its regression coefficients denoted as <span class="math inline">\(\beta\)</span>. It is recommended to look at all different measures of outliers before venturing ahead to performing a linear multiple regression. Going through the entire set of possible outliers would way beyond the scope of this book. So, I will focus on five popular measures which cover all three categories:</p>
<ul>
<li><p>Dependent variable: Externally studentized residuals</p></li>
<li><p>Independent variable: Leverage and Mahalanobis distance</p></li>
<li><p>Entire model: Cook‚Äôs d, DFBETAS</p></li>
</ul>
<p>The approach taken is the same for the other outlier detection methods. Thus, it should be fairly simple to reproduce these as well after having finished the chapters below.</p>
<div id="outliers-in-the-dependent-variable" class="section level4" number="12.2.1.1">
<h4><span class="header-section-number">12.2.1.1</span> Outliers in the dependent variable</h4>
<p>Irrespective of whether we look at independent or dependent variables, we always want to know whether there are extreme values present. Here I will use the externally studentized residual which ‚Äôis the preferred statistic to use to identify cases whose [‚Ä¶] values are highly discrepant from their predicted values <span class="citation">(<a href="#ref-cohen2014applied" role="doc-biblioref">P. Cohen, West, and Aiken 2014, 401</a>)</span>.</p>
<p>First we need to compute the residuals for our model as a new column in our dataset. Since we also want to use other methods to investigate outliers we can use the function <code>fortify()</code> which will add some of the later indicators as well and create a <code>tibble</code> which only includes the variables from our model. One function to do a lot of things at once. In a second step we add the studentized residuals using <code>rstudent()</code>.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="regression.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a tibble with some pre-computed stats</span></span>
<span id="cb28-2"><a href="regression.html#cb28-2" aria-hidden="true" tabindex="-1"></a>m1_outliers <span class="ot">&lt;-</span> <span class="fu">fortify</span>(m1) <span class="sc">%&gt;%</span> <span class="fu">as_tibble</span>()</span>
<span id="cb28-3"><a href="regression.html#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(m1_outliers)</span></code></pre></div>
<pre><code>## Rows: 1,188
## Columns: 11
## $ new_cases  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ movements  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ schools    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ businesses &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ travel     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ .hat       &lt;dbl&gt; 0.005159386, 0.005159386, 0.005159386, 0.005159386, 0.00515‚Ä¶
## $ .sigma     &lt;dbl&gt; 10077.22, 10077.22, 10077.22, 10077.22, 10077.22, 10077.22,‚Ä¶
## $ .cooksd    &lt;dbl&gt; 9.393496e-05, 9.393496e-05, 9.393496e-05, 9.393496e-05, 9.3‚Ä¶
## $ .fitted    &lt;dbl&gt; -3023.617, -3023.617, -3023.617, -3023.617, -3023.617, -302‚Ä¶
## $ .resid     &lt;dbl&gt; 3023.617, 3023.617, 3023.617, 3023.617, 3023.617, 3023.617,‚Ä¶
## $ .stdresid  &lt;dbl&gt; 0.3009375, 0.3009375, 0.3009375, 0.3009375, 0.3009375, 0.30‚Ä¶</code></pre>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="regression.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the externally studentized residuals</span></span>
<span id="cb30-2"><a href="regression.html#cb30-2" aria-hidden="true" tabindex="-1"></a>m1_outliers <span class="ot">&lt;-</span> m1_outliers <span class="sc">%&gt;%</span></span>
<span id="cb30-3"><a href="regression.html#cb30-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">.studresid =</span> <span class="fu">rstudent</span>(m1))</span>
<span id="cb30-4"><a href="regression.html#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(m1_outliers)</span></code></pre></div>
<pre><code>## Rows: 1,188
## Columns: 12
## $ new_cases  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ movements  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ schools    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ businesses &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ travel     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶
## $ .hat       &lt;dbl&gt; 0.005159386, 0.005159386, 0.005159386, 0.005159386, 0.00515‚Ä¶
## $ .sigma     &lt;dbl&gt; 10077.22, 10077.22, 10077.22, 10077.22, 10077.22, 10077.22,‚Ä¶
## $ .cooksd    &lt;dbl&gt; 9.393496e-05, 9.393496e-05, 9.393496e-05, 9.393496e-05, 9.3‚Ä¶
## $ .fitted    &lt;dbl&gt; -3023.617, -3023.617, -3023.617, -3023.617, -3023.617, -302‚Ä¶
## $ .resid     &lt;dbl&gt; 3023.617, 3023.617, 3023.617, 3023.617, 3023.617, 3023.617,‚Ä¶
## $ .stdresid  &lt;dbl&gt; 0.3009375, 0.3009375, 0.3009375, 0.3009375, 0.3009375, 0.30‚Ä¶
## $ .studresid &lt;dbl&gt; 0.3008218, 0.3008218, 0.3008218, 0.3008218, 0.3008218, 0.30‚Ä¶</code></pre>
<p>With our dataset ready for plotting we can do exactly that and see which observations are particularly far away from the rest of our <code>.studresid</code> values.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="regression.html#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an ID column</span></span>
<span id="cb32-2"><a href="regression.html#cb32-2" aria-hidden="true" tabindex="-1"></a>m1_outliers <span class="ot">&lt;-</span> m1_outliers <span class="sc">%&gt;%</span> <span class="fu">rownames_to_column</span>()</span>
<span id="cb32-3"><a href="regression.html#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="regression.html#cb32-4" aria-hidden="true" tabindex="-1"></a>m1_outliers <span class="sc">%&gt;%</span></span>
<span id="cb32-5"><a href="regression.html#cb32-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> rowname,</span>
<span id="cb32-6"><a href="regression.html#cb32-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">y =</span> .studresid)) <span class="sc">+</span></span>
<span id="cb32-7"><a href="regression.html#cb32-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/Plot%20externally%20studentized%20residuals-1.png" width="672" /></p>
<p>The values of the externally studentized residuals can be positive or negative. All we need to know now is which values count as outliers and which ones do not. <span class="citation"><a href="#ref-cohen2014applied" role="doc-biblioref">P. Cohen, West, and Aiken</a> (<a href="#ref-cohen2014applied" role="doc-biblioref">2014</a>)</span> (p.¬†401) provides some guidance:</p>
<ul>
<li><p>general: <span class="math inline">\(outlier = \pm 2\)</span></p></li>
<li><p>bigger samples: <span class="math inline">\(outlier = \pm 3\)</span> or <span class="math inline">\(\pm 3.5\)</span> or <span class="math inline">\(\pm 4\)</span></p></li>
</ul>
<p>As you can tell, it is a matter of well-informed personal judgement. Our dataset consists of over 1200 observations. As such, the dataset certainly counts as large. We can take a look and see how many outliers we would get for each of the benchmarks.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="regression.html#cb33-1" aria-hidden="true" tabindex="-1"></a>out_detect <span class="ot">&lt;-</span> m1_outliers <span class="sc">%&gt;%</span></span>
<span id="cb33-2"><a href="regression.html#cb33-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">pm_2 =</span> <span class="fu">ifelse</span>(<span class="fu">abs</span>(.studresid) <span class="sc">&gt;</span> <span class="dv">2</span>, <span class="st">&quot;TRUE&quot;</span>, <span class="st">&quot;FALSE&quot;</span>),</span>
<span id="cb33-3"><a href="regression.html#cb33-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">pm_3 =</span> <span class="fu">ifelse</span>(<span class="fu">abs</span>(.studresid) <span class="sc">&gt;</span> <span class="dv">3</span>, <span class="st">&quot;TRUE&quot;</span>, <span class="st">&quot;FALSE&quot;</span>),</span>
<span id="cb33-4"><a href="regression.html#cb33-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">pm_35 =</span> <span class="fu">ifelse</span>(<span class="fu">abs</span>(.studresid) <span class="sc">&gt;</span> <span class="fl">3.5</span>, <span class="st">&quot;TRUE&quot;</span>, <span class="st">&quot;FALSE&quot;</span>),</span>
<span id="cb33-5"><a href="regression.html#cb33-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">pm_4 =</span> <span class="fu">ifelse</span>(<span class="fu">abs</span>(.studresid) <span class="sc">&gt;</span> <span class="dv">4</span>, <span class="st">&quot;TRUE&quot;</span>, <span class="st">&quot;FALSE&quot;</span>))</span>
<span id="cb33-6"><a href="regression.html#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="regression.html#cb33-7" aria-hidden="true" tabindex="-1"></a>out_detect <span class="sc">%&gt;%</span> <span class="fu">count</span>(pm_2)</span></code></pre></div>
<pre><code>## # A tibble: 2 √ó 2
##   pm_2      n
##   &lt;chr&gt; &lt;int&gt;
## 1 FALSE  1132
## 2 TRUE     56</code></pre>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="regression.html#cb35-1" aria-hidden="true" tabindex="-1"></a>out_detect <span class="sc">%&gt;%</span> <span class="fu">count</span>(pm_3)</span></code></pre></div>
<pre><code>## # A tibble: 2 √ó 2
##   pm_3      n
##   &lt;chr&gt; &lt;int&gt;
## 1 FALSE  1171
## 2 TRUE     17</code></pre>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="regression.html#cb37-1" aria-hidden="true" tabindex="-1"></a>out_detect <span class="sc">%&gt;%</span> <span class="fu">count</span>(pm_35)</span></code></pre></div>
<pre><code>## # A tibble: 2 √ó 2
##   pm_35     n
##   &lt;chr&gt; &lt;int&gt;
## 1 FALSE  1173
## 2 TRUE     15</code></pre>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="regression.html#cb39-1" aria-hidden="true" tabindex="-1"></a>out_detect <span class="sc">%&gt;%</span> <span class="fu">count</span>(pm_4)</span></code></pre></div>
<pre><code>## # A tibble: 2 √ó 2
##   pm_4      n
##   &lt;chr&gt; &lt;int&gt;
## 1 FALSE  1179
## 2 TRUE      9</code></pre>
<p>The results indicate we could have as many as 53 outliers and as little as 7. It becomes apparent that choosing the right threshold is a tricky undertaking. Let‚Äôs plot the data again in an easier way to read and add some of the thresholds. I skip <code>3.5</code>, since it is very close to <code>3</code>. I also reorder the observations (i.e.¬†the x axis) based on <code>.studresid</code> using <code>reorder()</code>.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="regression.html#cb41-1" aria-hidden="true" tabindex="-1"></a>m1_outliers <span class="sc">%&gt;%</span></span>
<span id="cb41-2"><a href="regression.html#cb41-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder</span>(rowname, .studresid),</span>
<span id="cb41-3"><a href="regression.html#cb41-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">y =</span> .studresid)) <span class="sc">+</span></span>
<span id="cb41-4"><a href="regression.html#cb41-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb41-5"><a href="regression.html#cb41-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>), <span class="at">col =</span> <span class="st">&quot;green&quot;</span>) <span class="sc">+</span></span>
<span id="cb41-6"><a href="regression.html#cb41-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>), <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>) <span class="sc">+</span></span>
<span id="cb41-7"><a href="regression.html#cb41-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>), <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/Plotting%20the%20number%20of%20outliers%20externally%20studentized%20residuals-1.png" width="672" /></p>
<p>At this point, it is a matter of choosing the threshold that you feel is most appropriate. More importantly, though, you have to make sure you are transparent in your choices and provide some explanations around your decision-making. For example, a threshold of <code>2</code> appears a bit too harsh for my taste and identifies too many observations as outliers. Using the orange threshold of <code>3</code> seems to capture most outliers, because we can also visually see how the dots start to look less like a line and separate out more strongly. Besides, a threshold of <code>3</code> seems also more suitable for larger datasets. Since we have an id column (i.e.¬†<code>rownames</code>) we can also store our outliers in a separate object to easily reference it later for comparisons with other measures. You will see later why this is handy to have.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="regression.html#cb42-1" aria-hidden="true" tabindex="-1"></a>outliers <span class="ot">&lt;-</span> out_detect <span class="sc">%&gt;%</span></span>
<span id="cb42-2"><a href="regression.html#cb42-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(rowname, pm_3) <span class="sc">%&gt;%</span></span>
<span id="cb42-3"><a href="regression.html#cb42-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">studresid =</span> pm_3)</span></code></pre></div>
<p>There are still more diagnostic steps we have to take before we make a final decision on which observations we want to remove or deal with in other ways (see also Chapter <a href="sources-of-bias.html#dealing-with-outliers">9.5</a>.</p>
</div>
<div id="outliers-in-the-independent-variables" class="section level4" number="12.2.1.2">
<h4><span class="header-section-number">12.2.1.2</span> Outliers in the independent variables</h4>
<p>To identify outliers in the independent variables we can either use, for example, <em>Leverage scores</em>, or use the <em>Mahalanobis distances</em>. Both are legitimate approaches and can be computed very easily.</p>
<p>For the leverage scores, we can find them already in our <code>fortify()</code>-ed dataset <code>m1_outliers</code>. They are in the column <code>.hat</code>. An outlier is defined by the distance from the average leverage value, i.e.¬†the further the distance of an observation from this average leverage, the more likely we have to classify it as an outlier. The average leverage is computed as follows:</p>
<div id="average-leverage-equation" align="center">
<p><span class="math inline">\(average\ leverage = \frac{k + 1}{n}\)</span></p>
</div>
<p>In this equation, <em>k</em> stands for the number of predictors (i.e.¬†6) and <em>n</em> for the number of observations (i.e.¬†1188). Therefore, our average leverage can be computed as follows:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="regression.html#cb43-1" aria-hidden="true" tabindex="-1"></a>(avg_lvg <span class="ot">&lt;-</span> (<span class="dv">6</span> <span class="sc">+</span> <span class="dv">1</span>) <span class="sc">/</span> <span class="dv">1188</span>)</span></code></pre></div>
<pre><code>## [1] 0.005892256</code></pre>
<p>Also for this indicator we find different approaches to setting cut-off points. While <span class="citation"><a href="#ref-hoaglin1978hat" role="doc-biblioref">Hoaglin and Welsch</a> (<a href="#ref-hoaglin1978hat" role="doc-biblioref">1978</a>)</span> argue that a distance twice the average counts as an outlier, <span class="citation"><a href="#ref-stevens2012applied" role="doc-biblioref">Stevens</a> (<a href="#ref-stevens2012applied" role="doc-biblioref">2012</a>)</span> (p.105) suggests that values three times higher than the average leverage will negatively affect the model. The rationale is the same as for the externally studentized residuals: If the thresholds are too low, we might find ourselves with many observations which we would have to further investigate. This might not always be possible or even desirable. However, this should not imply that many outliers are not worth checking. Instead, if there are many, one would have to raise questions about the model itself and whether an important variable needs adding to explain a series of observations that appear to be somewhat ‚Äòoff.‚Äô</p>
<p>Let‚Äôs plot the leverages and use <span class="citation"><a href="#ref-stevens2012applied" role="doc-biblioref">Stevens</a> (<a href="#ref-stevens2012applied" role="doc-biblioref">2012</a>)</span> benchmark to draw our reference line.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="regression.html#cb45-1" aria-hidden="true" tabindex="-1"></a>m1_outliers <span class="sc">%&gt;%</span></span>
<span id="cb45-2"><a href="regression.html#cb45-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder</span>(rowname, .hat),</span>
<span id="cb45-3"><a href="regression.html#cb45-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">y =</span> .hat)) <span class="sc">+</span></span>
<span id="cb45-4"><a href="regression.html#cb45-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb45-5"><a href="regression.html#cb45-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">3</span><span class="sc">*</span>avg_lvg, <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>)</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/Plotting%20leverages-1.png" width="672" /></p>
<p>As before, we want to know which observations fall beyond the threshold.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="regression.html#cb46-1" aria-hidden="true" tabindex="-1"></a>new_outliers <span class="ot">&lt;-</span> m1_outliers <span class="sc">%&gt;%</span></span>
<span id="cb46-2"><a href="regression.html#cb46-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">avglvg =</span> <span class="fu">ifelse</span>(.hat <span class="sc">&gt;</span> <span class="dv">3</span><span class="sc">*</span>avg_lvg, <span class="st">&quot;TRUE&quot;</span>, <span class="st">&quot;FALSE&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb46-3"><a href="regression.html#cb46-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(rowname, avglvg)</span>
<span id="cb46-4"><a href="regression.html#cb46-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-5"><a href="regression.html#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Add new results to our reference list</span></span>
<span id="cb46-6"><a href="regression.html#cb46-6" aria-hidden="true" tabindex="-1"></a>outliers <span class="ot">&lt;-</span> <span class="fu">left_join</span>(outliers, new_outliers, <span class="at">by =</span> <span class="st">&quot;rowname&quot;</span>)</span></code></pre></div>
<p>If you look at our reference object <code>outliers</code> now, you will notice that there is only one observation that was detected by both methods. Thus, it very much depends on where we look for outliers, i.e.¬†dependent variable or independent variable.</p>
<p>The second method I will cover in this section is the Mahalanobis distance. Luckily the <code>rstatix</code> package includes a handy function <code>mahalanobis_distance()</code> which automatically detects outliers and presents them to us.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="regression.html#cb47-1" aria-hidden="true" tabindex="-1"></a>mhnbs_outliers <span class="ot">&lt;-</span> m1_outliers <span class="sc">%&gt;%</span></span>
<span id="cb47-2"><a href="regression.html#cb47-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(new_cases<span class="sc">:</span>travel) <span class="sc">%&gt;%</span></span>
<span id="cb47-3"><a href="regression.html#cb47-3" aria-hidden="true" tabindex="-1"></a>  rstatix<span class="sc">::</span><span class="fu">mahalanobis_distance</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb47-4"><a href="regression.html#cb47-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rownames_to_column</span>() <span class="sc">%&gt;%</span></span>
<span id="cb47-5"><a href="regression.html#cb47-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(rowname, mahal.dist, is.outlier) <span class="sc">%&gt;%</span></span>
<span id="cb47-6"><a href="regression.html#cb47-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">mhnbs =</span> is.outlier)</span>
<span id="cb47-7"><a href="regression.html#cb47-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-8"><a href="regression.html#cb47-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Add new results to our reference list</span></span>
<span id="cb47-9"><a href="regression.html#cb47-9" aria-hidden="true" tabindex="-1"></a>outliers <span class="ot">&lt;-</span> <span class="fu">left_join</span>(outliers, mhnbs_outliers, <span class="at">by =</span> <span class="st">&quot;rowname&quot;</span>)</span>
<span id="cb47-10"><a href="regression.html#cb47-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-11"><a href="regression.html#cb47-11" aria-hidden="true" tabindex="-1"></a><span class="co"># We need to remove mahal.dist because it does not indicate</span></span>
<span id="cb47-12"><a href="regression.html#cb47-12" aria-hidden="true" tabindex="-1"></a><span class="co"># whether a value is an outlier #data-cleaning</span></span>
<span id="cb47-13"><a href="regression.html#cb47-13" aria-hidden="true" tabindex="-1"></a>outliers <span class="ot">&lt;-</span> outliers <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>mahal.dist)</span></code></pre></div>
<p>While very convenient, it does pick the cut-off point for us. As we have learned so far, picking the ‚Äòright‚Äô cut-off point is important. Since the values follow a chi-square distribution, we can determine the cut-off points based on the relevant critical value at the chosen p value. R has a function which allows to find the critical value for our model, i.e.¬†<code>qchisq()</code>.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="regression.html#cb48-1" aria-hidden="true" tabindex="-1"></a>(mhnbs_th <span class="ot">&lt;-</span> <span class="fu">qchisq</span>(<span class="at">p =</span> <span class="fl">0.05</span>,</span>
<span id="cb48-2"><a href="regression.html#cb48-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">df =</span> <span class="dv">4</span>,</span>
<span id="cb48-3"><a href="regression.html#cb48-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">lower.tail =</span> <span class="cn">FALSE</span>))</span></code></pre></div>
<pre><code>## [1] 9.487729</code></pre>
<p>The <code>p</code> value reflects the probability we are willing to accept that our result is significant/not significant. The <code>df</code> refers to the degrees of freedom, which in this case relates to the number of independent variables, i.e.¬†6. Thus, it is fairly simple to identify a cut-off point yourself by choosing the p value you consider most appropriate. The function <code>mahalanobis_distance()</code> assumes <span class="math inline">\(p = 0.01\)</span>.</p>
<p>If we wanted to plot outliers like we did before, we can reuse our code from above and replace it with the relevant new variables.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="regression.html#cb50-1" aria-hidden="true" tabindex="-1"></a>mhnbs_outliers <span class="sc">%&gt;%</span></span>
<span id="cb50-2"><a href="regression.html#cb50-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder</span>(rowname, mahal.dist),</span>
<span id="cb50-3"><a href="regression.html#cb50-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">y =</span> mahal.dist)) <span class="sc">+</span></span>
<span id="cb50-4"><a href="regression.html#cb50-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb50-5"><a href="regression.html#cb50-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> mhnbs_th, <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>)</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/Plotting%20Mahalanobis%20distance%20scores-1.png" width="672" /></p>
<p>We can notice that the Mahalanobis distance identifies more outliers compared to the leverage, but the same ones. Thus, whether you really need to use both approaches for the same study is questionable and likely redundant. Still, in a few edge cases, you might want to double-check the results, especially when you feel uncertain which observations should be dealt with later. Of course, it does not take much time to consider both options.</p>
<p>Besides looking at each side of the regression separately, we might also want to consider whether removing an observation significantly changes the <span class="math inline">\(\beta\)</span> estimates. This is done with outlier detection diagnostics which consider the entire model.</p>
</div>
<div id="outlier-detection-global-measures" class="section level4" number="12.2.1.3">
<h4><span class="header-section-number">12.2.1.3</span> Outlier detection considering the entire model: Global measures</h4>
<p>To assess the global impact of outliers on the entirety of a regression model, Cook‚Äôs d <span class="citation">(<a href="#ref-cook1982residuals" role="doc-biblioref">Cook and Weisberg 1982</a>)</span> is a popular method in the Social Sciences. It measures to which extend a single observation can affect the predictive power of our model to explain all other observations. Obviously, we do not wish to keep observations that make it more difficult to predict most of the other observations correctly. However, as shown in Table <a href="regression.html#tab:outliers-in-multiple-regressions">12.1</a>, there are different approaches to this, but some are somewhat redundant. For example, <span class="citation"><a href="#ref-cohen2014applied" role="doc-biblioref">P. Cohen, West, and Aiken</a> (<a href="#ref-cohen2014applied" role="doc-biblioref">2014</a>)</span> highlight that DDFITS and Cook‚Äôs d are ‚Äòinterchangeable statistics‚Äô (p.¬†404). Thus, there is no point in demonstrating both since they function in a similar way. Your decision might be swayed by the preferences of a publisher, lecturer, supervisor or reviewer. Here, I will focus on Cook‚Äôs d since it was I see most frequently used in my own field. By all means, feel encouraged to go the extra mile and perform the same steps for the DDFITS.</p>
<p>The good news, <code>fortify()</code> automatically added <code>.cooksd</code> to our dataset <code>m1_outliers</code>. Thus, we cannot only compute whether outliers exist, but also inspect them visually as we did before. A good starting point to find outliers via the Cook‚Äôs D is to plot its distribution.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="regression.html#cb51-1" aria-hidden="true" tabindex="-1"></a>m1_outliers <span class="sc">%&gt;%</span></span>
<span id="cb51-2"><a href="regression.html#cb51-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> rowname,</span>
<span id="cb51-3"><a href="regression.html#cb51-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">y =</span> .cooksd)) <span class="sc">+</span></span>
<span id="cb51-4"><a href="regression.html#cb51-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_col</span>()</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/Plotting%20Cooks%20d-1.png" width="672" /></p>
<p>Inspecting this barplot, we can tell that there are some observations that have much higher <code>.cooksd</code> values than any other observations. Again, we first need to decide on a benchmark to determine whether we can consider these values as outliers. If we follow <span class="citation"><a href="#ref-cook1982residuals" role="doc-biblioref">Cook and Weisberg</a> (<a href="#ref-cook1982residuals" role="doc-biblioref">1982</a>)</span>, values that are higher than <span class="math inline">\(d &gt; 1\)</span> require reviewing. Looking at our plot, none of the observations reaches <code>1</code> and therefore we would have no need to investigate outliers. Alternatively, <span class="citation"><a href="#ref-cohen2014applied" role="doc-biblioref">P. Cohen, West, and Aiken</a> (<a href="#ref-cohen2014applied" role="doc-biblioref">2014</a>)</span> suggest that other benchmarks are also worth considering, for example based on the critical value of an F distribution, which we can determine with the function <code>qf()</code>, which requires us to determine two degrees of freedom (<span class="math inline">\(df_1\)</span> and <span class="math inline">\(df_2\)</span>) and a p value. To determine these values <span class="citation"><a href="#ref-cohen2014applied" role="doc-biblioref">P. Cohen, West, and Aiken</a> (<a href="#ref-cohen2014applied" role="doc-biblioref">2014</a>)</span> suggests <span class="math inline">\(p = 0.5\)</span> and the following formulas to determine the correct <span class="math inline">\(df\)</span>:</p>
<div id="critical-value-cooks-d" align="center">
<p><span class="math inline">\(df_1 = k +1\)</span></p>
<p><span class="math inline">\(df_2 = n - k - 1\)</span></p>
</div>
<p>Similar to the critical value for <a href="regression.html#average-leverage-equation" title="average leverage">average leverage</a>, <span class="math inline">\(k\)</span> reflects the number of predictors, and <span class="math inline">\(n\)</span> refers to the sample size. Thus, we can determine the critical value, and therefore our cut-off point as follows:</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="regression.html#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qf</span>(<span class="at">p =</span> <span class="fl">0.5</span>,</span>
<span id="cb52-2"><a href="regression.html#cb52-2" aria-hidden="true" tabindex="-1"></a>   <span class="at">df1 =</span> <span class="dv">4</span><span class="sc">+</span><span class="dv">1</span>,</span>
<span id="cb52-3"><a href="regression.html#cb52-3" aria-hidden="true" tabindex="-1"></a>   <span class="at">df2 =</span> <span class="dv">1188-6-1</span>)</span></code></pre></div>
<pre><code>## [1] 0.8707902</code></pre>
<p>Also this score would imply that we have no particular outliers to consider, because the highest value in <code>.cooksd</code> is 0.09 computed via `<code>max(m1_outliers$.cooksd)</code>`.</p>
<p>We might be lead to believe that our work is done here, but <span class="citation"><a href="#ref-cohen2014applied" role="doc-biblioref">P. Cohen, West, and Aiken</a> (<a href="#ref-cohen2014applied" role="doc-biblioref">2014</a>)</span> recommends that any larger deviation is worth inspecting and we do notice that relative to other observations there are some cases that appear extreme. Ideally, one would further investigate these cases and compare the results of the regression by removing such extreme cases iteratively. This way one can assess whether the extreme observations truly affect the overall estimates of our model. This would imply repeating steps we already covered earlier when performing multiple regressions with and without outliers. At the end of this chapter of outliers we will rerun the regression to see what effects it had on our model estimates. However, before we can do this, we have one more method of detecting outliers to cover.</p>
</div>
<div id="outlier-detection-specific-measures" class="section level4" number="12.2.1.4">
<h4><span class="header-section-number">12.2.1.4</span> Outlier detection considering the entire model: Specific measures</h4>
<p>While Cook‚Äôs d helps us to identify outliers which affect the quality of the entire model, there is also a way to investigate how outliers affect specific predictors. This is achieved with DIFBETAS, which also, similar to previous methods, identifies outliers by assessing the impact of deleting extreme cases has on other parts of the regression model.</p>
<p>The function <code>dbetas()</code> takes our model <code>m1</code> and returns the statistics for each predictor in our model. Thus, we do not receive a single score for each observation but multiple for each specific predictor, i.e.¬†each specific measure.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="regression.html#cb54-1" aria-hidden="true" tabindex="-1"></a>m1_dfbetas <span class="ot">&lt;-</span> <span class="fu">dfbetas</span>(m1) <span class="sc">%&gt;%</span></span>
<span id="cb54-2"><a href="regression.html#cb54-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span>               <span class="co"># convert into a tibble for convenience</span></span>
<span id="cb54-3"><a href="regression.html#cb54-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rownames_to_column</span>()          <span class="co"># add our rownames</span></span>
<span id="cb54-4"><a href="regression.html#cb54-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-5"><a href="regression.html#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(m1_dfbetas)</span></code></pre></div>
<pre><code>## Rows: 1,188
## Columns: 6
## $ rowname       &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;11&quot;,‚Ä¶
## $ `(Intercept)` &lt;dbl&gt; 0.02166365, 0.02166365, 0.02166365, 0.02166365, 0.021663‚Ä¶
## $ movements     &lt;dbl&gt; -0.0008908373, -0.0008908373, -0.0008908373, -0.00089083‚Ä¶
## $ schools       &lt;dbl&gt; -0.01412839, -0.01412839, -0.01412839, -0.01412839, -0.0‚Ä¶
## $ businesses    &lt;dbl&gt; -0.002957931, -0.002957931, -0.002957931, -0.002957931, ‚Ä¶
## $ travel        &lt;dbl&gt; -0.005546516, -0.005546516, -0.005546516, -0.005546516, ‚Ä¶</code></pre>
<p>All that is left to do is to compare the scores against a benchmark for each variable in this dataset and we know which observations substantially affect one of the predictors. <span class="citation"><a href="#ref-cohen2014applied" role="doc-biblioref">P. Cohen, West, and Aiken</a> (<a href="#ref-cohen2014applied" role="doc-biblioref">2014</a>)</span> provides us with the following recommendations for suitable thresholds:</p>
<ul>
<li><p>small or moderate dataset: <span class="math inline">\(DFBETAS &gt; \pm 1\)</span></p></li>
<li><p>large dataset: <span class="math inline">\(DFBETAS &gt; \pm\frac{2}{\sqrt(n)}\)</span></p></li>
</ul>
<p>Since our dataset falls rather into the ‚Äòlarge‚Äô camp, we should choose the second option. Again, <span class="math inline">\(n\)</span> stands for the sample size. Let‚Äôs create an object to store this value.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="regression.html#cb56-1" aria-hidden="true" tabindex="-1"></a>(dfbetas_th <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">1188</span>))</span></code></pre></div>
<pre><code>## [1] 0.05802589</code></pre>
<p>For demonstration purposes, I will pick <code>movements</code> to check for outliers. If this was a proper analysis for a project you would have to compute this for each variable separately. As the benchmarks indicate above, the DFEBTAS values can be positive or negative. So, when we compare the computed values with it we need to look at the absolute value, i.e.¬†use <code>abs()</code>.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="regression.html#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check whether values exceed the threshold</span></span>
<span id="cb58-2"><a href="regression.html#cb58-2" aria-hidden="true" tabindex="-1"></a>dfbetas_check <span class="ot">&lt;-</span> m1_dfbetas <span class="sc">%&gt;%</span></span>
<span id="cb58-3"><a href="regression.html#cb58-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">dfbetas_movements =</span> <span class="fu">ifelse</span>(<span class="fu">abs</span>(movements) <span class="sc">&gt;</span> dfbetas_th,</span>
<span id="cb58-4"><a href="regression.html#cb58-4" aria-hidden="true" tabindex="-1"></a>                                <span class="st">&quot;TRUE&quot;</span>,</span>
<span id="cb58-5"><a href="regression.html#cb58-5" aria-hidden="true" tabindex="-1"></a>                                <span class="st">&quot;FALSE&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb58-6"><a href="regression.html#cb58-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(rowname, dfbetas_movements)</span>
<span id="cb58-7"><a href="regression.html#cb58-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-8"><a href="regression.html#cb58-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Add result to our outliers object</span></span>
<span id="cb58-9"><a href="regression.html#cb58-9" aria-hidden="true" tabindex="-1"></a>outliers <span class="ot">&lt;-</span> outliers <span class="sc">%&gt;%</span> <span class="fu">left_join</span>(dfbetas_check, <span class="at">by =</span> <span class="st">&quot;rowname&quot;</span>)</span>
<span id="cb58-10"><a href="regression.html#cb58-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-11"><a href="regression.html#cb58-11" aria-hidden="true" tabindex="-1"></a><span class="co"># some housekeeping, i.e. making all columns &lt;lgl&gt; except for rowname</span></span>
<span id="cb58-12"><a href="regression.html#cb58-12" aria-hidden="true" tabindex="-1"></a>outliers <span class="ot">&lt;-</span> outliers <span class="sc">%&gt;%</span></span>
<span id="cb58-13"><a href="regression.html#cb58-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">rowname =</span> <span class="fu">as_factor</span>(rowname)) <span class="sc">%&gt;%</span></span>
<span id="cb58-14"><a href="regression.html#cb58-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate_if</span>(is.character, as.logical)</span></code></pre></div>
<p>Of course, we can also visualise the outliers as we did before.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="regression.html#cb59-1" aria-hidden="true" tabindex="-1"></a>m1_dfbetas <span class="sc">%&gt;%</span></span>
<span id="cb59-2"><a href="regression.html#cb59-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> rowname,</span>
<span id="cb59-3"><a href="regression.html#cb59-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">y =</span> movements)) <span class="sc">+</span></span>
<span id="cb59-4"><a href="regression.html#cb59-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb59-5"><a href="regression.html#cb59-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> dfbetas_th, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb59-6"><a href="regression.html#cb59-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="sc">-</span>dfbetas_th, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/Plot%20outliers%20using%20DFBETAS-1.png" width="672" /></p>
<p>I want to take this opportunity to also show that sometimes we can make visualisations even simpler. Remember, we used <code>abs()</code> to make all values positive? We can apply the simple principle here. This way we only need one line to indicate the theshold. In addition, we could also plot multiple variables at once. Instead of defining the <code>aes()</code> inside the <code>ggplot()</code> function, we can also define it for each <code>geom</code> separately. What do you think about the following version of the same plot?</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="regression.html#cb60-1" aria-hidden="true" tabindex="-1"></a>m1_dfbetas <span class="sc">%&gt;%</span></span>
<span id="cb60-2"><a href="regression.html#cb60-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb60-3"><a href="regression.html#cb60-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder</span>(rowname, <span class="fu">abs</span>(movements)),</span>
<span id="cb60-4"><a href="regression.html#cb60-4" aria-hidden="true" tabindex="-1"></a>                 <span class="at">y =</span> <span class="fu">abs</span>(movements),</span>
<span id="cb60-5"><a href="regression.html#cb60-5" aria-hidden="true" tabindex="-1"></a>                 <span class="at">col =</span> <span class="st">&quot;movements&quot;</span>),</span>
<span id="cb60-6"><a href="regression.html#cb60-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">size =</span> <span class="fl">0.5</span>,</span>
<span id="cb60-7"><a href="regression.html#cb60-7" aria-hidden="true" tabindex="-1"></a>             <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb60-8"><a href="regression.html#cb60-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> rowname,</span>
<span id="cb60-9"><a href="regression.html#cb60-9" aria-hidden="true" tabindex="-1"></a>                 <span class="at">y =</span> <span class="fu">abs</span>(travel),</span>
<span id="cb60-10"><a href="regression.html#cb60-10" aria-hidden="true" tabindex="-1"></a>                 <span class="at">col =</span> <span class="st">&quot;travel&quot;</span>),</span>
<span id="cb60-11"><a href="regression.html#cb60-11" aria-hidden="true" tabindex="-1"></a>             <span class="at">size =</span> <span class="fl">0.5</span>,</span>
<span id="cb60-12"><a href="regression.html#cb60-12" aria-hidden="true" tabindex="-1"></a>             <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb60-13"><a href="regression.html#cb60-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> dfbetas_th, <span class="at">col =</span> <span class="st">&quot;#FF503A&quot;</span>) <span class="sc">+</span></span>
<span id="cb60-14"><a href="regression.html#cb60-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;movements&quot;</span> <span class="ot">=</span> <span class="st">&quot;#2EA5FF&quot;</span>,</span>
<span id="cb60-15"><a href="regression.html#cb60-15" aria-hidden="true" tabindex="-1"></a>                       <span class="st">&quot;travel&quot;</span> <span class="ot">=</span> <span class="st">&quot;#7DB33B&quot;</span>)) <span class="sc">+</span></span>
<span id="cb60-16"><a href="regression.html#cb60-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb60-17"><a href="regression.html#cb60-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Label the legend appropriately</span></span>
<span id="cb60-18"><a href="regression.html#cb60-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">col =</span> <span class="st">&quot;COVID measures&quot;</span>)</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/Alternative%20way%20of%20plotting%20DFBETAS-1.png" width="672" /></p>
<p>After <code>reorder()</code>ing the variable <code>movements</code> and plotting <code>travel</code> as well, we can notice that there are seemingly less outliers for <code>movements</code> than for <code>travel</code>. Thus, some observations affect some predictors more strongly than others.</p>
</div>
<div id="reviewing-the-outliers" class="section level4" number="12.2.1.5">
<h4><span class="header-section-number">12.2.1.5</span> Reviewing the outliers</h4>
<p>After all this hard work and what turned out to be a very lengthy chapter, we finally arrive at the point to check which observations we might wish to remove or handle in some shape or form. First, we want to know which observations are affected. Therefore we need to review our reference data <code>outliers</code>. What we want to see are all observations which were identified by one or more diagnostic tool as an outlier. There is two ways to achieve this. We can use what we have learned so far and simply <code>filter()</code> each column for the value <code>TRUE</code>. This is the hard way of doing it and if you have many more columns this will take a little while and potentially drive you insane. The easy (and clever) way of filtering across multiple columns can be achieved by turning multiple columns into a single column. In the <code>tidyverse</code> this is called <code>pivot_longer()</code>. The following steps might sound complicated buy in fact it is relatively simple as long as you remember what ‚Äòtidy data‚Äô means, i.e.¬†each row represents one observation and each column represents one variable. Let‚Äôs do this step-by-step. Currently our data has five columns, of which four are different measures to detect outliers. Our goal is to create a <code>tibble</code> which has only three columns:</p>
<ul>
<li><p><code>rowname</code>, which we keep unchanged, because it is the ID which identifies each observation in our data</p></li>
<li><p><code>outlier_measure</code>, which is the variable that indicates which measure was used to find an outlier.</p></li>
<li><p><code>is.outlier</code>, which contains the values from the tibble, i.e.¬†the cell values of <code>TRUE</code> and <code>FALSE</code>.</p></li>
</ul>
<p>Here is an example of what we want to achieve. Imagine we have a very small dataset, which contains three columns, two of which are outlier detection measures, i.e.¬†<code>studresid</code> and <code>mhbns</code>:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="regression.html#cb61-1" aria-hidden="true" tabindex="-1"></a>data</span></code></pre></div>
<pre><code>## # A tibble: 3 √ó 3
##   rowname studresid mhbns
##   &lt;chr&gt;   &lt;lgl&gt;     &lt;lgl&gt;
## 1 1       FALSE     FALSE
## 2 2       TRUE      FALSE
## 3 3       TRUE      TRUE</code></pre>
<p>We now want to turn this dataset into a dataset with two columns. One column captures the <code>rowname</code> and the other one the outlier detection method. Thus, for every row in this dataset we have two values recorded. For example, the first observation has the values <code>studresid == FALSE</code> and <code>mhnbs == FALSE</code>. If we wanted to combine these two observations, we need to have two rows which have the <code>rowname</code> <code>1</code> to ensure that each row is still only contains one observation, i.e.¬†a tidy dataset. Therefore, we end up with more rows than in our original dataset, hence, a ‚Äòlonger‚Äô dataset. Here is how we can do this automatically:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="regression.html#cb63-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> data <span class="sc">%&gt;%</span></span>
<span id="cb63-2"><a href="regression.html#cb63-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="sc">!</span>rowname,</span>
<span id="cb63-3"><a href="regression.html#cb63-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">names_to =</span> <span class="st">&quot;outlier_measure&quot;</span>,</span>
<span id="cb63-4"><a href="regression.html#cb63-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">values_to =</span> <span class="st">&quot;is.outlier&quot;</span>)</span>
<span id="cb63-5"><a href="regression.html#cb63-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-6"><a href="regression.html#cb63-6" aria-hidden="true" tabindex="-1"></a>data</span></code></pre></div>
<pre><code>## # A tibble: 6 √ó 3
##   rowname outlier_measure is.outlier
##   &lt;chr&gt;   &lt;chr&gt;           &lt;lgl&gt;     
## 1 1       studresid       FALSE     
## 2 1       mhbns           FALSE     
## 3 2       studresid       TRUE      
## 4 2       mhbns           FALSE     
## 5 3       studresid       TRUE      
## 6 3       mhbns           TRUE</code></pre>
<p>Instead of three rows we have six and all the outlier detection values (i.e.¬†all <code>TRUE</code> and <code>FALSE</code> values) are now in one column. However, what exactly did just happen in this line of code? In light of what we specified above we did three things inside the function <code>pivot_longer()</code>:</p>
<ul>
<li><p>We excluded <code>rownames</code> from being pivoted, i.e.¬†<code>cols = !rowname</code>.</p></li>
<li><p>We specified a column where all the column names go to, i.e.¬†<code>names_to = "outlier_measure"</code>.</p></li>
<li><p>We specified a column where all cell values should be listed, i.e.¬†<code>values_to = "is.outlier</code>.</p></li>
</ul>
<p>From here, it is very easy to just <code>count()</code> the number of <code>is.outlier</code> per <code>rowname</code> and <code>filter()</code> out those that return a value of <code>TRUE</code>.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="regression.html#cb65-1" aria-hidden="true" tabindex="-1"></a>data <span class="sc">%&gt;%</span></span>
<span id="cb65-2"><a href="regression.html#cb65-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(rowname, is.outlier) <span class="sc">%&gt;%</span></span>
<span id="cb65-3"><a href="regression.html#cb65-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(is.outlier <span class="sc">==</span> <span class="st">&quot;TRUE&quot;</span>)</span></code></pre></div>
<pre><code>## # A tibble: 2 √ó 3
##   rowname is.outlier     n
##   &lt;chr&gt;   &lt;lgl&gt;      &lt;int&gt;
## 1 2       TRUE           1
## 2 3       TRUE           2</code></pre>
<p>Et voil√†. We now counted the number of times an observation was detected by all the different measures we use. This is obviously scaleable to more than two measures. Thus, we can apply it to our data as well.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="regression.html#cb67-1" aria-hidden="true" tabindex="-1"></a>outliers_true <span class="ot">&lt;-</span> outliers <span class="sc">%&gt;%</span></span>
<span id="cb67-2"><a href="regression.html#cb67-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">!</span>rowname, <span class="at">names_to =</span> <span class="st">&quot;measure&quot;</span>, <span class="at">values_to =</span> <span class="st">&quot;is.outlier&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb67-3"><a href="regression.html#cb67-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(rowname, is.outlier) <span class="sc">%&gt;%</span></span>
<span id="cb67-4"><a href="regression.html#cb67-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(is.outlier <span class="sc">==</span> <span class="st">&quot;TRUE&quot;</span>)</span>
<span id="cb67-5"><a href="regression.html#cb67-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-6"><a href="regression.html#cb67-6" aria-hidden="true" tabindex="-1"></a>outliers_true</span></code></pre></div>
<pre><code>## # A tibble: 85 √ó 3
##    rowname is.outlier     n
##    &lt;fct&gt;   &lt;lgl&gt;      &lt;int&gt;
##  1 224     TRUE           2
##  2 225     TRUE           2
##  3 226     TRUE           2
##  4 227     TRUE           2
##  5 344     TRUE           1
##  6 345     TRUE           1
##  7 346     TRUE           1
##  8 349     TRUE           1
##  9 350     TRUE           1
## 10 351     TRUE           1
## # ‚Ä¶ with 75 more rows</code></pre>
<p>The result is a <code>tibble</code> which tells us that we identified 85 distinct outliers. This might sound like a lot, but we also have to remember that our dataset consists of 1188 observations, i.e.¬†7% of our data are outliers.</p>
<p>Since the output is fairly long, we might want to plot the outcome to get an idea of the bigger picture.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="regression.html#cb69-1" aria-hidden="true" tabindex="-1"></a>outliers_true <span class="sc">%&gt;%</span></span>
<span id="cb69-2"><a href="regression.html#cb69-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder</span>(rowname, n),</span>
<span id="cb69-3"><a href="regression.html#cb69-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">y =</span> n)) <span class="sc">+</span></span>
<span id="cb69-4"><a href="regression.html#cb69-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_col</span>() <span class="sc">+</span></span>
<span id="cb69-5"><a href="regression.html#cb69-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_flip</span>()</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/Plot%20number%20of%20outliers-1.png" width="672" /></p>
<p>There are a few observations which were detected by three out of the four methods we used to detect outliers. there are some more which were detected by two different methods. However, there are also more than half of observations which were only detected by one method.</p>
<p>As highlighted in Chapter <a href="sources-of-bias.html#dealing-with-outliers">9.5</a> there are many ways we can go about outliers. To keep our analysis simple, I intend to remove those observations which were detected by multiple methods rather than only by one. Whether this approach is truly appropriate is a matter of further investigation. I take a very pragmatic approach with our sample data. However, it is worth reviewing the options available to transform data, as covered by <span class="citation"><a href="#ref-field2013discovering" role="doc-biblioref">Field</a> (<a href="#ref-field2013discovering" role="doc-biblioref">2013</a>)</span> (p.203). Data transformation can help to deal with outliers, instead of removing or imputing them. However, data transformation comes with severe drawbacks and in most cases, using a <em>bootstrapping</em> technique to account for violations of parametric conditions is often more advisable. Bootstrapping refers to the process of randomly resampling data from your dataset and rerun the same test multiple times, e.g.¬†2000 times. The process is somewhat similar to multiple imputation (see Chapter <a href="data-wrangling.html#replacing-removing-missing-data">7.6.3</a>, but instead of estimating a specific value in our dataset, we estimate statistical parameters, e.g.¬†confidence intervals or regression coefficients (i.e.¬†estimates). The package <code>rsample</code> allows to implement boostrapping in a fairly straightfoward way. For an example of a detailed look at a bootstrapped regression see Chapter <a href="#bootstrapped-regression"><strong>??</strong></a>.</p>
<p>For now, I settle on removing outliers entirely. The easiest way to remove these outliers is to combine our <code>outliers</code> dataset with the one for our regression (<code>m1_outliers</code>).</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="regression.html#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Include outliers which were detected by multiple methods</span></span>
<span id="cb70-2"><a href="regression.html#cb70-2" aria-hidden="true" tabindex="-1"></a>outliers_select <span class="ot">&lt;-</span> outliers_true <span class="sc">%&gt;%</span></span>
<span id="cb70-3"><a href="regression.html#cb70-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(n <span class="sc">&gt;</span> <span class="dv">1</span>)</span>
<span id="cb70-4"><a href="regression.html#cb70-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-5"><a href="regression.html#cb70-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Keep only columns which are NOT included in outliers_select</span></span>
<span id="cb70-6"><a href="regression.html#cb70-6" aria-hidden="true" tabindex="-1"></a>m1_no_outliers <span class="ot">&lt;-</span> <span class="fu">anti_join</span>(m1_outliers, outliers_select, <span class="at">by =</span> <span class="st">&quot;rowname&quot;</span>)</span>
<span id="cb70-7"><a href="regression.html#cb70-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-8"><a href="regression.html#cb70-8" aria-hidden="true" tabindex="-1"></a>m1_no_outliers</span></code></pre></div>
<pre><code>## # A tibble: 1,152 √ó 13
##    rowname new_cases movements schools businesses travel    .hat .sigma  .cooksd
##    &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;
##  1 1               0         0       0          0      0 0.00516 10077.  9.39e-5
##  2 2               0         0       0          0      0 0.00516 10077.  9.39e-5
##  3 3               0         0       0          0      0 0.00516 10077.  9.39e-5
##  4 4               0         0       0          0      0 0.00516 10077.  9.39e-5
##  5 5               0         0       0          0      0 0.00516 10077.  9.39e-5
##  6 6               0         0       0          0      0 0.00516 10077.  9.39e-5
##  7 7               0         0       0          0      0 0.00516 10077.  9.39e-5
##  8 8               0         0       0          0      0 0.00516 10077.  9.39e-5
##  9 9               0         0       0          0      0 0.00516 10077.  9.39e-5
## 10 10              0         0       0          0      0 0.00516 10077.  9.39e-5
## # ‚Ä¶ with 1,142 more rows, and 4 more variables: .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,
## #   .stdresid &lt;dbl&gt;, .studresid &lt;dbl&gt;</code></pre>
<p>The function <code>anti_join()</code> does exactly what the name implies. It takes the first data frame (i.e.¬†<code>m1_outliers</code>) and removes values that are included in the second data frame (i.e.¬†<code>outliers_select()</code>). This is tremendously helpful when having performed such complex outlier detection and want to remove them all in one go.</p>
<p>As the final step, we want to compare how the removal of outliers affected our model. Ideally, we managed to improve the regression.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="regression.html#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Original regression</span></span>
<span id="cb72-2"><a href="regression.html#cb72-2" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(new_cases <span class="sc">~</span> movements <span class="sc">+</span> schools <span class="sc">+</span> businesses <span class="sc">+</span> travel,</span>
<span id="cb72-3"><a href="regression.html#cb72-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">data =</span> covid_uk_ger)</span>
<span id="cb72-4"><a href="regression.html#cb72-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-5"><a href="regression.html#cb72-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Regression with outliers removed</span></span>
<span id="cb72-6"><a href="regression.html#cb72-6" aria-hidden="true" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(new_cases <span class="sc">~</span> movements <span class="sc">+</span> schools <span class="sc">+</span> businesses <span class="sc">+</span> travel,</span>
<span id="cb72-7"><a href="regression.html#cb72-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">data =</span> m1_no_outliers)</span>
<span id="cb72-8"><a href="regression.html#cb72-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-9"><a href="regression.html#cb72-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare the parameters between models</span></span>
<span id="cb72-10"><a href="regression.html#cb72-10" aria-hidden="true" tabindex="-1"></a><span class="co"># I added quite some reformatting, because</span></span>
<span id="cb72-11"><a href="regression.html#cb72-11" aria-hidden="true" tabindex="-1"></a><span class="co"># I prefer a regular tibble over pre-formatted</span></span>
<span id="cb72-12"><a href="regression.html#cb72-12" aria-hidden="true" tabindex="-1"></a><span class="co"># tables. Technially you only need the first line.</span></span>
<span id="cb72-13"><a href="regression.html#cb72-13" aria-hidden="true" tabindex="-1"></a>parameters<span class="sc">::</span><span class="fu">compare_parameters</span>(m1, m2) <span class="sc">%&gt;%</span></span>
<span id="cb72-14"><a href="regression.html#cb72-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span></span>
<span id="cb72-15"><a href="regression.html#cb72-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Parameter, Coefficient.m1, p.m1, Coefficient.m2, p.m2) <span class="sc">%&gt;%</span> </span>
<span id="cb72-16"><a href="regression.html#cb72-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">where</span>(is.double), round, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>## # A tibble: 5 √ó 5
##   Parameter   Coefficient.m1  p.m1 Coefficient.m2  p.m2
##   &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;
## 1 (Intercept)        -3024.      0       -1543.   0.031
## 2 movements            -64.0     0         -15.8  0.174
## 3 schools              382.      0         320.   0    
## 4 businesses            90.5     0          82.7  0    
## 5 travel                45.7     0          -1.27 0.875</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="regression.html#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare the performance between models</span></span>
<span id="cb74-2"><a href="regression.html#cb74-2" aria-hidden="true" tabindex="-1"></a>performance<span class="sc">::</span><span class="fu">compare_performance</span>(m1, m2)</span></code></pre></div>
<pre><code>## # Comparison of Model Performance Indices
## 
## Name | Model |       AIC |       BIC |    R2 | R2 (adj.) |      RMSE |     Sigma
## --------------------------------------------------------------------------------
## m1   |    lm | 25279.519 | 25309.999 | 0.229 |     0.227 | 10052.123 | 10073.343
## m2   |    lm | 24099.630 | 24129.926 | 0.179 |     0.177 |  8398.110 |  8416.395</code></pre>
<p>If you look at <code>R2 (adj.)</code> you might feel a sense of disappointment. How does the model explain less variance? Well, if we consider <code>AIC</code>, <code>BIC</code> and <code>RMSE</code>, we managed to improve the model, because their values are lower for <code>m2</code>. However, it seems that the outliers affected <code>R2</code>, which means they inflated this model indicator. In other words, our regression explains less than we hoped for. However, we can be more confident that the predictions of this model will be more accurate.</p>
<p>If we inspect the estimates more closely, we also notice that <code>movements</code> (where we removed outliers) and <code>travel</code> are not significant anymore (i.e <code>p.m2 &gt; 0.05</code>). As such, we should remove these from our model, since they do not help explain the variance of <code>new_cases</code>.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="regression.html#cb76-1" aria-hidden="true" tabindex="-1"></a>m3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(new_cases <span class="sc">~</span> schools <span class="sc">+</span> businesses,</span>
<span id="cb76-2"><a href="regression.html#cb76-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">data =</span> m1_no_outliers)</span>
<span id="cb76-3"><a href="regression.html#cb76-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-4"><a href="regression.html#cb76-4" aria-hidden="true" tabindex="-1"></a>parameters<span class="sc">::</span><span class="fu">model_parameters</span>(m3) <span class="sc">%&gt;%</span></span>
<span id="cb76-5"><a href="regression.html#cb76-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span></span>
<span id="cb76-6"><a href="regression.html#cb76-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Parameter, Coefficient, p) <span class="sc">%&gt;%</span> </span>
<span id="cb76-7"><a href="regression.html#cb76-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">where</span>(is.double), round, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>## # A tibble: 3 √ó 3
##   Parameter   Coefficient     p
##   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;
## 1 (Intercept)     -1664.  0.016
## 2 schools           307.  0    
## 3 businesses         78.9 0</code></pre>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="regression.html#cb78-1" aria-hidden="true" tabindex="-1"></a>performance<span class="sc">::</span><span class="fu">model_performance</span>(m3)</span></code></pre></div>
<pre><code>## # Indices of model performance
## 
## AIC       |       BIC |    R2 | R2 (adj.) |     RMSE |    Sigma
## ---------------------------------------------------------------
## 24098.209 | 24118.406 | 0.178 |     0.176 | 8407.515 | 8418.483</code></pre>
<p>Similar to before, after removing the insignificant predictors we end up with an equally good model (in terms of <span class="math inline">\(adjusted R^2\)</span>, but we need less variables to explain the same amount of variance in <code>new_cases</code>.</p>
</div>
</div>
<div id="standardised-beta-beta-vs.-unstandardised-beta-b" class="section level3" number="12.2.2">
<h3><span class="header-section-number">12.2.2</span> Standardised beta (<span class="math inline">\(\beta\)</span>) vs.¬†unstandardised beta (<span class="math inline">\(B\)</span>)</h3>
<p>In multiple regressions we often use variables which are measured in different ways and which use different measurement units, e.g.¬†currency, age, etc. Thus, it is fairly difficult to compare their <span class="math inline">\(\beta\)</span>s without standardising the measurement units. Standardised scores (also referred to as <em>z-scores</em>) for a variable, imply that each variable has a distribution where the mean equals 0 and the standard deviation equals 1. By transforming your variables to z-scores makes them ‚Äòunit free‚Äô <span class="citation">(<a href="#ref-cohen2014applied" role="doc-biblioref">P. Cohen, West, and Aiken 2014, 25</a>)</span> and therefore we can easily compare them irrespective of their original measurement unites. Here is an example with a simplified dataset:</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="regression.html#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create some data</span></span>
<span id="cb80-2"><a href="regression.html#cb80-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>))</span>
<span id="cb80-3"><a href="regression.html#cb80-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-4"><a href="regression.html#cb80-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute z-scores</span></span>
<span id="cb80-5"><a href="regression.html#cb80-5" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> data <span class="sc">%&gt;%</span></span>
<span id="cb80-6"><a href="regression.html#cb80-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">x_scaled =</span> <span class="fu">scale</span>(x))</span>
<span id="cb80-7"><a href="regression.html#cb80-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-8"><a href="regression.html#cb80-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare the mean and sd for each variable</span></span>
<span id="cb80-9"><a href="regression.html#cb80-9" aria-hidden="true" tabindex="-1"></a><span class="co"># and put it into a nice table</span></span>
<span id="cb80-10"><a href="regression.html#cb80-10" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb80-11"><a href="regression.html#cb80-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">variable =</span> <span class="fu">c</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;x_scaled&quot;</span>),</span>
<span id="cb80-12"><a href="regression.html#cb80-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">mean =</span> <span class="fu">c</span>(<span class="fu">mean</span>(data<span class="sc">$</span>x), <span class="fu">mean</span>(data<span class="sc">$</span>x_scaled)),</span>
<span id="cb80-13"><a href="regression.html#cb80-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">sd =</span> <span class="fu">c</span>(<span class="fu">sd</span>(data<span class="sc">$</span>x), <span class="fu">sd</span>(data<span class="sc">$</span>x_scaled))</span>
<span id="cb80-14"><a href="regression.html#cb80-14" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## # A tibble: 2 √ó 3
##   variable  mean    sd
##   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;
## 1 x          2.5  1.29
## 2 x_scaled   0    1</code></pre>
<p>If you feel that my choice of values has likely affected the outcome, feel free to change the <code>x</code> values to whatever you like. The results for <code>x_scaled</code> will always remain the same. There is something else that remains the same: the actual distribution. Because we are only rescaling our data, we are obviously not affecting the differences between these scores. We can show this in a scatterplot and by running a correlation correlation. Both will show that both variables are perfectly correlated with each other, which means our transformations had not effect on the relative relationship of values to each other.</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="regression.html#cb82-1" aria-hidden="true" tabindex="-1"></a>data <span class="sc">%&gt;%</span></span>
<span id="cb82-2"><a href="regression.html#cb82-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x,</span>
<span id="cb82-3"><a href="regression.html#cb82-3" aria-hidden="true" tabindex="-1"></a>             <span class="at">y =</span> x_scaled)) <span class="sc">+</span></span>
<span id="cb82-4"><a href="regression.html#cb82-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb82-5"><a href="regression.html#cb82-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb82-6"><a href="regression.html#cb82-6" aria-hidden="true" tabindex="-1"></a>              <span class="at">formula =</span> y <span class="sc">~</span> x,</span>
<span id="cb82-7"><a href="regression.html#cb82-7" aria-hidden="true" tabindex="-1"></a>              <span class="at">se =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="r_for_non_programmers_files/figure-html/Similarities%20of%20scaled%20and%20unscaled%20data-1.png" width="672" /></p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="regression.html#cb83-1" aria-hidden="true" tabindex="-1"></a>correlation<span class="sc">::</span><span class="fu">correlation</span>(data)</span></code></pre></div>
<pre><code>## # Correlation Matrix (pearson-method)
## 
## Parameter1 | Parameter2 |    r |       95% CI | t(2) |         p
## ----------------------------------------------------------------
## x          |   x_scaled | 1.00 | [1.00, 1.00] |  Inf | &lt; .001***
## 
## p-value adjustment method: Holm (1979)
## Observations: 4</code></pre>
<p>For regressions, it is not a bad idea to report both, unstandardised (<span class="math inline">\(B\)</span>) and standardised (<span class="math inline">\(\beta\)</span>) values for each predictor. However, one could argue that it is a bit redundant. If you only want to report one estimate, it is common to provide the <span class="math inline">\(\beta\)</span> values rather than the unstandardised equivalent.</p>
<p>Unfortunately, this is not the end of the story, because ‚Äòstandardisation‚Äô can mean different things. We can generate standardised <span class="math inline">\(\beta\)</span> after the fact (post-hoc), or decide to ‚Äòrefit‚Äô our regression with standardised predictors, i.e.¬†before we run the regression. Both options are appropriate, but the scores will differ slightly. However, the main idea remains the same: We want to be able to compare different regression coefficients with each other. In R there are many different packages which offer standardised estimates. On package I particularly recommend is <code>parameters</code>. It offers different options of how to return our estimates from a linear model by specifying the <code>standardize</code> argument (see <a href="https://easystats.github.io/parameters/reference/model_parameters.default.html" target="blank" title="detailed documentation">detailed documentation</a>). Of course, you can also opt to scale the variables by hand, using the <code>scale()</code> function, like we did in the previous example. Both approaches are shown in the following code chunk.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="regression.html#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Scaling &#39;by hand&#39;</span></span>
<span id="cb85-2"><a href="regression.html#cb85-2" aria-hidden="true" tabindex="-1"></a>m3_scaled <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">scale</span>(new_cases) <span class="sc">~</span></span>
<span id="cb85-3"><a href="regression.html#cb85-3" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">scale</span>(schools) <span class="sc">+</span></span>
<span id="cb85-4"><a href="regression.html#cb85-4" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">scale</span>(businesses),</span>
<span id="cb85-5"><a href="regression.html#cb85-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> m1_no_outliers)</span>
<span id="cb85-6"><a href="regression.html#cb85-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-7"><a href="regression.html#cb85-7" aria-hidden="true" tabindex="-1"></a>parameters<span class="sc">::</span><span class="fu">model_parameters</span>(m3_scaled)</span></code></pre></div>
<pre><code>## Parameter   | Coefficient |   SE |        95% CI |  t(1149) |      p
## --------------------------------------------------------------------
## (Intercept) |    1.31e-15 | 0.03 | [-0.05, 0.05] | 4.91e-14 | &gt; .999
## schools     |        0.29 | 0.03 | [ 0.23, 0.34] |     9.90 | &lt; .001
## businesses  |        0.21 | 0.03 | [ 0.16, 0.27] |     7.38 | &lt; .001</code></pre>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="regression.html#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Scaling using &#39;parameters&#39; package</span></span>
<span id="cb87-2"><a href="regression.html#cb87-2" aria-hidden="true" tabindex="-1"></a>parameters<span class="sc">::</span><span class="fu">model_parameters</span>(m3, <span class="at">standardize =</span> <span class="st">&quot;refit&quot;</span>)</span></code></pre></div>
<pre><code>## Parameter   | Coefficient |   SE |        95% CI |  t(1149) |      p
## --------------------------------------------------------------------
## (Intercept) |    1.31e-15 | 0.03 | [-0.05, 0.05] | 4.91e-14 | &gt; .999
## schools     |        0.29 | 0.03 | [ 0.23, 0.34] |     9.90 | &lt; .001
## businesses  |        0.21 | 0.03 | [ 0.16, 0.27] |     7.38 | &lt; .001</code></pre>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="regression.html#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Scaling using `parameters` package without refitting the model</span></span>
<span id="cb89-2"><a href="regression.html#cb89-2" aria-hidden="true" tabindex="-1"></a>parameters<span class="sc">::</span><span class="fu">model_parameters</span>(m3, <span class="at">standardize =</span> <span class="st">&quot;posthoc&quot;</span>)</span></code></pre></div>
<pre><code>## Parameter   | Std. Coef. |   SE |         95% CI | t(1149) |      p
## -------------------------------------------------------------------
## (Intercept) |       0.00 | 0.00 | [ 0.00,  0.00] |   -2.41 | 0.016 
## schools     |       0.29 | 0.03 | [ 0.23,  0.34] |    9.90 | &lt; .001
## businesses  |       0.21 | 0.03 | [ 0.16,  0.27] |    7.38 | &lt; .001</code></pre>
<p>Personally, I prefer to standardise using the <code>parameters</code> package, because it makes my life easier and I need to write less code. I can also easily compare results prior to scaling my variables. The choice is yours, of course.</p>
</div>
<div id="multicollinearity-the-dilemma-of-highly-correlated-independent-variables" class="section level3" number="12.2.3">
<h3><span class="header-section-number">12.2.3</span> Multicollinearity: The dilemma of highly correlated independent variables</h3>
<p>We finished fitting our model and we are likely exhausted, but also happy that we accomplished something. However, we are not yet done with our analysis. One of the most important post-tests for multiple regressions is a test for <em>multicollinearity</em> or sometimes referred to as <em>collinearity</em>. The phenomenon of multicollinearity defines a situation in which some of our independent variables can be explained by other independent variables in our regression model. In other words, there exists a very large correlation (a linear relationship) between two or more independent variables. It is important to not confuse this with correlations between dependent variable and independent variables. Remember, a regression reflects the linear relationship between the predictor variables and the outcome variable. As such, we do hope to find a correlation between these variables, just not among the independent variables.</p>
<p>If we have evidence that multicollinearity exists in our data, we face some problems:</p>
<ul>
<li><p>We cannot trust our regression coefficients, i.e.¬†our estimates (<span class="math inline">\(\beta\)</span>).</p></li>
<li><p>Since the same variance can be explained by two different independent variables, it is unclear which one is important, since both can be easily interchanged without affecting the model all that much.</p></li>
<li><p>It is impossible to test any hypotheses, since we cannot trust our estimates.</p></li>
<li><p>We also likely underestimate the variance our model can explain, i.e.¬†our <span class="math inline">\(R^2\)</span>.</p></li>
<li><p>We produce more Type II errors, i.e.¬†we likely reject important predictors due to statistical insignificance. (see also <span class="citation"><a href="#ref-cohen2014applied" role="doc-biblioref">P. Cohen, West, and Aiken</a> (<a href="#ref-cohen2014applied" role="doc-biblioref">2014</a>)</span>, <span class="citation"><a href="#ref-field2013discovering" role="doc-biblioref">Field</a> (<a href="#ref-field2013discovering" role="doc-biblioref">2013</a>)</span>, <span class="citation"><a href="#ref-sage-methods2004dbw" role="doc-biblioref">Grandstrand</a> (<a href="#ref-sage-methods2004dbw" role="doc-biblioref">2004</a>)</span>)</p></li>
</ul>
<p>The Variance Inflation Factor (VIF) and it little sibling Tolerance are methods to identify issues of multicollinearity. Compared to detecting outliers, it is very simple to compute these indicators and surprisingly uncomplicated. The package <code>performance</code> offers a simple function called <code>check_collinearity()</code> which provides both. The tolerance is computed as <span class="math inline">\(\frac{1}{VIF}\)</span>.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="regression.html#cb91-1" aria-hidden="true" tabindex="-1"></a>performance<span class="sc">::</span><span class="fu">check_collinearity</span>(m3)</span></code></pre></div>
<pre><code>## # Check for Multicollinearity
## 
## Low Correlation
## 
##        Term  VIF Increased SE Tolerance
##     schools 1.18         1.09      0.85
##  businesses 1.18         1.09      0.85</code></pre>
<p>As the output already indicates, there is a <code>Low Correlation</code> between our independent variables. So, good news for us. In terms of interpretations we find the following benchmarks as recommendations to determine multicollinearity:</p>
<ul>
<li><p><code>VIF &gt; 10</code>: Evidence for multicollinearity</p></li>
<li><p><code>mean(VIF) &gt; 1</code>: If the mean of all VIFs lies substantially above <code>1</code>, multicollinearity might be an issue.</p></li>
<li><p><code>Tolerance &lt; 0.1</code>: Multicollinearity is a serious concern. This is the same condition as <code>VIF &gt; 10</code>.</p></li>
<li><p><code>Tolerance &lt; 0.2</code>: Multicollinearity could be a concern.</p></li>
</ul>
<p>However, <span class="citation"><a href="#ref-cohen2014applied" role="doc-biblioref">P. Cohen, West, and Aiken</a> (<a href="#ref-cohen2014applied" role="doc-biblioref">2014</a>)</span> warns to not rely on these indicators alone. There is sufficient evidence that lower VIF scores could also cause issues. Thus, it is always important to still investigate relationships of independent variables statistically (e.g.¬†correlation) and visually (e.g.¬†scatterplots). For example, outliers can often be a cause for too high or too low VIFs. In short, the simplicity in computation of these indicators should not be mistaken as a convenient shortcut.</p>
<p>Besides the VIF and tolerance, multicollinearity can also exist among residuals, i.e.¬†the error terms that are associated with each predictor. Regressions assume that errors are independent from each other. The Durbin-Watson Test offers an equally easy way to detect multicollinearity among residuals. We can use the <code>car</code> package and the function <code>durbinWatsonTest()</code> to retrieve the relevant information.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="regression.html#cb93-1" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">durbinWatsonTest</span>(m3) <span class="sc">%&gt;%</span></span>
<span id="cb93-2"><a href="regression.html#cb93-2" aria-hidden="true" tabindex="-1"></a>  broom<span class="sc">::</span><span class="fu">tidy</span>()</span></code></pre></div>
<pre><code>## # A tibble: 1 √ó 5
##   statistic p.value autocorrelation method             alternative
##       &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;      
## 1     0.135       0           0.930 Durbin-Watson Test two.sided</code></pre>
<p>The <code>statistic</code> of the Durbin-Watson Test can range from 0 to 4, where 2 indicates no correlation. Luckily for us, we do not have to guess whether the difference from our computation is significantly different from 2, because we also get the <code>p.value</code>. In our case, we find that our model suffers from multicollinearity of error terms.</p>
<p>If we wanted to remedy multicollinearity we could consider for example:</p>
<ul>
<li><p>Revisiting the regression model and potentially dropping those variables we know measure the same or similar underlying factors <span class="citation">(<a href="#ref-cohen2014applied" role="doc-biblioref">P. Cohen, West, and Aiken 2014</a>)</span>.</p></li>
<li><p>We could collect more data, because a larger dataset will always increase the precision of the regression coefficients <span class="citation">(<a href="#ref-cohen2014applied" role="doc-biblioref">P. Cohen, West, and Aiken 2014</a>)</span>.</p></li>
<li><p>Use different modelling techniques <span class="citation">(<a href="#ref-cohen2014applied" role="doc-biblioref">P. Cohen, West, and Aiken 2014</a>)</span> which can resolve such issues, for example using generalised least squares (GLS) and transform some variables of concern <span class="citation">(<a href="#ref-sage-methods2004dbw" role="doc-biblioref">Grandstrand 2004</a>)</span>.</p></li>
</ul>
<p>The cause for this multicollinearity of residuals is often rooted in either ‚Äòclustered data‚Äô or a ‚Äòserial dependency‚Äô <span class="citation">(<a href="#ref-cohen2014applied" role="doc-biblioref">P. Cohen, West, and Aiken 2014</a>)</span>. In our dataset, both could apply. First, we have data that was collected over time which could lead to wrong standard errors. Second, we have data from two different countries (<code>United Kingdom</code> and <code>Germany</code>). Thus, our data might be clustered.</p>
<p>In order to remove serial dependency effects we would have to transform data and account for the correlation across time. Such a technique is shown in <span class="citation"><a href="#ref-cohen2014applied" role="doc-biblioref">P. Cohen, West, and Aiken</a> (<a href="#ref-cohen2014applied" role="doc-biblioref">2014</a>)</span> (p.149). To counteract the issue of clustered data, we need to use multilevel regression models, also known as hierarchical regressions, which we will cover in the next chapter.</p>
</div>
</div>
<div id="hierarchical-regression" class="section level2" number="12.3">
<h2><span class="header-section-number">12.3</span> Hierarchical regression</h2>
<p>Adding independent variables to a regression model often happens in a stepwise approach, i.e.¬†we do not add all variables at once. Instead we might add variables that are most important (based on prior research) first, run the regression and examine the results. After that we add more variables that could possibly add value to our regression model and run the regression again. We then have two models which we can compare and see whether we improved it.</p>
<p>In hierarchical regressions, we most frequently distinguish three types of independent variables which also reflect the order in which we add variables to a multiple regression:</p>
<ol style="list-style-type: decimal">
<li><p><em>control variables</em>: These are independent variables which might affect the dependent variable in some way and we want to make sure its effects are accounted for. Control variables tend to be not the primary focus of a study, but ensure that effects of other independent variables (the main effects) are not spurious. Control variables are added first to a regression.</p></li>
<li><p><em>main effects variables</em>: These are the independent variables that the researcher expects to best predict the independent variable. Main effects variables are added after any control variables.</p></li>
<li><p><em>moderating variables</em>: These are variables that attenuate the strength of the relationship between independent and depend variables. They are also referred to as <em>interactions</em> or <em>interaction terms.</em> Moderating variables are added last, i.e.¬†after main effects variables and control variables.</p></li>
</ol>
<p>In the field of Social Sciences, it is fairly rare to not find control variables and/or moderators in multiple regressions, because the social phenomena we tend to investigate are usually affected by other factors as well. Classic control variables or moderators are socio-demographic variables, such as age and gender. The following two chapters cover control variables and moderation effects separately from each other. However, it is not unusual to find both types of variables in the same regression model. They are not mutually exclusive approaches, but simply different types of independent variables as shown above.</p>
<div id="regressions-with-control-variables" class="section level3" number="12.3.1">
<h3><span class="header-section-number">12.3.1</span> Regressions with control variables</h3>
<p>Hierarchical regression imply that we added variables step-by-step, or as some call it ‚Äòblock-wise.‚Äô Each block represents a group of variables. Control variables tend to be the first ones to be added to a regression model. However, there are many other ways to perform multiple regression, for example starting with all variables and removing those that are not significant, which <span class="citation"><a href="#ref-cohen2014applied" role="doc-biblioref">P. Cohen, West, and Aiken</a> (<a href="#ref-cohen2014applied" role="doc-biblioref">2014</a>)</span> calls a ‚Äòtear-down‚Äô approach (p.¬†158). As the title indicates, we take less of an exploratory approach to our analysis, because we define the hierarchy, i.e.¬†the order, in which we enter variables. In the Social Sciences this is very often the preferred method, which is why I cover it in greater detail.</p>
<p>However, I should probably explain what the purpose of entering variables in a stepwise approach<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> is. As we just discussed in the previous chapter, sometimes we face issues of multicollinearity which makes it difficult to understand which variables are more important than others for our model. As such we can decide to enter variables which we want to control for first and then create another model which contains all variables. You will often find the term ‚Äòcontrol variable‚Äô associated with those independent variables that are not the main focus of the study, but likely exert an impact on the other independent variables and therefore on our model. The procedure is fairly straight forward for our research focus of predicting <code>new_cases</code> of COVID-19:</p>
<ol style="list-style-type: decimal">
<li>Create a model <code>m1</code> (or whichever name you want to give it) which only contains the dependent variable <code>new_cases</code> and a control variable <code>country</code>.</li>
<li>Inspect the results of this model and note down the performance measures.</li>
<li>Create another model <code>m2</code> and include the control variable <code>country</code> and all other independent variables of interest, i.e.¬†<code>schools</code>, and <code>businesses</code>.</li>
<li>Inspect the results of this model and note down the performance measures.</li>
<li>Compare the <code>m1</code> and <code>m2</code> to see whether they are significantly different from each other. We can use <code>anova()</code> to perform this step.</li>
</ol>
<p>Let‚Äôs put these steps into action and start with formulating our first model. I choose <code>country</code> as a control variable, because we have sufficient evidence that clustered data could be a reason for the high autocorrelation of residuals we found in Chapter <a href="regression.html#multicollinearity-the-dilemma-of-highly-correlated-independent-variables">12.2.3</a>. For all computations in this section we use the original dataset, i.e.¬†<code>covid_uk_ger</code>, because we changed the model and therefore would have to revisit outliers from scratch, which we shall skip at this point. We also have to remove observations with missing data to allow comparisons of models in a statistical way. We begin by first selecting our variables of interest and then remove missing data.</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="regression.html#cb95-1" aria-hidden="true" tabindex="-1"></a>hr_data <span class="ot">&lt;-</span> covid_uk_ger <span class="sc">%&gt;%</span></span>
<span id="cb95-2"><a href="regression.html#cb95-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(new_cases, country, schools, businesses) <span class="sc">%&gt;%</span></span>
<span id="cb95-3"><a href="regression.html#cb95-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">droplevels</span>() <span class="sc">%&gt;%</span></span>
<span id="cb95-4"><a href="regression.html#cb95-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na.omit</span>()</span></code></pre></div>
<p>You find a new function called <code>droplevels()</code> in this code chunk. What this function does it removes unused factor levels from a factor. To demonstrate this more clearly, let me give you an example:</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="regression.html#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="co"># A factor with all levels</span></span>
<span id="cb96-2"><a href="regression.html#cb96-2" aria-hidden="true" tabindex="-1"></a>(gender <span class="ot">&lt;-</span> <span class="fu">as_factor</span>(<span class="fu">c</span>(<span class="st">&quot;female&quot;</span>, <span class="st">&quot;male&quot;</span>, <span class="st">&quot;NA&quot;</span>)))</span></code></pre></div>
<pre><code>## [1] female male   NA    
## Levels: female male NA</code></pre>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="regression.html#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Subset our data by picking the first two values only</span></span>
<span id="cb98-2"><a href="regression.html#cb98-2" aria-hidden="true" tabindex="-1"></a>(gender2 <span class="ot">&lt;-</span> gender[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>])</span></code></pre></div>
<pre><code>## [1] female male  
## Levels: female male NA</code></pre>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="regression.html#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Dropping unused levels</span></span>
<span id="cb100-2"><a href="regression.html#cb100-2" aria-hidden="true" tabindex="-1"></a>(gender3 <span class="ot">&lt;-</span> gender[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>] <span class="sc">%&gt;%</span> <span class="fu">droplevels</span>())</span></code></pre></div>
<pre><code>## [1] female male  
## Levels: female male</code></pre>
<p>The problem we encounter is that even though we only have data points with <code>female</code> and <code>male</code> values in <code>gender2</code>, it still keeps the levels of <code>NA</code>, based on the original factor <code>gender</code>. In my experience, it is always good to remove unused categories, because sometimes they can lead to unexpected outcomes. Thus, it is a matter of good practice than a real necessity.</p>
<p>We have to recode factors into so called <a href="https://methods.sagepub.com/reference/the-sage-encyclopedia-of-educational-research-measurement-and-evaluation/i7682.xml?fromsearch=true" target="blank" title="dummy variables">dummy variables</a> or indicator variables. Dummy variables represent categories as <code>0</code>s (i.e.¬†FALSE for this observation) and <code>1</code>s (TRUE for this observation). The good news is, the <code>lm()</code> function will do this automatically for us. If you want to inspect the coding ex-ante you can use the function <code>contrasts().</code></p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="regression.html#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(hr_data<span class="sc">$</span>country)</span></code></pre></div>
<pre><code>##                United Kingdom
## Germany                     0
## United Kingdom              1</code></pre>
<p>The column reflects the coding and the rows represent the levels of our factor. If we had more levels, we will find that the coding will always be the number of levels minus 1. This is a common mistake that novices easily make. You might think you need to have a dummy variable for <code>Germany</code> (i.e.¬†0 and 1) and a dummy variable for <code>United Kingdom</code> (i.e.¬†0 and 1). However, all you really need is one variable, which tells us whether the <code>country</code> is the <code>United Kingdom</code> (i.e.¬†1) or not (i.e.¬†0).</p>
<p>If our control variable has more than two levels, for example by adding <code>Italy</code>, the dummy coding would change to the following:</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="regression.html#cb104-1" aria-hidden="true" tabindex="-1"></a>hr_uk_ger_ita <span class="ot">&lt;-</span> covid <span class="sc">%&gt;%</span></span>
<span id="cb104-2"><a href="regression.html#cb104-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(country <span class="sc">==</span> <span class="st">&quot;United Kingdom&quot;</span> <span class="sc">|</span></span>
<span id="cb104-3"><a href="regression.html#cb104-3" aria-hidden="true" tabindex="-1"></a>           country <span class="sc">==</span> <span class="st">&quot;Germany&quot;</span> <span class="sc">|</span></span>
<span id="cb104-4"><a href="regression.html#cb104-4" aria-hidden="true" tabindex="-1"></a>           country <span class="sc">==</span> <span class="st">&quot;Italy&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb104-5"><a href="regression.html#cb104-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">droplevels</span>()</span>
<span id="cb104-6"><a href="regression.html#cb104-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb104-7"><a href="regression.html#cb104-7" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(hr_uk_ger_ita<span class="sc">$</span>country)</span></code></pre></div>
<pre><code>##                Italy United Kingdom
## Germany            0              0
## Italy              1              0
## United Kingdom     0              1</code></pre>
<p>With this new dataset, our regression would include more control variables as well, because for each level of the factor (minus one level!) we create a new control variable. For an example of such a scenario, please take a look at the case study in Chapter <a href="#bootstrapped-regression"><strong>??</strong></a>.</p>
<p>Returning to our hierarchical regression we can start by building our first model, i.e.¬†<code>m0</code>, which only contains our control variable <code>country</code>.</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="regression.html#cb106-1" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> new_cases <span class="sc">~</span> country,</span>
<span id="cb106-2"><a href="regression.html#cb106-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">data =</span> hr_data)</span>
<span id="cb106-3"><a href="regression.html#cb106-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-4"><a href="regression.html#cb106-4" aria-hidden="true" tabindex="-1"></a>parameters<span class="sc">::</span><span class="fu">model_parameters</span>(m1, <span class="at">standardize =</span> <span class="st">&quot;posthoc&quot;</span>)</span></code></pre></div>
<pre><code>## Parameter                | Std. Coef. |   SE |       95% CI | t(1186) |      p
## ------------------------------------------------------------------------------
## (Intercept)              |       0.00 | 0.00 | [0.00, 0.00] |   13.97 | &lt; .001
## country [United Kingdom] |       0.37 | 0.06 | [0.25, 0.48] |    6.40 | &lt; .001</code></pre>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="regression.html#cb108-1" aria-hidden="true" tabindex="-1"></a>performance<span class="sc">::</span><span class="fu">model_performance</span>(m1)</span></code></pre></div>
<pre><code>## # Indices of model performance
## 
## AIC       |       BIC |    R2 | R2 (adj.) |      RMSE |     Sigma
## -----------------------------------------------------------------
## 25542.724 | 25557.964 | 0.033 |     0.033 | 11258.076 | 11267.564</code></pre>
<p>Our control variable turns out to be significant for our model, but it explains only a small proportion of the variance in <code>new_cases</code>. If you are testing hypotheses, you would consider this a good result, because you do not want your control variables to explain too much of the variance. At the same time, it is a significant variable and should be retained in our model. Let‚Äôs construct our next model <code>m1</code> by adding the main effects variables and then compare our models.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="regression.html#cb110-1" aria-hidden="true" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> new_cases <span class="sc">~</span></span>
<span id="cb110-2"><a href="regression.html#cb110-2" aria-hidden="true" tabindex="-1"></a>           schools <span class="sc">+</span></span>
<span id="cb110-3"><a href="regression.html#cb110-3" aria-hidden="true" tabindex="-1"></a>           businesses <span class="sc">+</span></span>
<span id="cb110-4"><a href="regression.html#cb110-4" aria-hidden="true" tabindex="-1"></a>           country,</span>
<span id="cb110-5"><a href="regression.html#cb110-5" aria-hidden="true" tabindex="-1"></a>         <span class="at">data =</span> hr_data)</span>
<span id="cb110-6"><a href="regression.html#cb110-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb110-7"><a href="regression.html#cb110-7" aria-hidden="true" tabindex="-1"></a>parameters<span class="sc">::</span><span class="fu">compare_parameters</span>(m1, m2, <span class="at">standardize =</span> <span class="st">&quot;refit&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb110-8"><a href="regression.html#cb110-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span></span>
<span id="cb110-9"><a href="regression.html#cb110-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Parameter, Coefficient.m1, p.m1, Coefficient.m2, p.m2) <span class="sc">%&gt;%</span> </span>
<span id="cb110-10"><a href="regression.html#cb110-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">where</span>(is.double), round, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>## # A tibble: 5 √ó 5
##   Parameter                Coefficient.m1  p.m1 Coefficient.m2  p.m2
##   &lt;chr&gt;                             &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;
## 1 (Intercept)                      -0.183     0         -0.235     0
## 2 country (United Kingdom)          0.365     0         NA        NA
## 3 schools                          NA        NA          0.328     0
## 4 businesses                       NA        NA          0.246     0
## 5 country (United Kingdom)         NA        NA          0.469     0</code></pre>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="regression.html#cb112-1" aria-hidden="true" tabindex="-1"></a>performance<span class="sc">::</span><span class="fu">compare_performance</span>(m1, m2)</span></code></pre></div>
<pre><code>## # Comparison of Model Performance Indices
## 
## Name | Model |       AIC |       BIC |    R2 | R2 (adj.) |      RMSE |     Sigma
## --------------------------------------------------------------------------------
## m1   |    lm | 25542.724 | 25557.964 | 0.033 |     0.033 | 11258.076 | 11267.564
## m2   |    lm | 25234.099 | 25259.499 | 0.257 |     0.255 |  9870.095 |  9886.753</code></pre>
<p>After adding all our variables, <span class="math inline">\(adjusted\ R^2\)</span> went up from 0.033 to 0.255. While this might seem like a huge improvement, we have to perform a statistical test to compare the two models. This leads us back to comparing groups and in many ways this is what we do here by using the function <code>anova()</code>, but we compare models based on the residual sum of squares, i.e.¬†the amount of error the models produce. Thus, if the ANOVA returns a significant result, <code>m1</code> shows a significantly reduced residual sum of squares compared to <code>m1</code>. Thus, <code>m2</code> would be the better model.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="regression.html#cb114-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m1, m2)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: new_cases ~ country
## Model 2: new_cases ~ schools + businesses + country
##   Res.Df        RSS Df  Sum of Sq      F    Pr(&gt;F)    
## 1   1186 1.5057e+11                                   
## 2   1184 1.1573e+11  2 3.4839e+10 178.21 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The results show that our model significantly improved. You might argue that this is not surprising, because we added those variables which already worked in the final model of Chapter <a href="regression.html#multicollinearity-the-dilemma-of-highly-correlated-independent-variables">12.2.3</a>. However, the important takeaway here is that our control variable <code>country</code> helps us explain some more variance in <code>new_cases</code>. Comparing the model with and without the control variable, we would find that the <span class="math inline">\(adjusted\ R^2\)</span> improves our model by about 25%. As such, it is worth keeping it as part of our final regression model. Here is evidence of this improvement:</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="regression.html#cb116-1" aria-hidden="true" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> new_cases <span class="sc">~</span></span>
<span id="cb116-2"><a href="regression.html#cb116-2" aria-hidden="true" tabindex="-1"></a>           schools <span class="sc">+</span></span>
<span id="cb116-3"><a href="regression.html#cb116-3" aria-hidden="true" tabindex="-1"></a>           businesses,</span>
<span id="cb116-4"><a href="regression.html#cb116-4" aria-hidden="true" tabindex="-1"></a>         <span class="at">data =</span> hr_data)</span>
<span id="cb116-5"><a href="regression.html#cb116-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb116-6"><a href="regression.html#cb116-6" aria-hidden="true" tabindex="-1"></a>performance<span class="sc">::</span><span class="fu">compare_performance</span>(m0, m2)</span></code></pre></div>
<pre><code>## # Comparison of Model Performance Indices
## 
## Name | Model |       AIC |       BIC |    R2 | R2 (adj.) |      RMSE |     Sigma
## --------------------------------------------------------------------------------
## m0   |    lm | 25309.145 | 25329.465 | 0.207 |     0.206 | 10195.397 | 10208.294
## m2   |    lm | 25234.099 | 25259.499 | 0.257 |     0.255 |  9870.095 |  9886.753</code></pre>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="regression.html#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m0, m2)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: new_cases ~ schools + businesses
## Model 2: new_cases ~ schools + businesses + country
##   Res.Df        RSS Df  Sum of Sq      F    Pr(&gt;F)    
## 1   1185 1.2349e+11                                   
## 2   1184 1.1573e+11  1 7754483300 79.332 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>There are many more combinations of models we could test. For example, instead of specifying two models (<code>m1</code>, <code>m2</code>) we could also add independent variables one at a time and compare it to a baseline model (<code>m0</code>) which does contain the control variable, which means we end up with more models to compare, for example:</p>
<div id="four-hierarchical-regressions-nested">
<p>m1 &lt;- lm(formula = new_cases ~ country, data = hr_data)</p>
<p>m2 &lt;- lm(formula = new_cases ~ country + schools, data = hr_data)</p>
<p>m3 &lt;- lm(formula = new_cases ~ country + schools + businesses, data = hr_data)</p>
</div>
<p>All these decisions have to be guided by the purpose of your research and whether you explore your data or have specified hypotheses. However, the steps largely remain the same in terms of computation in R.</p>
</div>
<div id="moderated-regression" class="section level3" number="12.3.2">
<h3><span class="header-section-number">12.3.2</span> Moderated regression</h3>
<p>While regressions with control variables help us primarily make decisions on whether we should include or exclude variables by controling for another variable, moderation models imply that we expect a certain interaction between an independent variable and a so-called moderator. A moderator also enters the equation on the right-hand side and therefore constitutes an independent variable, but is usually entered after control variables and main effects variables..</p>
<p>IIn our quest to find a model that predcits new COVID cases, we change our mind and not assume that country is a control variable, but a moderating one. We could suspect that the measures for <code>businesses</code> and <code>schools</code> were significantly differently implemented in each of the countries. Therefore, the strength of the relationship between <code>businesses</code>/<code>schools</code> and <code>new_cases</code> varies depending on the country we look at.</p>
<p>When performing a moderation regression, we assume that the relationship between independent variables and dependent variable differs based on a third variable, in our case <code>country</code>. If we visualise the idea of moderation we would plot the data against each other for each country separately and fit two (instead of one) regression line by defining the colours of each group with <code>col = country</code>.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="regression.html#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the interaction between schools and country</span></span>
<span id="cb120-2"><a href="regression.html#cb120-2" aria-hidden="true" tabindex="-1"></a>hr_data <span class="sc">%&gt;%</span></span>
<span id="cb120-3"><a href="regression.html#cb120-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> schools,</span>
<span id="cb120-4"><a href="regression.html#cb120-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">y =</span> new_cases,</span>
<span id="cb120-5"><a href="regression.html#cb120-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">col =</span> country)) <span class="sc">+</span></span>
<span id="cb120-6"><a href="regression.html#cb120-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_jitter</span>(<span class="at">width =</span> <span class="dv">5</span>,</span>
<span id="cb120-7"><a href="regression.html#cb120-7" aria-hidden="true" tabindex="-1"></a>              <span class="at">size =</span> <span class="fl">0.5</span>,</span>
<span id="cb120-8"><a href="regression.html#cb120-8" aria-hidden="true" tabindex="-1"></a>              <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb120-9"><a href="regression.html#cb120-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb120-10"><a href="regression.html#cb120-10" aria-hidden="true" tabindex="-1"></a>              <span class="at">formula =</span> y <span class="sc">~</span> x,</span>
<span id="cb120-11"><a href="regression.html#cb120-11" aria-hidden="true" tabindex="-1"></a>              <span class="at">se =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:plotting-moderation-effects-schools"></span>
<img src="r_for_non_programmers_files/figure-html/plotting-moderation-effects-schools-1.png" alt="Plotting moderation effects for `schools`" width="672" />
<p class="caption">
Figure 12.1: Plotting moderation effects for <code>schools</code>
</p>
</div>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="regression.html#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the interaction between schools and country</span></span>
<span id="cb121-2"><a href="regression.html#cb121-2" aria-hidden="true" tabindex="-1"></a>hr_data <span class="sc">%&gt;%</span></span>
<span id="cb121-3"><a href="regression.html#cb121-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> businesses,</span>
<span id="cb121-4"><a href="regression.html#cb121-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">y =</span> new_cases,</span>
<span id="cb121-5"><a href="regression.html#cb121-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">col =</span> country)) <span class="sc">+</span></span>
<span id="cb121-6"><a href="regression.html#cb121-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_jitter</span>(<span class="at">width =</span> <span class="dv">5</span>,</span>
<span id="cb121-7"><a href="regression.html#cb121-7" aria-hidden="true" tabindex="-1"></a>              <span class="at">size =</span> <span class="fl">0.5</span>,</span>
<span id="cb121-8"><a href="regression.html#cb121-8" aria-hidden="true" tabindex="-1"></a>              <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb121-9"><a href="regression.html#cb121-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb121-10"><a href="regression.html#cb121-10" aria-hidden="true" tabindex="-1"></a>              <span class="at">formula =</span> y <span class="sc">~</span> x,</span>
<span id="cb121-11"><a href="regression.html#cb121-11" aria-hidden="true" tabindex="-1"></a>              <span class="at">se =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:plotting-moderation-effects-businesses"></span>
<img src="r_for_non_programmers_files/figure-html/plotting-moderation-effects-businesses-1.png" alt="Plotting moderation effects for `businesses`" width="672" />
<p class="caption">
Figure 12.2: Plotting moderation effects for <code>businesses</code>
</p>
</div>
<p>There are three interesting insights we can gain from these plots:</p>
<ol style="list-style-type: decimal">
<li>In terms of <code>business</code> measures, the <code>United Kingdom</code> seems to have more observations at the lower end, i.e.¬†taking less protective measures than Germany, while for <code>schools</code> it is rather the opposite.</li>
<li>For <code>schools</code> the UK reports more frequently higher numbers of <code>new_cases</code> even if measures taken are high, which results in a steeper regression line.</li>
<li>However, for <code>businesses</code>, the <code>United Kingdom</code> has barely any <code>new_cases</code> when the measures were high, but Germany still reports high numbers of new cases when tight measures were taken.</li>
</ol>
<p>Given the difference in the slope of the regression lines, we have to assume that the <span class="math inline">\(\beta\)</span>s for <code>Germany</code> are quite different from the ones for the <code>United Kingdom</code>. It seems that the relationship between <code>new_cases</code> and <code>business</code>/<code>schools</code> can be partially explained by <code>country</code>. Therefore, we could include <code>country</code> as a moderator and introduce the interaction of these variables with each other into a new model, i.e.¬†<code>m4</code>.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="regression.html#cb122-1" aria-hidden="true" tabindex="-1"></a>m4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> new_cases <span class="sc">~</span></span>
<span id="cb122-2"><a href="regression.html#cb122-2" aria-hidden="true" tabindex="-1"></a>           schools <span class="sc">+</span></span>
<span id="cb122-3"><a href="regression.html#cb122-3" aria-hidden="true" tabindex="-1"></a>           businesses <span class="sc">+</span></span>
<span id="cb122-4"><a href="regression.html#cb122-4" aria-hidden="true" tabindex="-1"></a>           schools <span class="sc">*</span> country <span class="sc">+</span>  <span class="co"># moderator 1</span></span>
<span id="cb122-5"><a href="regression.html#cb122-5" aria-hidden="true" tabindex="-1"></a>           businesses <span class="sc">*</span> country, <span class="co"># moderator 2</span></span>
<span id="cb122-6"><a href="regression.html#cb122-6" aria-hidden="true" tabindex="-1"></a>         <span class="at">data =</span> hr_data)</span>
<span id="cb122-7"><a href="regression.html#cb122-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb122-8"><a href="regression.html#cb122-8" aria-hidden="true" tabindex="-1"></a>parameters<span class="sc">::</span><span class="fu">compare_parameters</span>(m0, m4, <span class="at">standardize =</span> <span class="st">&quot;refit&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb122-9"><a href="regression.html#cb122-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span></span>
<span id="cb122-10"><a href="regression.html#cb122-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Parameter, Coefficient.m0, p.m0, Coefficient.m4, p.m4) <span class="sc">%&gt;%</span> </span>
<span id="cb122-11"><a href="regression.html#cb122-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="fu">across</span>(<span class="fu">where</span>(is.double), round, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>## # A tibble: 6 √ó 5
##   Parameter                            Coefficient.m0  p.m0 Coefficient.m4  p.m4
##   &lt;chr&gt;                                         &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;
## 1 (Intercept)                                   0         1         -0.229 0    
## 2 schools                                       0.366     0          0.077 0.104
## 3 businesses                                    0.168     0          0.183 0    
## 4 country (United Kingdom)                     NA        NA          0.546 0    
## 5 schools * country (United Kingdom)           NA        NA          0.274 0    
## 6 businesses * country (United Kingdo‚Ä¶         NA        NA          0.371 0</code></pre>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="regression.html#cb124-1" aria-hidden="true" tabindex="-1"></a>performance<span class="sc">::</span><span class="fu">compare_performance</span>(m0, m4)</span></code></pre></div>
<pre><code>## # Comparison of Model Performance Indices
## 
## Name | Model |       AIC |       BIC |    R2 | R2 (adj.) |      RMSE |     Sigma
## --------------------------------------------------------------------------------
## m0   |    lm | 25309.145 | 25329.465 | 0.207 |     0.206 | 10195.397 | 10208.294
## m4   |    lm | 25149.907 | 25185.468 | 0.310 |     0.307 |  9510.456 |  9534.564</code></pre>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="regression.html#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m0, m4)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: new_cases ~ schools + businesses
## Model 2: new_cases ~ schools + businesses + schools * country + businesses * 
##     country
##   Res.Df        RSS Df  Sum of Sq      F    Pr(&gt;F)    
## 1   1185 1.2349e+11                                   
## 2   1182 1.0745e+11  3 1.6035e+10 58.795 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The moderation effect is quite strong. Compared to our baseline model <code>m0</code>, which only contains <code>businesses</code> and <code>schools</code> as predictors, our new model <code>m4</code> considerably outperforms it with an <span class="math inline">\(adjusted\ R^2 = 0.307\)</span>. If we consider our previous model with country as a control variable, we managed to improve the accuracy our model from an <span class="math inline">\(adjusted\ R^2 = 0.255\)</span>. I would argue this is quite an improvement.</p>
<p>While significant moderation effects help us to make better predictions with our model, they come with a caveat: Interpreting the relative importance of the main effects becomes considerably more difficult, because their effect is depending on the level of our moderation variable. As such, you cannot really interpret the main effects without considering the moderation variable as well. You might have noticed that our variable <code>schools</code> is not significant anymore. However, this does not imply it is not important, because its significance depends on the level of our moderator, i.e.¬†whether we look at <code>Germany</code> or the <code>United Kingdom</code>. The insignificance of <code>schools</code> results from the cross-over interaction that we have shown in our plot in Figure <a href="regression.html#fig:plotting-moderation-effects-schools">12.1</a>.</p>
</div>
</div>
<div id="other-regression-models-alternatives-to-ols" class="section level2" number="12.4">
<h2><span class="header-section-number">12.4</span> Other regression models: Alternatives to OLS</h2>
<p>So far we covered linear regressions which are considered Ordinary Least Squares (OLS) regression models. These are by far the most frequently used models in the Social Sciences. However, what can we do if the assumptions of OLS models is violated, e.g.¬†if our dependent variable is dichotomous and not continuous? In such cases we might have to consider other types of regression models. There are several different approaches but we most frequently find one of the following:</p>
<ul>
<li><p>Polynomial regressions, which fit multiple linear regressions onto a curvilinear relationship</p></li>
<li><p>Generalised Linear models, such as Logistic regressions or Poisson regressions.</p></li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-boehmke2019hands" class="csl-entry">
Boehmke, Brad, and Brandon Greenwell. 2019. <em>Hands-on Machine Learning with r</em>. Chapman; Hall/CRC.
</div>
<div id="ref-cohen1988statistical" class="csl-entry">
Cohen, Jacob. 1988. <span>‚ÄúStatistical Power Analysis for the Behavioral Sciences New York.‚Äù</span> <em>NY: Academic</em>, 54.
</div>
<div id="ref-cohen2014applied" class="csl-entry">
Cohen, Patricia, Stephen G West, and Leona S Aiken. 2014. <em>Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences</em>. Psychology press.
</div>
<div id="ref-cook1982residuals" class="csl-entry">
Cook, R Dennis, and Sanford Weisberg. 1982. <em>Residuals and Influence in Regression</em>. New York: Chapman; Hall.
</div>
<div id="ref-field2013discovering" class="csl-entry">
Field, Andy. 2013. <em>Discovering Statistics Using IBM SPSS Statistics</em>. Sage Publications.
</div>
<div id="ref-gelman2020regression" class="csl-entry">
Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. <em>Regression and Other Stories</em>. Cambridge University Press.
</div>
<div id="ref-sage-methods2004dbw" class="csl-entry">
Grandstrand, Ove. 2004. <span>‚ÄúDurbin-Watson Statistic.‚Äù</span> In <em>The SAGE Encyclopedia of Social Science Research Methods</em>, edited by Bryman Lewis-Beck Michael S. Thousand Oaks, California. <a href="https://methods.sagepub.com/reference/the-sage-encyclopedia-of-social-science-research-methods">https://methods.sagepub.com/reference/the-sage-encyclopedia-of-social-science-research-methods</a>.
</div>
<div id="ref-hoaglin1978hat" class="csl-entry">
Hoaglin, David C, and Roy E Welsch. 1978. <span>‚ÄúThe Hat Matrix in Regression and ANOVA.‚Äù</span> <em>The American Statistician</em> 32 (1): 17‚Äì22.
</div>
<div id="ref-stevens2012applied" class="csl-entry">
Stevens, James P. 2012. <em>Applied Multivariate Statistics for the Social Sciences</em>. Routledge.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Germany is called ‚ÄòDeutschland‚Äô in German, hence the abbreivation ‚ÄòDEU‚Äô according to the <a href="https://www.iso.org/obp/ui/#iso:code:3166:DE" target="blank" title="ISO country code">ISO country code</a>.<a href="regression.html#fnref1" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn2"><p>This function does not take the model as an attribute, but instead requires the data, the column means and the covariance. Thus, we have to specify this function in the following way: <code>mahalanobis(data, colMeans(data), cov(data))</code><a href="regression.html#fnref2" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn3"><p>Function from <code>rstatix</code> package which automatically classifies values as outliers for us.<a href="regression.html#fnref3" class="footnote-back">‚Ü©Ô∏é</a></p></li>
<li id="fn4"><p>Stepwise approach does not refer to ‚ÄòStepwise‚Äô in SPSS regressions, i.e.¬†were the software decides which variables to enter. I would recommend to never let the software decide how to handle your variables in a multiple regression.<a href="regression.html#fnref4" class="footnote-back">‚Ü©Ô∏é</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="comparing-groups.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mixed-methods-research-analysing-qualitative-in-r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["r_for_non_programmers.pdf", "r_for_non_programmers.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

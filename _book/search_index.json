[["regression.html", "12 Regression: Creating models to predict future observations 12.1 Single linear regression 12.2 Multiple regression 12.3 Hierarchical regression", " 12 Regression: Creating models to predict future observations ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.4 ✓ dplyr 1.0.7 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 2.0.1 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() Regressions are an interesting area of data analysis since it enables us to make very specific predictions about the future incorporating different variables at the same time. As the name implies, regressions ‘regress,’ i.e. they draw on past observations to make predictions about future observations. Thus, any analysis incorporating a regression makes the implicit assumption that the future can be best explained by the past. I once heard someone refer to regressions as driving a car by looking at the rearview mirror. As long as the road is straight, we will be able to successfully navigate the car. However, if there is a sudden turn, we might drive into the abyss. This makes it very clear when and how regressions are can be helpful. Regressions are also a machine learning method, which falls under models with supervised learning. If you find machine learning fascinating, you might find the book “Hands-on Machine Learning with R” (Boehmke and Greenwell 2019) very insightful and interesting. In the following chapters we will cover three common types of regressions: Single linear regression Multiple regression Hierarchical regression These three types will allow you to perform any other type of linear regression you could think of. We can further distinguish two approaches to modelling via regressions: hypothesis testing: A regression model is defined ex-ante machine learning: A model is developed based on empirical data In the following chapters we will slightly blur the lines between both approaches. All our regressions will be performed using the covid dataset of the r4np package to investigate whether certain factors can predict COVID numbers in different countries. I felt, this book would not have been complete without covering this topic. After all, I wrote this book during the pandemic and it likely will mark a dark chapter in human history. 12.1 Single linear regression A single linear regression looks very similar to a correlation (see Chapter 10, but it is different in that it defines which variable affects another variable, i.e. a single direction relationship. I used the terms dependent variable (DV) and independent variable (IV) previously when comparing groups (see Chapter 11, and we will use them here again. In group comparisons, the independent variable was usually a factor, but in regressions we can use data that is not a categorical variable, i.e. integer, double, etc. While I understand that mathematical equations can be confusing, with regressions, they are fairly simple to understand. Also, when writing our models in R, we will continuously use a formula to specify our regression. A single linear regression consists of one independent variable and one dependent variable: \\[ DV = \\beta_{0} + IV * \\beta_{1} + error \\] Beta (\\(\\beta\\)) represents the coefficient of the independent variable, i.e. how much a change in IV causes a change in DV. For example, a one unit change in the IV might mean that the DV changes by two units of IV: \\[ DV = \\beta_0 + 2 * IV + error \\] If we ignore \\(\\beta_0\\) and \\(error\\) for a moment, we find that that if \\(IV = 3\\), our \\(DV = 2*3 = 6\\). Therefore, if \\(IV = 5\\), we find that \\(DV = 10\\), and so on. According to this model, DV will always be twice as large as IV. You might be wondering what \\(\\beta_0\\) stands for. It indicates an offset for each value, also called the intercept. Thus, no matter which value we choose for IV, DV will always be \\(\\beta_0\\) different from IV. It is a constant in our model. This can be best explained by visualising a regression line. Pay particular attention to the to the expressions after function(x) # A: Two models with different beta(0) ggplot() + geom_function(fun = function(x) 2 * x, colour = &quot;red&quot;) + geom_function(fun = function(x) 1 + 2 * x, colour = &quot;blue&quot;) + see::theme_modern() # B: Two models with the same beta(0), but different beta(1) ggplot() + geom_function(fun = function(x) 2 * x, colour = &quot;red&quot;) + geom_function(fun = function(x) 3 * x, colour = &quot;blue&quot;) + see::theme_modern() Plot B also shows what happens if we change \\(\\beta_1\\), i.e. the slope. The two models both have the same intercept (and therefore the same origin), but the blue line ascends quicker than the red one, because its \\(\\beta_1\\) is higher than the one for the red model. Lastly, the \\(error\\) component in the regression model refers to the deviation of data from this regression lines. Ideally, we want this value to be as small as possible. 12.1.1 Fitting a regression model by hand, i.e. trial and error If this sounds all awfully theoretical, let’s try to fit a regression model by hand. First we need to consider what our model should be able to predict. Let’s say that the number of COVID-19 cases predicts the number of deaths due to COVID-19. Intuitively we would assume this should be a linear relationship, because the more cases there are, the more likely we find more deaths caused by it. # We only select most recent numbers, i.e. &quot;2021-08-26&quot; # and countries which have COVID cases covid %&gt;% filter(date_reported == &quot;2021-08-26&quot; &amp; cumulative_cases != 0) %&gt;% ggplot(aes(x = cumulative_cases, y = cumulative_deaths)) + geom_point() This data visualisation shows us not much. We can see that there are three countries, which appear to have considerably more cases than most other countries. Thus, all other countries are crammed together in the bottom left corner. To improve this visualisation without removing the outliers, we can rescale the x and y axis using the function scale_x_continuous() and scale_y_continuous(). covid %&gt;% filter(date_reported == &quot;2021-08-26&quot; &amp; cumulative_cases != 0) %&gt;% ggplot(aes(x = cumulative_cases, y = cumulative_deaths)) + geom_point() + scale_x_continuous(trans = &quot;log&quot;) + scale_y_continuous(trans = &quot;log&quot;) As we can see, the scatterplot is now easier to read and the dots are more spread out. This reveals that there is quite a strong relationship between cumulative_cases and cumulative_deaths. However, similar to before, we should avoid outliers when performing our analysis. For the sake of simplicity, in this section I will limit the number of countries included in our analysis, which also removes the requirement of using scale_x_continuous() and scale_y_continuous(). covid_sample &lt;- covid %&gt;% filter(date_reported == &quot;2021-08-26&quot; &amp; cumulative_cases &gt;= 2500 &amp; cumulative_cases &lt;= 150000 &amp; cumulative_deaths &lt;= 3000) plot &lt;- covid_sample %&gt;% ggplot(aes(x = cumulative_cases, y = cumulative_deaths)) + geom_point() plot Through trial and error we can try to fit a linear line on top by adjusting the beta values. This is effectively what we hope to achieve with a regression: the best \\(\\beta\\) values which best explain our data. Let’s start with the basic assumption of \\(y = x\\) without specific \\(\\beta\\)s, i.e. they are zero. plot + geom_function(fun = function(x) x, colour = &quot;red&quot;) What we try to achieve is that the red line fits nicely inside the cloud of dots. Our very simple model provides a very poor fit to our data points, because all the dots are way below it. This makes sense, because \\(y = x\\) would imply that every COVID-19 case leads to a death, i.e. everyone with COVID did not survive. From our own experience we know that this is luckily not true. Ideally we want the line to be less steep, because our first model does not make much sense. We can do this by adding a \\(\\beta_1\\) to our equation. Maybe only 2% of people who got COVID-19 might not have recovered, i.e. \\(\\beta_1 = 0.02\\). plot + geom_function(fun = function(x) 0.02*x, colour = &quot;red&quot;) This time the line looks much more aligned with our observations. One could argue that it might have to move a little to the right as well, to cover the observations at the bottom a bit better. Therefore, we should add a \\(\\beta_0\\) to our equation, e.g. -50.= to move it to the right and tweak the \\(\\beta_1\\) ever so slightly. plot + geom_function(fun = function(x) -50 + 0.015*x, colour = &quot;red&quot;) We finished creating our regression model. If we wanted to express it as a formula we would write \\(DV = -5 + 0.015 * IV\\). We could now use this model to predict how high COVID cases likely will be in other countries. Estimating a regression model in this way is not ideal and far from accurate. Instead, we would compute the \\(\\beta\\)s based on our observed data, i.e. cumulative_cases and cumulative_deaths. We can use the function lm() to achieve this. I also rounded the all numeric values to two decimal places to make the output easier to read. We also use tidy() to retrieve a cleaner output from the computation. # classic r m0 &lt;- lm(cumulative_deaths ~ cumulative_cases, data = covid_sample) broom::tidy(m0) %&gt;% mutate(across(where(is.numeric), round, 2)) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 88.1 70.5 1.25 0.21 ## 2 cumulative_cases 0.01 0 10.7 0 We first might notice that the p.value indicates that the relationship between cumulative death and cumulative_cases is significant. Thus, we can conclude that countries with more COVID cases also suffer higher numbers of people who do not successfully recover from it. However, you might be wondering where our \\(\\beta\\) scores are. They are found where it says estimate. The standard error (std.error) denotes the error we specified in the previous equation. We find that in the first row we get the \\(\\beta_0\\), i.e. the one for the intercept which is 88.10. This one is larger than what we estimated, i.e. -50. However, \\(\\beta_1\\) is 0.01, which means we have done a very good job in guessing this estimate. Still, it becomes hopefully obvious that it is much easier to use the function lm() to estimate a model than ‘eyeballing it.’ We can now visualise the computed model (in blue) and our guessed model (in red) in one plot and see the differences. The plot shows that we have not been too far off. However, it was relatively easy to fit a model onto the observed data in this case. Often, it is much more difficult, especially when more than two variables are involved. plot + geom_function(fun = function(x) -50 + 0.015*x, colour = &quot;red&quot;) + geom_function(fun = function(x) 88.1 + 0.01*x, colour = &quot;blue&quot;) With our final model computed, we also need to check its quality in terms of predictive power based on how well it can actually explain our observed data. We have tested models before when we looked at confirmatory factor analyses for latent variables (see Chapter 7.7. This time we want to know how accurate our model is in explaining observed data and therefore how accurate it will be predicting future observations. The package performance offers a nice little shortcut to compute many different things at once: check_model(): Checks for linearity, homogeneity, collinearity and outliers model_performance(): Tests the quality of our model. For now, we are mainly interested in the performance of our model. So, we can compute it the following way: performance::model_performance(m0) ## # Indices of model performance ## ## AIC | BIC | R2 | R2 (adj.) | RMSE | Sigma ## ----------------------------------------------------------- ## 1472.626 | 1480.319 | 0.548 | 0.543 | 502.573 | 507.891 There are quite a number of performance indicators and here is how to read them: AIC stands for Akaike Information Criterion and the lower the score the better the model BIC stands for Bayesian Information Criterion and the lower the score the better the model R2 stands for R squared (\\(R^2\\)) and is also known as the coefficient of determination. It measures how much the independent variable can explain the variance in the dependent variable. In other words, the higher \\(R^2\\) the better is our model, because more of the variance can be explained by our model. \\(R^2\\) falls between 0-1, where 1 would imply that our model can explain 100% of the variance in our sample. \\(R^2\\) is also considered a goodness-of-fit measure. R2 (adj.) stands for adjusted R squared. The adjusted version of \\(R^2\\) becomes important if we have more than one preditor (i.e. independent variable) in our regression. The adjustment of \\(R^2\\) accounts for the number of independent variables in our model. Thus, it is possible for us to compare different models with each other, even though they might have different numbers of predictors. It is important to note that \\(R^2\\) will always increase if we add more predictors. RMSE stands for Root Mean Square Error and is an indicate how small or large the prediction error of the model is. Conceptually, it aims to measure the average deviations of values from our model when we attempt predictions. The lower the score the better, i.e. a score of 0 would imply that our model perfectly fits the data, which is likely never the case in the field of Social Sciences. The RMSE is particularly useful when trying to compare models. Sigma stands for the standard deviation of our residuals (the difference between predicted and empirically observed values) and is therefore a measure of prediction accuracy. Sigma is ‘a measure of the average distance each observation falls from its prediction from the model’ (Gelman, Hill, and Vehtari 2020, 168). Many of these indices will become more relevant when we compare models. However, \\(R^2\\) can also be meaningfully interpreted without a reference model. We know that the bigger \\(R^2\\) the better. In our case it is 0.548 which is very good considering that our model consist of only one predictor only. It is not easy to interpret whether an particular \\(R^2\\) value is good or bad. In our simple single linear regression, \\(R^2\\) is literally ‘r squared,’ which we already know from correlations and their effect sizes (see Table ??. Thus, if we take the square root of \\(R^2\\) we can retrieve the correlation coefficient, i.e. \\(r = \\sqrt{R^2} = \\sqrt{0.548} = 0.740\\). According to Cohen (1988), this would count as a large effect size. However, for multiple regressions, the situation is slightly more complicated, but the interpretation of \\(R^2\\) and its adjusted version remain largely the same. Once you have a model and it is fairly accurate, you can start making predictions. This can be achieved by using our model object m0 in combination with the function predict(). However, first we should define a set of values for our independent variable, i.e. cumulative_cases, which we store in a tibble using the tribble() function. df_predict &lt;- tribble( ~cumulative_cases, 100, 1000, 10000, 100000 ) predict(m0, newdata = df_predict) ## 1 2 3 4 ## 89.53289 102.45411 231.66629 1523.78810 As a result we find out how many likely deaths from COVID have to be expected based on our model for each value in our dataset. Single linear regressions are simple and a good way to introduce novice scholars to modeling social phenomena. However, hardly ever will find that a single variable can explain enough variance to be a useful model. Instead, we most likely can improve the majority of our single regression models by considering more variables in the form of multiple regressions. 12.2 Multiple regression Multiple regressions expand single linear regressions by allowing us to add more variables. Maybe surprisingly, it is fairly similar to compute these in R, because it requires the same function, i.e. lm(). In the last section we wanted to know how many people will likely not recover from COVID. However, it might be even more interesting to understand how we can reduce the number of cumulative cases and prevent casualties from the outset. Since I live in the UK at the time of the pandemic, I am curious to know whether certain COVID measures helped to reduce the number of new COVID cases. Of course, feel free to pick a different country (maybe the one you live in?) to follow with my example. First, we create a dataset that only contains information from the United Kingdom, which means we use filter() and remove observations with missing data. As a first step, we might want to know how the UK performed over the course of time in terms of new cases. Let’s plot this first. covid %&gt;% ggplot(aes(x = date_reported, y = new_cases)) + geom_col() One issue you might encounter when visualising large values in a plot like this are the axis labels. In our case, the values are very high on the y axis and ggplot abbreviates them, i.e. uses a scientific notation. We might not find this useful, especially when comparing it to the values in our dataset. Therefore, we should change it. The scales package is fantastic, because it can help us take care of it. Here is the same plot, but with better axis labels which everyone can easily understand by just adding labels = scales::comma. covid %&gt;% filter(date_reported == &quot;2021-08-26&quot; &amp; cumulative_cases &gt; 0) %&gt;% ggplot(aes(y = cumulative_cases)) + geom_boxplot() + scale_y_continuous(trans = &quot;log&quot;, labels = scales::comma) In short, it seems there is quite a range of countries with different numbers of COVID cases. However, we also notice that we have outliers which we need to deal with in a moment. If you have hypotheses you want to test, you would already know which variables to include in your regression. However, in our case, we do not really have a hypothesis based on our prior reading or other studies. Thus, we pick, for example three variables of interest that we suspect could help us with modelling COVID cases across countries. We can be fairly certain that the number of COVID cases should be lower in countries were more safety measures are in place (assuming they are effective, of course). The covid dataset includes such information over a very long period of time. Since we do not wish to work with time-series data in this section, we compute the average amount of measures taken expressed as the average index score for our variables of interest, e.g. masks, movements, and gatherings. m1 &lt;- lm(cumulative_cases ~ who_region + number_vaccines_types_used, data = covid) performance::model_performance(m1) ## # Indices of model performance ## ## AIC | BIC | R2 | R2 (adj.) | RMSE | Sigma ## ----------------------------------------------------------------- ## 4.180e+06 | 4.180e+06 | 0.025 | 0.025 | 1.859e+06 | 1.859e+06 covid %&gt;% filter(country == &quot;United Kingdom&quot;) %&gt;% select(where(is.numeric)) %&gt;% correlation::correlation() %&gt;% filter(r &gt;= 0.4) ## Registered S3 method overwritten by &#39;datawizard&#39;: ## method from ## plot.visualisation_recipe see ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## Warning in cor(x, y): the standard deviation is zero ## # Correlation Matrix (pearson-method) ## ## Parameter1 | Parameter2 | r | 95% CI | t | df | p ## ------------------------------------------------------------------------------------- ## new_cases | cumulative_cases | 0.46 | [0.39, 0.52] | 12.59 | 600 | &lt; .001*** ## new_cases | new_deaths | 0.45 | [0.39, 0.52] | 12.51 | 600 | &lt; .001*** ## new_cases | travel | 0.51 | [0.45, 0.57] | 14.56 | 592 | &lt; .001*** ## new_cases | schools | 0.52 | [0.45, 0.57] | 14.66 | 592 | &lt; .001*** ## new_cases | businesses | 0.51 | [0.44, 0.56] | 14.28 | 592 | &lt; .001*** ## new_cases | global_index | 0.65 | [0.61, 0.70] | 21.08 | 592 | &lt; .001*** ## cumulative_cases | cumulative_deaths | 0.96 | [0.96, 0.97] | 86.69 | 600 | &lt; .001*** ## cumulative_cases | global_index | 0.42 | [0.35, 0.49] | 11.30 | 592 | &lt; .001*** ## new_deaths | schools | 0.48 | [0.42, 0.54] | 13.44 | 592 | &lt; .001*** ## new_deaths | businesses | 0.48 | [0.41, 0.54] | 13.20 | 592 | &lt; .001*** ## new_deaths | movements | 0.54 | [0.48, 0.59] | 15.50 | 592 | &lt; .001*** ## new_deaths | global_index | 0.55 | [0.50, 0.61] | 16.17 | 592 | &lt; .001*** ## cumulative_deaths | global_index | 0.45 | [0.39, 0.52] | 12.42 | 592 | &lt; .001*** ## masks | businesses | 0.44 | [0.37, 0.50] | 11.99 | 592 | &lt; .001*** ## masks | global_index | 0.64 | [0.59, 0.68] | 20.06 | 592 | &lt; .001*** ## travel | global_index | 0.51 | [0.45, 0.57] | 14.50 | 592 | &lt; .001*** ## schools | businesses | 0.53 | [0.47, 0.59] | 15.23 | 592 | &lt; .001*** ## schools | movements | 0.52 | [0.46, 0.58] | 14.93 | 592 | &lt; .001*** ## schools | global_index | 0.72 | [0.68, 0.75] | 25.08 | 592 | &lt; .001*** ## businesses | movements | 0.42 | [0.35, 0.48] | 11.29 | 592 | &lt; .001*** ## businesses | global_index | 0.67 | [0.62, 0.71] | 21.98 | 592 | &lt; .001*** ## movements | global_index | 0.59 | [0.53, 0.64] | 17.57 | 592 | &lt; .001*** ## ## p-value adjustment method: Holm (1979) ## Observations: 594-602 covid %&gt;% ggplot(aes(x = gatherings, y = new_cases)) + geom_jitter() + scale_y_continuous(label = scales::comma) ## Warning: Removed 10806 rows containing missing values (geom_point). 12.3 Hierarchical regression References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

[["data-wrangling.html", "7 Data Wrangling 7.1 Import your data 7.2 Inspecting your data 7.3 Cleaning your column names: Call the janitor 7.4 Data types: What are they and how can you change them 7.5 Handling factors 7.6 Dealing with missing data 7.7 Latent constructs and their reliability", " 7 Data Wrangling ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.3 ✓ dplyr 1.0.7 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 2.0.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() You collected your data over months (and sometimes years) and all you really want to know is whether your data makes sense and reveals something nobody would have ever expected. However, before we can truly go ahead with our analysis, it is essential to understand whether our data is ‘tidy.’ Very often, data we receive is everything else but clean and we need to not only check whether our data is fit for analysis, but also ensure it is in a format that is easy to handle and work with. For small datasets, this is usually a brief exercise. However, I found myself cleaning data for a month, because the dataset was spread out into multiple spreadsheets (no pun intended) with different numbers of columns and odd column names. Thus, data cleaning or data wrangling is an essential first step in any data analysis. It is a step that cannot be skipped and has to be performed on every new dataset. Luckily, R provides many useful functions to make our lives easier. If you are like me and used to do this in Excel, you will be in for a treat. It is a lot simpler using R to do achieve a clean dataset. Here is an overview of the different steps we usually work through before we can start with our main analysis. This is list is certainly not exhaustive: Data import Checking data types Recoding and arranging factors, i.e. categorical data. Running missing data diagnostics and other things 7.1 Import your data The r4np package hosts a number of different datasets to work with, but at some point you might want to apply your R knowledge to your own data. Therefore, an important first step is to import your data into RStudio. There are three different methods all of which are very handy: Click on your data file in the Files pane and choose `Import Dataset’. Use the Import Dataset button in the Environment pane. Import your data calling one of the readr functions in the console or RScript We will use the readr package to import our data. Using this package we can import a range of different file formats, including .csv, .tsv, .txt. If you want to import data from an .xlsx file you have to use another package called readxl. The following sections will primarily focus on how to use reader via RStudio or directly in your Console or RScript. 7.1.1 Import data from the Files pane This approach is by far the easiest. Let’s assume you have a dataset called gender_age.csv in your 00_raw_data folder. If you wish to import it, you can do the following: Click on the name of the file Select Import Dataset. A new window will open and you can choose different options. You also see a little preview of how the data looks like. This is great if you are not sure whether you did it correctly. You can make changes to how the data should be imported, but in most cases the default should be fine. Here is quick breakdown of the most important options: Name allows you to change the object name, i.e. the name of the object this data will be assigned to. I tend to use df_raw (df stand for dataframe, which is how R calls such rectangular datasets). Skip is useful if your data file starts with a number of empty rows at the top. You can remove them here. First Row as Names is ticked by default. In most Social Science projects we tend to have the name of the variables as the first row in your dataset. Trim Spaces removes any unnecassry white-space in your dataset. Leave it ticked. Open Data Viewer allows you to look at your imported dataset. I use it rarely, but it can be helpful at times. Delimiter defines how your columns are separate from each other in your file. If it is a .csv it would imply it is a ‘comma-separated value,’ i.e. ,. This can be changed for different files, depending on how your data is delimited. You can even use the option Other… to specify a custom separation option. NA specifies how missing values in your data are acknowledged. By default, empty cells in your data will be recognised as missing data. Once you are happy with your choices, you can click on Import. You will find your dataset in the Environment pane. In the console you can see that R also provides the Column specification, which we need later when inspecting ‘data types.’ readr automatically imports all text-based columns as chr, i.e. character values. However, this might not be always true. More on this aspect of data wrangling in Chapter 7.4. 7.1.2 Importing data from the Environment pane The process of important datasets from the Environment pane follows largely the one from the Files pane. Click on Import Dataset &gt; From Text (readr)…. The only main difference lies in having to find the file using the Browse… button. The rest of the steps are the same as above. You will have to use the Environment pane for importing data from specific file types, e.g. .txt, because using the File pane would only open the file, but not import the data for further processing. 7.1.3 Importing data using functions directly If you organised your files well it can be very easy and quick to just use all the functions from readr directly. Here are two examples of how you can use readr to import your data. Make sure you have the tidyverse package loaded. # Import data from &#39;.csv&#39; read_csv(&quot;00_raw_data/gender_age.csv&quot;) # Import data from any file text file by defining the separator yourself read_delim(&quot;00_raw_data/gender_age.txt&quot;, delim = &quot;|&quot;) You might be wondering, whether you can use read_delim() to import .csv files too. The answer is ‘Yes, you can!’ In contrast to read_delim(), read_csv() sets the delimiter to , by default. This is mainly for convenience, because .csv files are one of the most popular file formats that people use for storing their data. You might be also wondering what actually a ‘delimiter’ is. When you record data in a plain-text file it is easy to see where a new observation starts and ends, because it is defined by a row in your file. However, we also need to tell our software where a new column starts, i.e. where a cell starts and ends. Consider the following example. We have a file that holds our data which looks like this: idagegender 124male 256female 333male The first row we probably can still decipher as id, age, gender. However, the next row makes it difficult to understand which value represents the id of a participant and which value reflects the age of that participant. Like us, computer software would find it hard too to make a decision regarding this ambiguous content. Thus, we need to use delimiters to make it very clear which value belongs to which column. In a .csv file the data would be separated by a ,. id,age,gender 1,24,male 2,56,female 3,33,male Considering our example from above, we could also use | as a delimiter. id|age|gender 1|24|male 2|56|female 3|33|male There is a lot more to readr than could be covered in this book. If you want to know more about this R package, I highly recommend to look at the readr webpage. 7.2 Inspecting your data For the rest of this chapter, we will use the wvs dataset from the r4np package. However, we do not know much about this dataset and therefore we cannot ask any research questions worth investigating. Therefore we need to look at what it contains. The first method of inspecting a dataset is to type the name of the object, i.e. wvs. # Ensure you loaded the &#39;r4np&#39; package first library(r4np) # Show the data in the console wvs ## # A tibble: 69,578 × 7 ## `Participant ID` `Country name` Gender Age relationship_status ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 20070001 Andorra 1 60 married ## 2 20070002 Andorra 0 47 living together as married ## 3 20070003 Andorra 0 48 separated ## 4 20070004 Andorra 1 62 living together as married ## 5 20070005 Andorra 0 49 living together as married ## 6 20070006 Andorra 1 51 married ## 7 20070007 Andorra 1 33 married ## 8 20070008 Andorra 0 55 widowed ## 9 20070009 Andorra 1 40 single ## 10 20070010 Andorra 1 38 living together as married ## # … with 69,568 more rows, and 2 more variables: Freedom.of.Choice &lt;dbl&gt;, ## # Satisfaction-with-life &lt;dbl&gt; The result is a series of rows and columns. The first information we receive is: A tibble: 69,578 x 9. This indicates that our dataset has 69,578 observations (i.e. rows) and 9 columns (i.e. variables). This rectangular format is the one we encounter most frequently in Social Sciences (and probably beyond). If you ever worked in Microsoft Excel, this format will look familiar. Even though it might be nice to look at a dataset in this way, it is not particularly useful. Depending on your monitor size you might only see a small number of columns and therefore we do not get to see a complete list of all variables. In short, we hardly ever will find much use in inspecting data this way. Luckily there are other functions that can help us. If you want to see each variable covered in the dataset and their data types, you can use the function glimpse() from the dplyr package (which is loaded when load the tidyverse package). glimpse(wvs) ## Rows: 69,578 ## Columns: 7 ## $ `Participant ID` &lt;dbl&gt; 20070001, 20070002, 20070003, 20070004, 20070… ## $ `Country name` &lt;chr&gt; &quot;Andorra&quot;, &quot;Andorra&quot;, &quot;Andorra&quot;, &quot;Andorra&quot;, &quot;… ## $ Gender &lt;dbl&gt; 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, … ## $ Age &lt;dbl&gt; 60, 47, 48, 62, 49, 51, 33, 55, 40, 38, 54, 3… ## $ relationship_status &lt;chr&gt; &quot;married&quot;, &quot;living together as married&quot;, &quot;sep… ## $ Freedom.of.Choice &lt;dbl&gt; 10, 9, 9, 9, 8, 10, 10, 8, 8, 10, 9, 8, 10, 7… ## $ `Satisfaction-with-life` &lt;dbl&gt; 10, 9, 9, 8, 7, 10, 5, 8, 8, 10, 8, 8, 10, 7,… The output of glimpse shows us the name of each column/variable after the $, for example `Participant ID`. The $ is used to look up certain variables in our dataset. If we want to inspect the column relationship_status only, we could write the following: wvs$relationship_status ## [1] &quot;married&quot; &quot;living together as married&quot; ## [3] &quot;separated&quot; &quot;living together as married&quot; ## [5] &quot;living together as married&quot; &quot;married&quot; ## [7] &quot;married&quot; &quot;widowed&quot; .... After the variable name we find the recognised datatype for each column in &lt;...&gt;, for example &lt;chr&gt;. We will return to data types in Chapter 7.4. Lastly, we get examples of the data that is included. This output is much more helpful. I use glimpse() very frequently for different purposes, for example: to understand what variables are included in a dataset, to check correctness of data types, to inspect variable names for typos or unconventional names, to look up variable names. There is one more way to inspect your data and receive a lot more information about it by using a specialised R package. The skimr package is excellent in ‘skimming’ your dataset. It provides not only information about variable names and data types, but also provides some descriptive statistics. If you installed the r4np package and called the function install_r4np(), you will have skimr installed already. skimr::skim(wvs) The output in the console should look like this: As you can tell, there is a lot more information in this output. Many descriptive statistics that could be useful are already displayed. skim() provides a summary of the dataset first and then sorts the variables automatically by data type. Depending on the data type you also receive different descriptive statistics. As an added bonus, the function also provides a histogram for numeric variables. However, there is one main problem: Some of the numeric variables are actually not numeric: Participant ID and Gender. Thus, we will have to correct the data types in a moment. Inspecting your data in this way can be helpful to get a better understanding of what your data includes and spot problems with it. If you receive data from someone else, these methods are a good way to familiarise yourself with the dataset relatively quickly. Since this particular dataset was prepared for this book, I also made sure I provide documentation for it. You can access it by using ?wvs in the Console. This will open the documentation in the Help pane. Such documentation can be found for every dataset we use in this book. 7.3 Cleaning your column names: Call the janitor If you have an eagle eye, you might have noticed that most of the variable names in wvs are not consistent or easy to read/use. # Whitespace and inconsistent capitalisation Participant ID Country name Gender Age # Difficult to read YearOfBirth Freedom.of.Choice Satisfaction-with-life From Chapter 5.5, you will remember that being consistent in the way your write your code and name your objects is essential. The same applies, of course, to variable names. R will not break using the existing names, but it will save you a lot of frustration if we take a minute to clean the names and make them more consistent. You are probably thinking: This is easy, I just open the dataset in Excel and change all the column names. Indeed, it would be a viable and easy option, but it is not very efficient, especially with larger datasets that have many more variables. Instead, we can make use of the janitor package. By definition, janitor is a package that helps to clean up whatever needs cleaning. In our case we want to tidy our column names. We can use the function clean_names() to achieve this. We store the result in a new object called wvs to keep those changes. The object will also show up in our Environment pane. wvs &lt;- janitor::clean_names(wvs) glimpse(wvs) ## Rows: 69,578 ## Columns: 7 ## $ participant_id &lt;dbl&gt; 20070001, 20070002, 20070003, 20070004, 2007000… ## $ country_name &lt;chr&gt; &quot;Andorra&quot;, &quot;Andorra&quot;, &quot;Andorra&quot;, &quot;Andorra&quot;, &quot;An… ## $ gender &lt;dbl&gt; 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,… ## $ age &lt;dbl&gt; 60, 47, 48, 62, 49, 51, 33, 55, 40, 38, 54, 39,… ## $ relationship_status &lt;chr&gt; &quot;married&quot;, &quot;living together as married&quot;, &quot;separ… ## $ freedom_of_choice &lt;dbl&gt; 10, 9, 9, 9, 8, 10, 10, 8, 8, 10, 9, 8, 10, 7, … ## $ satisfaction_with_life &lt;dbl&gt; 10, 9, 9, 8, 7, 10, 5, 8, 8, 10, 8, 8, 10, 7, 1… Now that janitor has done its magic, we suddenly have easy to read variable names that are consistent with the ‘Tidyverse style guide’ (Wickham 2021). If, for whatever reason, the variable names are still not looking the way you want, you can use the function rename() from the dplyr package to manually assign new variable names. wvs &lt;- wvs %&gt;% rename(satisfaction = satisfaction_with_life, country = country_name) glimpse(wvs) ## Rows: 69,578 ## Columns: 7 ## $ participant_id &lt;dbl&gt; 20070001, 20070002, 20070003, 20070004, 20070005, … ## $ country &lt;chr&gt; &quot;Andorra&quot;, &quot;Andorra&quot;, &quot;Andorra&quot;, &quot;Andorra&quot;, &quot;Andor… ## $ gender &lt;dbl&gt; 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,… ## $ age &lt;dbl&gt; 60, 47, 48, 62, 49, 51, 33, 55, 40, 38, 54, 39, 44… ## $ relationship_status &lt;chr&gt; &quot;married&quot;, &quot;living together as married&quot;, &quot;separate… ## $ freedom_of_choice &lt;dbl&gt; 10, 9, 9, 9, 8, 10, 10, 8, 8, 10, 9, 8, 10, 7, 10,… ## $ satisfaction &lt;dbl&gt; 10, 9, 9, 8, 7, 10, 5, 8, 8, 10, 8, 8, 10, 7, 10, … You are probably wondering what %&gt;% stands for. This symbol is called a ‘piping operator’ and it allows us to chain multiple functions together by considering the output of the previous function. So, do not confuse &lt;- with %&gt;%. Each operator serves a different purpose. The %&gt;% has become synonymous with the tidyverse approach to R programming and is the chosen approach for this book. Many functions from the tidyverse are designed to be chained together. If we wanted to spell out what we just did we could say: wvs &lt;-: We assigned whatever happened to the right of the assign operator to the object wvs. wvs %&gt;%: We defined the dataset we want to use with the functions defined after the %&gt;%. rename(satisfaction = satisfcation_with_life): We define a new name satisfaction for the column satisfaction_with_life. Notice that the order is new_name = old_name. Here we also use =. A rare occasion where it makes sense to do so. Just for clarification, the following two lines of code accomplish the same task. The only difference lies that with %&gt;% we could chain another function right after it. You could say, it is a matter of taste which approach you prefer. However, in later chapters it will become obvious why using %&gt;% is very advantageous. # Renaming a column using &#39;%&gt;%&#39; wvs %&gt;% rename(satisfaction_new = satisfaction) # Renaming a column without &#39;%&gt;%&#39; rename(wvs, satisfaction_new = satisfaction) Since you will be using the pipe operator very frequently, it is a good idea to remember the keyboard shortcut for it: Ctrl+Shift+M for PC and Cmd+Shift+M for Mac. 7.4 Data types: What are they and how can you change them When we inspected our data, I mentioned that some of the variables do not have the correct data type. You might be familiar with different data types by classifying them as: Nominal data, which is categorical data of no particular order, Ordinal data, which is categorical data with a defined order, and Quantitative data, which is data that usually is represented by numeric values. In R we have a slightly different distinction: character / &lt;chr&gt;: Textual data, for example the text of a tweet. factor / &lt;fct&gt;: Categorical data with a finite number of categories with no particular order. ordered / &lt;ord&gt;: Categorical data with a finite number of categories with a particular order. double / &lt;dbl&gt;: Numerical data with decimal places. integer / &lt;int&gt;: Numerical data with whole numbers only (i.e. no decimals). logical / &lt;lgl&gt;: Logical data, which only consists of values ‘TRUE’ and ‘FALSE.’ date / date: Data which consists dates, e.g. ‘2021-08-05.’ date-time / dttm: Data which consists dates and times, e.g. ‘2021-08-05 16:29:25 BST.’ For a complete list of data types I recommend to take a look at ‘Column Data Types’ (Müller and Wickham 2021). It is obvious that R has a more fine-grained categorisation of data types. The most important distinction, though, lies between &lt;chr&gt;, &lt;fct&gt;/&lt;ord&gt; and &lt;dbl&gt; for most datasets in the Social Sciences. Still, it is good to be aware of what the abbreviations in your tibble mean and how they might affect your analysis. Now that we have a solid understanding of different data types, we can have a look at our dataset and see whether readr classified our variables correctly. glimpse(wvs) ## Rows: 69,578 ## Columns: 7 ## $ participant_id &lt;dbl&gt; 20070001, 20070002, 20070003, 20070004, 20070005, … ## $ country &lt;chr&gt; &quot;Andorra&quot;, &quot;Andorra&quot;, &quot;Andorra&quot;, &quot;Andorra&quot;, &quot;Andor… ## $ gender &lt;dbl&gt; 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,… ## $ age &lt;dbl&gt; 60, 47, 48, 62, 49, 51, 33, 55, 40, 38, 54, 39, 44… ## $ relationship_status &lt;chr&gt; &quot;married&quot;, &quot;living together as married&quot;, &quot;separate… ## $ freedom_of_choice &lt;dbl&gt; 10, 9, 9, 9, 8, 10, 10, 8, 8, 10, 9, 8, 10, 7, 10,… ## $ satisfaction &lt;dbl&gt; 10, 9, 9, 8, 7, 10, 5, 8, 8, 10, 8, 8, 10, 7, 10, … readr did a great job in identifying all the numeric variables. However, by default, readr imports all variables that include text as &lt;chr&gt;. It appears, in our dataset this is not quite correct. The variables countr_name, gender and relationship_status specify a finite number of categories. Therefore they should be classified as factor. The variable participant_id is represented by numbers, but its meaning is also rather categorical in nature. We would not use the ID numbers of participants to perform additions or multiplications. This would simply make no sense. Therefore, it might be wise to turn them into a factor as well, even though we likely will not use it in our analysis and would make no difference. However, I am a stickler for those kind of things, so I would include in it. In order to perform the conversion we need to use two new functions from dplyr: mutate(): Changes, i.e. ‘mutates,’ a variable. as_factor(): Converts data from one type into a factor. If we want to convert all variables in one go, we can put them into the same function, separated by a ,. wvs &lt;- wvs %&gt;% mutate(country = as_factor(country), gender = as_factor(gender), relationship_status = as_factor(relationship_status), participant_id = as_factor(participant_id) ) glimpse(wvs) ## Rows: 69,578 ## Columns: 7 ## $ participant_id &lt;fct&gt; 20070001, 20070002, 20070003, 20070004, 20070005, … ## $ country &lt;fct&gt; &quot;Andorra&quot;, &quot;Andorra&quot;, &quot;Andorra&quot;, &quot;Andorra&quot;, &quot;Andor… ## $ gender &lt;fct&gt; 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,… ## $ age &lt;dbl&gt; 60, 47, 48, 62, 49, 51, 33, 55, 40, 38, 54, 39, 44… ## $ relationship_status &lt;fct&gt; married, living together as married, separated, li… ## $ freedom_of_choice &lt;dbl&gt; 10, 9, 9, 9, 8, 10, 10, 8, 8, 10, 9, 8, 10, 7, 10,… ## $ satisfaction &lt;dbl&gt; 10, 9, 9, 8, 7, 10, 5, 8, 8, 10, 8, 8, 10, 7, 10, … The output in the console shows that we successfully performed the transformation and our data types are as we intended them to be. Mission accomplished. 7.5 Handling factors 7.5.1 Recoding factors Another common problem we have to tackle when working with data is their representation in the dataset. For example, gender could be measured as male and female1 or as 0 and 1. R does not mind which way you represent your data, but some other software does. Therefore, when we import data from somewhere else the values of a variable might not look the way we want. The practicality of having your data represented accurately as what they are becomes obvious when you intend to create tables and plots. For example, we might be interested to know how many participants in the wvs were male and how many were female. The function count() from dplyr does exactly that. wvs %&gt;% count(gender) ## # A tibble: 3 × 2 ## gender n ## &lt;fct&gt; &lt;int&gt; ## 1 0 33049 ## 2 1 36478 ## 3 &lt;NA&gt; 51 Now we know how many people were male and female and how many did not disclose their gender. Or do we? The issue here is that you would have to know what the 0 and 1 stand for. Surely you would have a coding manual that gives you the answer, but it seems a bit of a complication. For gender, this might still be easy to remember, but can you recall the numbers for 48 countries? It certainly would be easier to replace the 0s and 1s with their corresponding labels. This can be achieved with a simple function called fct_recode() from forcats. Since we ‘mutate’ a variable into something else, we have to use the mutate() function too. wvs &lt;- wvs %&gt;% mutate(gender = fct_recode(gender, &quot;male&quot; = &quot;0&quot;, &quot;female&quot; = &quot;1&quot;)) If you have been following along very carefully you might spot one oddity in this code: \"0\" and \"1\". You likely recall that in Chapter 5 I mentioned that we use \"\" for character values but not for numbers. What happens if we run the code and remove \"\". wvs %&gt;% mutate(gender = fct_recode(gender, &quot;male&quot; = 0, &quot;female&quot; = 1)) ## Error: Problem with `mutate()` column `gender`. ## ℹ `gender = fct_recode(gender, male = 0, female = 1)`. ## x Each input to fct_recode must be a single named string. Problems at positions: 1, 2 The error message is very easy to understand: fct_recode() only expects strings as input and not numbers. R recognises 0 and 1 as numbers, but fct_recode() converts a factor value into another factor value. To refer to a factor level (i.e. one of the categories in our factor), we have to use \"\". In other words, data types matter and are often a source of problems with your code. Thus, always pay close attention to it. If we rerun our analysis from before and generate a frequency table for gender, we now get a much more readable output. wvs %&gt;% count(gender) ## # A tibble: 3 × 2 ## gender n ## &lt;fct&gt; &lt;int&gt; ## 1 male 33049 ## 2 female 36478 ## 3 &lt;NA&gt; 51 Another benefit of going through the trouble of recoding your factors is the readability of your plots. We could easily generate a bar plot based on the above table and have appropriate labels, instead of 0 and 1. wvs %&gt;% count(gender) %&gt;% ggplot(aes(gender, n)) + geom_col() Plots are an excellent way to explore your data and understand relationships between variables. More about this when we start to perform analytical steps on our data (see Chapter 8 and beyond). Another use case for recoding factors could be for purely cosmetic reasons. When looking through our dataset, we might notice that some country names are very long and would not look great in data visualisations or tables. Thus, we could consider shortening them. First we need to find out which country names are particularly long. There are 48 countries in this dataset, so it could take some time to look through them all. Instead we could use the functionfilter() from dplyr to pick only countries with a long name. However, this poses another problem: How can we tell the filter function to pick only country names with a certain length? Ideally, we would want a function that does the counting for us. As you probably anticipated, there is a package, called stringr, which also belongs to the tidyverse, and has a function which counts the number of characters that represent a value in our dataset: str_length(). This function takes any character variable and returns the length of it. This also works with factors, because this function can ‘coerce’ it into a character, i.e. it just ignores that it is a factor and looks at is as if it was a regular character variable. Good news for us, because now we can put the puzzle pieces together. wvs %&gt;% filter(stringr::str_length(country) &gt;= 15) %&gt;% count(country) ## # A tibble: 3 × 2 ## country n ## &lt;fct&gt; &lt;int&gt; ## 1 Bolivia, Plurinational State of 2067 ## 2 Iran, Islamic Republic of 1499 ## 3 Korea, Republic of 1245 I use the value 15 arbitrarily after some trial and error. You can change the value and see which other countries would show up with a lower threshold. However, this number seems to do the trick and returns three countries that seem to have longer labels. All we have to do is to replace these categories with new ones the same way we recoded gender. You probably can guess already what we have to do to achieve this. wvs &lt;- wvs %&gt;% mutate(country = fct_recode(country, &quot;Bolivia&quot; = &quot;Bolivia, Plurinational State of&quot;, &quot;Iran&quot; = &quot;Iran, Islamic Republic of&quot;, &quot;Korea&quot; = &quot;Korea, Republic of&quot;)) 7.5.2 Reordering factor levels TODO: CONTINUE FROM HERE Consider whether this should happen here or later. Probably later, actually when we talk about descriptive statistics. This is not really data cleaning at this point. Too much stuff already. Move to descriptive statistics section. 7.6 Dealing with missing data There is hardly any Social Sciences project where researchers do not have to deal with data that is missing. Participants are sometimes not willing to complete a questionnaire or miss a second round of data collection entirely, e.g in longitudinal studies. It is not the purpose of this chapter to delve into all aspects of analysing missing data, but provide a solid starting point. There are mainly three steps involved in dealing with missing data: Mapping missing data Identifying patterns of missing data Replacing or removing missing data 7.6.1 Mapping missing data Every study that intends to be rigorous will have to first identify how much data is missing. In R this can be achieved in multiple ways, but using a specialised package like naniar does help us to do this very quickly and systematically. First we have to load the naniar package and then we use the function vis_miss() to visualise how much and where exactly data is missing. library(naniar) vis_miss(wvs) Figure 7.1: Mapping missing data with naniar naniar plays along nicely with the tidyverse approach of programming. As such, it would also be possible to write wvs %&gt;% vis_miss(). As we can see, 99,7% of our dataset is complete and we are only missing 0.3%. The dark lines (actually blocks) refer to missing data points. On the x-axis we can see all our variables and on the y-axis we see our observations. This is the same layout as our rectangular dataset: Rows are observations and columns are variables. Overall, this dataset appears relatively complete (luckily). In addition, we can see the percentage of missing data per variable. freedom_of_choice is clearly the variable with most missing data, i.e. 0.79%. Still, the amount of missing data is not very large. When working with larger datasets it might also be useful to rank variables by their degree of missing data to see where the biggest problems lie. gg_miss_var(wvs) Figure 7.2: Missing data per variable It is very noticeable that freedom_of_choice has the most missing data points, while participant_id, and country_name have no missing values. If you prefer to see the actual numbers instead, we can use as series of functions that start with miss_ (for a complete list of all functions see the reference page of naniar). In order to retrieve the numeric values which are reflected in the plot above, we can write the following: # Summarise the missingness in each variable miss_var_summary(wvs) ## # A tibble: 7 × 3 ## variable n_miss pct_miss ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 freedom_of_choice 547 0.786 ## 2 relationship_status 335 0.481 ## 3 age 318 0.457 ## 4 satisfaction 239 0.343 ## 5 gender 51 0.0733 ## 6 participant_id 0 0 ## 7 country 0 0 For mapping missing data, especially in larger datasets with many variables, I tend to prefer data visualisations over numerical results. This also has the benefit that patterns of missing data can be more easily identified as well. 7.6.2 Identifying patterns of missing data If you find that your data ‘suffers’ from missing data, it is essential to answer another question: Is data missing systematically? This is quite an important diagnostic step since systematically missing data would imply that if we remove these observations from our dataset as is, we likely produce wrong results. We can distinguish missing data based on how it is missing, i.e. missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). (Rubin 1976) 7.6.2.1 Missing completely at random (MCAR) Missing completely at random (MCAR) means that neither observed nor missing data can systematically explain why data is missing. It is pure coincidence how data is missing and there is no underlying pattern. The naniar package comes with the very popular Little’s MCAR test (Little 1988), which provides insights into whether our data is missing completely at random. Thus, we can call the function mcar_test() and inspect the result. wvs %&gt;% select(-participant_id) %&gt;% # Remove variables which do not reflect a response mcar_test() ## # A tibble: 1 × 4 ## statistic df p.value missing.patterns ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 411. 67 0 19 When you run such a test, you have to make sure that variables which are not part of the data collection are removed. In our case, the participant_id is generated by the researcher and does not represent an actual response by the participants. As such, we need to remove it using select() before we can run the test. A - inverts the meaning of select(). While select(participant_id) would do what it says, i.e. include it as the only variable in the test, select(-participant_id) results in selecting everything but this variable in our test. You will find, it is sometimes easier to remove a variable with select() rather than listing all the variables you want to keep. Since the p.value of the test is so small that it got rounded down to 0, i.e. \\(p&lt;0.0001\\), we have to assume that our data is not missing completely at random. If we found that \\(p&gt;0.05\\), we would have confirmation that data is missing completely at random. 7.6.2.2 Missing at random (MAR) Missing at random (MAR) refers to a situation where missing data can only be explained by the observed data, but not the missing data. Dong and Peng (2013) (p. 2) provide a good example when this is the case: Let’s suppose that students who scored low on the pre-test are more likely to drop out of the course, hence, their scores on the post-test are missing. If we assume that the probability of missing the post-test depends only on scores on the pre-test, then the missing mechanism on the post-test is MAR. In other words, for students who have the same pre-test score, the probability of [them] missing the post-test is random. Thus, a main difference between MCAR and MAR data lies in the fact that we can observe some patterns of missing data if data is MAR. These patterns are only based on data we actually have, i.e. observed data. We also assume that no unobserved variables can explain these or other patterns. Accordingly, we first look into variables that have no missing data and see whether they can explain our missing data in other variables. For example, we could investigate whether missing data in freedom_of_choice is attributed to specific countries. wvs %&gt;% group_by(country) %&gt;% filter(is.na(freedom_of_choice)) %&gt;% count() %&gt;% arrange(desc(n)) # Rearranging scores for easier reading ## # A tibble: 32 × 2 ## # Groups: country [32] ## country n ## &lt;fct&gt; &lt;int&gt; ## 1 Japan 47 ## 2 Brazil 44 ## 3 New Zealand 44 ## 4 Russia 43 ## 5 Bolivia 40 ## 6 Romania 29 ## 7 Kazakhstan 27 ## 8 Turkey 24 ## 9 Egypt 23 ## 10 Serbia 20 ## # … with 22 more rows As becomes clear, there are four countries that have particularly high numbers of missing data for freedom_of_choice: Japan, Brazil, New Zealand, Russia and Bolivia. Why this is the case lies beyond this dataset and is something only the researchers themselves could possibly explain. Collecting data in different countries is particularly challenging and one is quickly faced with different unfavourable conditions. Tthe missing data is not completely random, as it seems, because we have some first evidence that the location of data collection might have affected the completeness of it. Another way of understanding patterns of missing data can be achieved by looking at whether there are relationships between missing values, for example the co-occurrence of missing values across different variables. This can be achieved by using upset plots. An upset plot consists of three parts: Set size, intersection size and a Venn diagram which defines the intersections. gg_miss_upset(wvs) In our dataset, the most frequent combination of missing data occurs when only freedom_of_choice is missing (the first column), but nothing else. Similar results can be found for relationships_status and age. The first combination of missing data is defined by two variables: satisfaction and freedom_of_choice. In total 107 participants had satisfaction and freedom_of_choice missing but nothing else. The ‘set size’ shown in the upset plot refers to the number of missing values for each variable in the diagram. This correspondents to what we have found when looking at Figure 7.2). Our analysis also suggests that values are not completely randomly missing but that we have data to help explain why they are missing. 7.6.2.3 Missing not at random (MNAR) Lastly, missing not at random (MNAR) implies that data is missing systematically and that other variables or reasons exist that explain why data is missing, but they are not fully known to us. In questionnaire-based research an easily overlooked reason that can explain missing data is the ‘page-drop-off’ phenomenon. In such cases, participants stop completing a questionnaire once they advance to another page. Figure 7.3 shows this very clearly for a large scale project where an online questionnaire was used. After almost every page break in the questionnaire, some participants decided to discontinue. Finding these types of patterns is difficult when only working with numeric values. Thus, it is always advisable to visualise your data as well. Such missing data is linked to the design of the data collection tool. Figure 7.3: MNAR pattern in a dataset due to ‘page-drop-offs’ Defining whether a dataset is MNAR or not is mainly achieved by ruling out MCAR and MAR assumptions. It is not possible to test whether missing data is MNAR, unless we have more information about the underlying population available (Ginkel et al. 2020). We have sufficient evidence that our data is MAR as was shown above, because we managed to identify some relationships between unobserved and observed data. In practice, it is very rare to find datasets that are truly MCAR (Buuren 2018). Therefore we might consider ‘imputation’ as a possible strategy to solve our missing data problem. More about imputation in the next Chapter. If you are looking for more inspiration of how you could visualise and identify patterns of missingness in your data, you might find the ‘Gallery’ of the naniar website particularly useful. 7.6.3 Replacing or removing missing data Once you determined which pattern of missing data applies to your dataset, it is time to evaluate the options of how we want to deal with those missing values. Generally, you can either keep the missing values as they are, replace them or remove them entirely. Jakobsen et al. (2017) provide a rule of thumb of 5% where researchers can consider missing data as negligible, i.e. we can ignore the missing values, because they won’t affect our analysis in a significant way. They also argue that if the proportion of missing data exceeds 40%, we should also only work with our observed data. However, with such a large amount of missing data, it is very questionable whether we can rely on our analysis as much as we want to. If the missing data lies somewhere in-between this range, we need to consider the missing data pattern at hand. If data is MCAR, we could remove missing data. This process is called ‘listwise deletion,’ i.e. you remove all data from a participant with missing data. As just mentioned, removing missing values is only suitable if you have relatively few missing values in your dataset (see also Schafer (1999)) as is the case with the wvs dataset. There are additional problems with deleting observations listwise, many of which are summarised by Buuren (2018) in his introduction. Usually, we try to avoid removing data as much as possible. If you wanted to perform listwise deletion it can be done with a single function call: na.omit() from the built-in stats package. Here is an example of how we can apply this function. # No more missing data in this plot wvs %&gt;% na.omit() %&gt;% vis_miss() # How much observations are left after we removed all missing data? wvs %&gt;% na.omit() %&gt;% count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 68312 If you wanted to remove the missing data without the plot you can just use wvs_no_na &lt;- na.omit(wvs). I always recommend to ‘save’ the result in a new object to ensure I keep my original data available. This can be helpful when trying to compare how this decision affects my analysis, i.e. I can run the analysis with and without missing data removed. However, for the wvs dataset this does not seem to be the best option. Based on our analysis of the wvs dataset (by no means complete!), we could assume that data is MAR. In such cases, it is possible to ‘impute’ the missing values, i.e. we replace the missing data with computed scores. This is possible because we can model the missing data based on variables we have. For obvious reasons, we cannot model missing data based on variables we have not measured in our study. You might be thinking: Why would it make data better if we ‘make up’ numbers? Is this not cheating? Imputation of missing values is a science in itself. There is plenty to read about the process of handling missing data which would reach far beyond the scope of this book. However, the seminal work of Buuren (2018), Dong and Peng (2013) and Jakobsen et al. (2017) are excellent starting points. In short: Simply removing missing data can lead to biases in your data, e.g. if we removed missing data in our dataset, we would mainly exclude participants from Japan, Brazil, New Zealand, Russia and Bolivia (as we found out earlier). While imputation is not perfect, scholars have shown that it can produce more reliable results than not using imputation at all (add some references here), assuming the data meets the requirements for such imputation. Even though our dataset has only a very small amount of missing data (relative to the entire size), we can still make use of imputation. There are many different ways of how one can approach this task, one of which is ‘multiple imputation.’ As highlighted by Ginkel et al. (2020), multiple imputation has not yet reached the same popularity as listwise deletion, despite its benefits. A main reason for this lies in the complexity of using this technique. Therefore I included an example of how to use multiple imputation separately in Chapter @ref(). There are many more approaches to imputation and going through them all in detail would be impossible and distract from the main purpose of the book. Still, I would like to share some interesting packages with you that use different imputation methods: mice (Multivariate Imputation via Chained Equations) Amelia (Uses a bootstrapped-based algorithm) missForest (Uses a random forest algorithm) Hmisc (Uses Additive Regression, Bootstrapping, and Predictive Mean Matching) mi (Uses posterior predictive distribution, predictive mean matching, mean-imputation, median-imputation, or conditional mean-imputation) Besides multiple imputation, there is also the option for single imputation. The simputation package offers a great variety of different imputation functions, one of which also fits our data quite well impute_knn(). This function makes use of a clustering technique called ‘K-nearest neighbour.’ In this case, the function will look for observations that are closest to the one that has missing data and takes the value of that observation. In other words, it looks for similar participants who answered the questionnaire in a very similar way. The great convenience of this approach is that it can handle all kinds of data types at the same time, which is not true for all imputation techniques. If we apply this function to our own dataset, we have to write the following: wvs_nona &lt;- wvs %&gt;% select(-participant_id) %&gt;% as.data.frame() %&gt;% # Transforms our tibble into a dataframe simputation::impute_knn(. ~ .) The function impute_knn(. ~ .) might look like a combination of text with an emoji (. ~ .). This imputation function requires us to specify a model to impute the data. Since we want to impute all missing values in the dataset and use all variables available, we put . on both sides of the equation, which is separated by a ~. The . reflects that we do not specify a specific variable, but instead tell the function to use all variables that are not mentioned. In our case, we did not mention any variables at all and therefore it chooses all of them. If we wanted to impute only freedom_of_choice we would have to put impute_knn(freedom_of_choice ~ .). We will elaborate more on how to specify models when we cover regression models (see Chapter 11). As you will have noticed, we also had to convert our tibble into a dataframe using as.data.frame(). As mentioned in Chapter 5.3, some functions require a certain data type or format. The simputation package works with dplyr, but it prefers dataframes. Based on my experiences, the wrong data type or format is one of the most frequent causes of errors that novice R programmers report. So, keep an eye on it and read the documentation carefully. Be aware that imputation of any kind can take a long time. On my MacBook Pro it took about 4.22 seconds to complete impute_knn() with the wvs dataset. If we used multiple imputation, this would have taken considerably longer, i.e. several minutes and more. We now should have a dataset that is free of any missing values. To make sure this is the case we can create the missing data matrix that we created at the very beginning of this chapter (see Figure 7.1). vis_miss(wvs_nona) 7.6.4 Main takeaways regarding dealing with missing data Handling missing data is hardly ever a simple process. Do not feel discouraged if you get lost in the myriad of options. While there is some guidance on how and when to use certain strategies to deal with missing values in your dataset, the most important point to remember is: Be transparent of what you did. As long as you can explain why you did something and how you did it, everyone can follow your thought process and help improve your analysis. However, ignoring the fact that data is missing and not acknowledging it is more than just unwise. 7.7 Latent constructs and their reliability A common challenge Social Scientists face is that we want to measure something that cannot be measured directly. For example, ‘happiness’ is a feeling that does not naturally occur as a number which we can observe and measure. The opposite is true for ‘temperature,’ which is naturally measured in numbers. At the same time, ‘happiness’ is much more complex of a variable than ‘temperature.’ In order to account for this we often work with ‘latent variables.’ These are defined as variables which are not directly measured, but are inferred from other variables. In practice, we often use multiple questions in a questionnaire to measure one latent variable, usually by computing the mean of those questions. The gep dataset from the r4np package includes data about students’ social integration experience (si) and communication skills development (cs). Data was obtained using the Global Education Profiler (GEP). Each of these latent variables is measured by 6 different questions in the questionnaire. glimpse(gep) ## Rows: 300 ## Columns: 15 ## $ age &lt;dbl&gt; 22, 26, 24, 24, 20, 21, 22, 20, 20, 2… ## $ gender &lt;chr&gt; &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;,… ## $ level_of_study &lt;chr&gt; &quot;UG&quot;, &quot;PGT&quot;, &quot;PGT&quot;, &quot;PGT&quot;, &quot;UG&quot;, &quot;UG&quot;… ## $ si_socialise_with_people_exp &lt;dbl&gt; 5, 1, 2, 5, 3, 2, 1, 4, 3, 1, 3, 4, 4… ## $ si_supportive_friends_exp &lt;dbl&gt; 5, 1, 5, 5, 4, 2, 1, 4, 3, 1, 4, 4, 4… ## $ si_joined_activitiy_group_exp &lt;dbl&gt; 3, 1, 2, 4, 2, 1, 1, 1, 3, 4, 1, 5, 2… ## $ si_time_socialising_exp &lt;dbl&gt; 4, 2, 3, 4, 4, 3, 1, 3, 2, 2, 3, 4, 4… ## $ si_accommodation_meet_people_exp &lt;dbl&gt; 3, 1, 3, 4, 5, 1, 1, 3, 4, 1, 1, 5, 2… ## $ si_take_part_events_exp &lt;dbl&gt; 3, 3, 3, 3, 1, 1, 1, 1, 1, 2, 3, 3, 3… ## $ cs_learn_different_styles_exp &lt;dbl&gt; 4, 2, 6, 6, 3, 6, 1, 3, 4, 3, 3, 2, 5… ## $ cs_speak_different_context_exp &lt;dbl&gt; 5, 4, 5, 6, 4, 4, 1, 2, 3, 6, 4, 3, 3… ## $ cs_improve_less_fluent_exp &lt;dbl&gt; 5, 3, 6, 6, 2, 1, 2, 5, 3, 6, 4, 4, 4… ## $ cs_awareness_own_style_exp &lt;dbl&gt; 5, 3, 4, 6, 4, 3, 1, 3, 3, 6, 4, 2, 3… ## $ cs_work_globally_exp &lt;dbl&gt; 5, 4, 6, 6, 4, 5, 1, 5, 5, 4, 4, 3, 5… ## $ cs_find_clarification_exp &lt;dbl&gt; 5, 3, 6, 6, 2, 4, 1, 4, 3, 6, 4, 4, 5… For example, if we wanted to know how each student scored with regards to social integration (si), we have to compute the mean (mean()) of all related items (i.e. all variables starting with si_), for each row (rowwise()) because each row presents one participant. The same is true for communication skills (cs). We can compute both variables in one go: # Compute the scores for the latent variable &#39;si&#39; and &#39;cs&#39; gep &lt;- gep %&gt;% rowwise() %&gt;% mutate(si = mean(c(si_socialise_with_people_exp, si_supportive_friends_exp, si_joined_activitiy_group_exp, si_time_socialising_exp, si_accommodation_meet_people_exp, si_take_part_events_exp ) ), cs = mean(c(cs_learn_different_styles_exp, cs_speak_different_context_exp, cs_improve_less_fluent_exp, cs_awareness_own_style_exp, cs_work_globally_exp, cs_find_clarification_exp ) ) ) glimpse(gep) ## Rows: 300 ## Columns: 17 ## Rowwise: ## $ age &lt;dbl&gt; 22, 26, 24, 24, 20, 21, 22, 20, 20, 2… ## $ gender &lt;chr&gt; &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;,… ## $ level_of_study &lt;chr&gt; &quot;UG&quot;, &quot;PGT&quot;, &quot;PGT&quot;, &quot;PGT&quot;, &quot;UG&quot;, &quot;UG&quot;… ## $ si_socialise_with_people_exp &lt;dbl&gt; 5, 1, 2, 5, 3, 2, 1, 4, 3, 1, 3, 4, 4… ## $ si_supportive_friends_exp &lt;dbl&gt; 5, 1, 5, 5, 4, 2, 1, 4, 3, 1, 4, 4, 4… ## $ si_joined_activitiy_group_exp &lt;dbl&gt; 3, 1, 2, 4, 2, 1, 1, 1, 3, 4, 1, 5, 2… ## $ si_time_socialising_exp &lt;dbl&gt; 4, 2, 3, 4, 4, 3, 1, 3, 2, 2, 3, 4, 4… ## $ si_accommodation_meet_people_exp &lt;dbl&gt; 3, 1, 3, 4, 5, 1, 1, 3, 4, 1, 1, 5, 2… ## $ si_take_part_events_exp &lt;dbl&gt; 3, 3, 3, 3, 1, 1, 1, 1, 1, 2, 3, 3, 3… ## $ cs_learn_different_styles_exp &lt;dbl&gt; 4, 2, 6, 6, 3, 6, 1, 3, 4, 3, 3, 2, 5… ## $ cs_speak_different_context_exp &lt;dbl&gt; 5, 4, 5, 6, 4, 4, 1, 2, 3, 6, 4, 3, 3… ## $ cs_improve_less_fluent_exp &lt;dbl&gt; 5, 3, 6, 6, 2, 1, 2, 5, 3, 6, 4, 4, 4… ## $ cs_awareness_own_style_exp &lt;dbl&gt; 5, 3, 4, 6, 4, 3, 1, 3, 3, 6, 4, 2, 3… ## $ cs_work_globally_exp &lt;dbl&gt; 5, 4, 6, 6, 4, 5, 1, 5, 5, 4, 4, 3, 5… ## $ cs_find_clarification_exp &lt;dbl&gt; 5, 3, 6, 6, 2, 4, 1, 4, 3, 6, 4, 4, 5… ## $ si &lt;dbl&gt; 3.833333, 1.500000, 3.000000, 4.16666… ## $ cs &lt;dbl&gt; 4.833333, 3.166667, 5.500000, 6.00000… Compared to dealing with missing data, this is a fairly straightforward task. However, there is a caveat. Before we are allowed to compute the mean across all these variables, we need to know and understand whether all these scores reliably contribute to one single latent variable. If not, we would be in trouble and make a significant mistake. By far the most common approach to assessing reliability (or more accurately ‘internal consistency’) of latent variables is Cronbach’s \\(\\alpha\\). This indicator looks at how strongly a set of items (i.e. questions in your questionnaire) are related to each other. Thus, the stronger the relationship of all items starting with si_ to each other, the more likely we achieve a higher Cronbach’s \\(\\alpha\\). The psych package has a suitable function to compute it for us. Instead of listing all the items by hand, I use the function starts_with() to only pick the variables whose names start with si_. It certainly pays off to think about your variable names more thoroughly in advance to benefit from such shortcuts (see also Chapter 7.3. gep %&gt;% select(starts_with(&quot;si_&quot;)) %&gt;% psych::alpha() ## ## Reliability analysis ## Call: psych::alpha(x = .) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.87 0.88 0.87 0.54 7.2 0.011 3.1 1.2 0.53 ## ## lower alpha upper 95% confidence boundaries ## 0.85 0.87 0.9 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N ## si_socialise_with_people_exp 0.85 0.85 0.84 0.53 5.7 ## si_supportive_friends_exp 0.85 0.85 0.84 0.54 5.9 ## si_joined_activitiy_group_exp 0.85 0.86 0.84 0.55 6.1 ## si_time_socialising_exp 0.83 0.84 0.82 0.51 5.1 ## si_accommodation_meet_people_exp 0.88 0.88 0.87 0.60 7.5 ## si_take_part_events_exp 0.85 0.85 0.84 0.54 5.9 ## alpha se var.r med.r ## si_socialise_with_people_exp 0.014 0.0120 0.52 ## si_supportive_friends_exp 0.014 0.0096 0.53 ## si_joined_activitiy_group_exp 0.014 0.0131 0.53 ## si_time_socialising_exp 0.015 0.0083 0.52 ## si_accommodation_meet_people_exp 0.011 0.0055 0.62 ## si_take_part_events_exp 0.014 0.0128 0.52 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## si_socialise_with_people_exp 300 0.80 0.81 0.77 0.71 3.6 1.4 ## si_supportive_friends_exp 300 0.79 0.80 0.75 0.69 3.4 1.5 ## si_joined_activitiy_group_exp 300 0.78 0.78 0.72 0.67 2.8 1.6 ## si_time_socialising_exp 300 0.87 0.87 0.86 0.80 3.2 1.5 ## si_accommodation_meet_people_exp 300 0.68 0.67 0.56 0.52 3.0 1.7 ## si_take_part_events_exp 300 0.80 0.80 0.75 0.70 2.6 1.5 ## ## Non missing response frequency for each item ## 1 2 3 4 5 6 miss ## si_socialise_with_people_exp 0.07 0.15 0.25 0.25 0.19 0.09 0 ## si_supportive_friends_exp 0.15 0.15 0.18 0.25 0.18 0.09 0 ## si_joined_activitiy_group_exp 0.27 0.23 0.19 0.12 0.13 0.07 0 ## si_time_socialising_exp 0.14 0.22 0.26 0.17 0.13 0.08 0 ## si_accommodation_meet_people_exp 0.26 0.17 0.20 0.15 0.12 0.10 0 ## si_take_part_events_exp 0.30 0.22 0.21 0.15 0.06 0.05 0 The function alpha() returns a lot of information. The most important part, though, is shown at the very beginning: ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.87 0.88 0.87 0.54 7.18 0.01 3.11 1.19 0.53 In most publications, researchers would primarily report the raw_alpha value. This is fine, but it is not a bad idea to also include at least std.alpha and G6(smc). In terms of interpretation, Cronbrach’s \\(\\alpha\\) scores can range from 0 to 1. The closer the score to 1 the higher we would judge its reliability. Nunally (1967a) originally provided the following classification for Cronbach’s \\(\\alpha\\): between 0.6 and 0.5 can be sufficient during early stages of development, 0.8 or higher is sufficient for most basic research, 0.9 or higher is suitable for applied research, where the questionnaires is used to make critical decisions, e.g. clinical studies, university admission tests, etc., with a ‘desired standard’ of 0.95. However, a few years later Nunally (1967b) revisited his original categorisation and also considered 0.7 or higher as a suitable benchmark in case of more exploratory-type research. Needless to say, this gave grounds for researchers to pick and choose the ‘right’ publication for them (Henson 2001). Consquently, depending on your research field, the expected reliability score might lean more towards 0.7 or 0.8. Still, the higher the score, the better it is. Our dataset shows that si scores a solid \\(\\alpha = 0.87\\), which is excellent. We should repeat this step for cs as well. gep %&gt;% select(starts_with(&quot;cs_&quot;)) %&gt;% psych::alpha() ## ## Reliability analysis ## Call: psych::alpha(x = .) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.9 0.9 0.88 0.59 8.6 0.0092 4 1.1 0.62 ## ## lower alpha upper 95% confidence boundaries ## 0.88 0.9 0.91 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N ## cs_learn_different_styles_exp 0.88 0.88 0.86 0.59 7.2 ## cs_speak_different_context_exp 0.87 0.87 0.85 0.57 6.7 ## cs_improve_less_fluent_exp 0.88 0.88 0.86 0.60 7.4 ## cs_awareness_own_style_exp 0.87 0.87 0.85 0.58 6.9 ## cs_work_globally_exp 0.87 0.87 0.85 0.57 6.7 ## cs_find_clarification_exp 0.89 0.89 0.88 0.63 8.4 ## alpha se var.r med.r ## cs_learn_different_styles_exp 0.0109 0.0041 0.60 ## cs_speak_different_context_exp 0.0117 0.0042 0.60 ## cs_improve_less_fluent_exp 0.0106 0.0057 0.62 ## cs_awareness_own_style_exp 0.0115 0.0055 0.57 ## cs_work_globally_exp 0.0117 0.0064 0.55 ## cs_find_clarification_exp 0.0098 0.0025 0.63 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## cs_learn_different_styles_exp 300 0.81 0.81 0.76 0.72 3.9 1.4 ## cs_speak_different_context_exp 300 0.85 0.85 0.82 0.77 4.0 1.4 ## cs_improve_less_fluent_exp 300 0.80 0.79 0.74 0.70 3.8 1.5 ## cs_awareness_own_style_exp 300 0.84 0.84 0.80 0.75 3.9 1.4 ## cs_work_globally_exp 300 0.85 0.85 0.81 0.77 3.9 1.5 ## cs_find_clarification_exp 300 0.72 0.74 0.65 0.62 4.3 1.2 ## ## Non missing response frequency for each item ## 1 2 3 4 5 6 miss ## cs_learn_different_styles_exp 0.07 0.09 0.24 0.26 0.21 0.14 0 ## cs_speak_different_context_exp 0.05 0.10 0.20 0.27 0.20 0.17 0 ## cs_improve_less_fluent_exp 0.07 0.15 0.20 0.23 0.20 0.15 0 ## cs_awareness_own_style_exp 0.07 0.09 0.18 0.29 0.21 0.15 0 ## cs_work_globally_exp 0.07 0.12 0.20 0.24 0.21 0.16 0 ## cs_find_clarification_exp 0.02 0.06 0.16 0.31 0.24 0.21 0 Similarly, to si, cs shows a very good internal consistency score of \\(\\alpha = 0.90\\). Thus, we can be confident that if we used our latent variables we would could trust that our results are likely reliable. While Cronbach’s \\(\\alpha\\) is very popular, due to its simplicity, there is plenty of criticism as well (add references). Therefore, very often it is not enough to just report the Cronbach’s \\(\\alpha\\), but undertake additional steps. Depending on the stage of development of your measurement instrument (i.e. your questionnaire) you likely have to perform one of the following prior to computing the \\(\\alpha\\) scores: Exploratory factor analysis (EFA): Generally used to identify latent variables in a set of questionnaire items. Confirmatory factor analysis (CFA): To confirm whether a set of items truly reflect a latent variable. An example of exploratory factor analysis is presented in Chapter @ref(). Since the gep data is based on an established measurement tool we will have to perform a CFA. To perform a CFA we use the popular lavaan (Latent Variable Analysis) package. The steps of running a CFA in R include: Define which variables are supposed to measure a specific latent variable (i.e. creating a model) Run the CFA to see whether our assumptions are true Interpret the results based on various indicators. library(lavaan) ## This is lavaan 0.6-9 ## lavaan is FREE software! Please report any bugs. # The model which explains how items of the questionnaire relate to a latent variable model &lt;- &#39;social_integration =~ si_socialise_with_people_exp + si_supportive_friends_exp + si_joined_activitiy_group_exp + si_time_socialising_exp + si_accommodation_meet_people_exp + si_take_part_events_exp communication_skills =~ cs_learn_different_styles_exp + cs_speak_different_context_exp + cs_improve_less_fluent_exp + cs_awareness_own_style_exp + cs_work_globally_exp + cs_find_clarification_exp&#39; # We need to perform the CFA to see how well this model fits out data fit &lt;- cfa(model, data = gep) # Extract the performance indicators fit_indices &lt;- fitmeasures(fit) # We can use the &#39;broom&#39; package to make the results easier to read and pick only those indices we are most interested in broom::tidy(fit_indices) %&gt;% filter(names == &quot;gfi&quot; | names == &quot;agfi&quot; | names == &quot;rmsea&quot;) %&gt;% mutate(x = round(x, 3)) # Round the results to 3 decimal places ## # A tibble: 3 × 2 ## names x ## &lt;chr&gt; &lt;lvn.vctr&gt; ## 1 rmsea 0.063 ## 2 gfi 0.937 ## 3 agfi 0.908 References "]]

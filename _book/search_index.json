[["correlations.html", "9 Correlations 9.1 Plotting correlations 9.2 Significance: A way to help you judge your findings 9.3 Limitations of correlations 9.4 Correlations can be spurious 9.5 Simpson’s Paradox: When correlations betray you", " 9 Correlations library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.3 ✓ dplyr 1.0.7 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 2.0.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(r4np) library(correlation) ## Registered S3 methods overwritten by &#39;parameters&#39;: ## method from ## as.double.parameters_kurtosis datawizard ## as.double.parameters_skewness datawizard ## as.double.parameters_smoothness datawizard ## as.numeric.parameters_kurtosis datawizard ## as.numeric.parameters_skewness datawizard ## as.numeric.parameters_smoothness datawizard ## print.parameters_distribution datawizard ## print.parameters_kurtosis datawizard ## print.parameters_skewness datawizard ## summary.parameters_kurtosis datawizard ## summary.parameters_skewness datawizard library(patchwork) Sometimes counting and measuring means, medians and standard deviations is not enough, because they are all based on a single variable. Instead, we might have questions related to the relationship of two or more variables. In this section we will explore how correlations can (and kind of cannot - see Chapter 9.5) provide insights into the following questions: Do movie watchers agree with critics regarding the rating of movies? Do popular movies receive more votes from users than less popular movies? Do movies with more votes also make more money? There are many different ways of how one can compute the correlation and it partially depends on the type of data you want to relate. The ‘Pearson’s correlation’ is by far the most frequently used correlation technique for data that is normally distributed. On the other hand, if data is not normally distributed, we can opt for the ‘Spearman’s rank’ correlation. One could argue that the relationship between these two correlations is like the mean (Pearson) to the median (Spearman). Both approaches require numeric values to be computed properly. If our data is ordinal, or worse dichotomous (like a logical variables), we have to opt for different options. Table 9.1: Different ways of computing correlations Correlation Used Pearson Requires variables to be parametric and therefore numeric Spearman Used when data is non-parametric and requires numeric variables Polychoric Used when investigating two ordinal variables Tetrachoric Use when both variables are dichotomous, e.g. ‘yes/no,’ ‘True/False.’ There are many more variations of correlations, which you can explore on the website of the package we will use in this chapter: correlation. We will primarily focus on Pearson and Spearman as the most commonly used correlation types in academic publications to understand the relationship between two variables. In addition, we also look at ‘partial correlations,’ which allow us to introduce a third variable into this mix. 9.1 Plotting correlations Since correlations only show the relationship between two variables, we can easily put one variable onto the x axis and one variable onto the y axis creating a so-called ‘scatterplot.’ We used the functions to create scatterplots before, i.e. geom_point() and geom_jitter. Let’s try to answer our first research question, i.e. whether regular movie watchers and critics (people who review movies as a profession) rate the top 250 in the same way. One assumption could be that it does not matter whether you are a regular movie watcher or someone who does it professionally. After all, we are just human beings. A counter thesis could be that critics have a different perspective on movies and might use different evaluation criteria. Either way, we first need to identify the two variables of interest: imdb_rating is based on IMDb users metascore is based on movie critics imdb_top_250 %&gt;% filter(!is.na(metascore)) %&gt;% ggplot(aes(imdb_rating, metascore)) + geom_jitter() + see::theme_modern() The results from our scatterplot are, well, somewhat random. We can see that some movies receive high imdb_ratings as well as high metascores. However, there are also some movies that receive high imdb_ratings, but low metascores. Overall, the points look like they are randomly scattered all over our canvas. The only visible pattern we can notice is that there are more movies at the lower end of the rating system relative to all the movies in the top 250. In fact, ther are only 2 movies which actually received an IMDb rating of over 9. Be aware, geom_jitter() makes it look like there were more than 9!). Since correlations only explain linear relationships, a perfect correlation would be represented by a straight line. Consider the following examples of correlations: A correlation can be either positive or negative and its score (i.e. r in case of Pearson) can range from -1 to 1: -1 defines a perfectly negative correlation, 0 defines no correlation (completely random), and 1 defines a perfectly positive correlation. In other words, the further the score is away from zero, the stronger is the relationship between variables. We also have benchmarks which we can use to asses the strength of a relationship, for example the one by Cohen (1988). The strength of the relationship is also called ‘effect size.’ Table 9.2 shows the relevant benchmarks. Note that effect sizes are always provided as absolute figures. Therefore, -0.4 would also count as a moderate relationship. Table 9.2: Assessing effect size of relationships according to Cohen (1988) effect size interpretation r &lt; 0.1 very small 0.1 \\(\\leq\\) r &lt; 0.3 small 0.3 \\(\\leq\\) r &lt; 0.5 moderate r \\(\\geq\\) 0.5 large If we compare the plot from our data with the sample plots, we would come to the conclusion that the relationship is weak, and therefore the r must be close to zero as well. We can test this with the Pearson correlation. imdb_top_250 %&gt;% select(imdb_rating, metascore) %&gt;% correlation() ## # Correlation Matrix (pearson-method) ## ## Parameter1 | Parameter2 | r | 95% CI | t(214) | p ## ---------------------------------------------------------------- ## imdb_rating | metascore | 0.08 | [-0.06, 0.21] | 1.11 | 0.270 ## ## p-value adjustment method: Holm (1979) ## Observations: 216 Indeed, our analysis reveals that the effect size is very small (r = 0.08 &lt; 0.1). Therefore, critics appear to rate movies differently than regular movie watchers. This triggers an interesting follow-up question: Which movie is the most controversial, i.e. where is the difference between imdb_rating and metascore the highest? We can answer this question with the tools we already know. We create a new variable to subtract the metascore from the imdb_rating and plot it. We have to make sure both scales are the same length. The variable imdb_rating ranges from 0-10, but the metascore ranges from 0-100. As such, I used the percentile score instead by dividing the scores by 10 and 100 respectively. Since plotting 250 movies would have been too much and would also not help us find the answer to our question, I chose arbitrary values to pick only those movies with the highest differences in scores. Feel free to adjust the filter to your liking to see more or less movies. plot &lt;- imdb_top_250 %&gt;% mutate(r_diff = imdb_rating/10 - metascore/100) %&gt;% filter(!is.na(r_diff) &amp; r_diff &gt;= 0.25 | r_diff &lt;= -0.165) plot %&gt;% ggplot(aes(x = reorder(title, r_diff), y = r_diff, label = title)) + geom_col(aes(fill = ifelse(r_diff &gt; 0, &quot;Viewers&quot;, &quot;Critics&quot;))) + geom_text(aes(x = title, y = 0, label = title), size = 2.5, # to adjust the labels of the plot vjust = ifelse(plot$r_diff &gt;= 0, 0.5, 0.5), hjust = ifelse(plot$r_diff &gt;= 0, 1.05, -0.05) ) + coord_flip() + # Cleaning up the plot to make it look more readable and colourful scale_fill_manual(values = c(&quot;#FFCE7D&quot;, &quot;#7DC0FF&quot;)) + theme(axis.title = element_blank(), plot.background = element_blank(), panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), legend.title = element_blank() ) Another question we posed at the beginning was: Do popular movies receive more votes from users than less popular movies? Our intuition might say ‘yes.’ More popular movies are likely seen by more people, which makes them more popular. Consequently, the more people have seen a movie, the more likely they might vote for this movie. For less popular movies, the oppposite should be true. Let’s create another scatterplot. imdb_top_250 %&gt;% ggplot(aes(x = imdb_rating, y = votes)) + geom_jitter() The scatterplot shows some positive trend. Often, it can be touch to see the trend clearly. In order to impove our plot we can make use of the function geom_smooth(), which can help us draw a straight line that fits our data points the best. We need to set the method for drawing the line to lm, which stands for linear model. Remember, correlations assume a linear relationship between two variables. imdb_top_250 %&gt;% ggplot(aes(x = imdb_rating, y = votes)) + geom_jitter() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, colour = &quot;red&quot;) Another problem we face with this plot (and correlations) are extreme values, i.e. outliers. We already know that outliers tend to cause trouble for our analysis (see Chapter 8.3.3 and in correlations they can affect the strength of relationships. If we compute the the Pearson correlation with and without some of the outliers, the differences are very significant. Be aware, since we work with two variables at the same time, we should consider outliers on both. There our filter() will have to include two conditions. # The correlation with outliers imdb_top_250 %&gt;% select(imdb_rating, votes) %&gt;% correlation() ## # Correlation Matrix (pearson-method) ## ## Parameter1 | Parameter2 | r | 95% CI | t(248) | p ## ------------------------------------------------------------------- ## imdb_rating | votes | 0.59 | [0.51, 0.67] | 11.57 | &lt; .001*** ## ## p-value adjustment method: Holm (1979) ## Observations: 250 # Define outliers votes_out &lt;- 1.5*IQR(imdb_top_250$votes) + median(imdb_top_250$votes) imdb_r_out &lt;- 1.5*IQR(imdb_top_250$imdb_rating) + median(imdb_top_250$imdb_rating) # The correlation with outliers (based on 1.5 * IQR) imdb_top_250 %&gt;% filter(votes &lt; votes_out &amp; imdb_rating &lt; imdb_r_out) %&gt;% select(imdb_rating, votes) %&gt;% correlation() ## # Correlation Matrix (pearson-method) ## ## Parameter1 | Parameter2 | r | 95% CI | t(230) | p ## ------------------------------------------------------------------- ## imdb_rating | votes | 0.31 | [0.19, 0.42] | 4.94 | &lt; .001*** ## ## p-value adjustment method: Holm (1979) ## Observations: 232 While both correlations are highly significant (p &lt; 0.01), the drop in r from 0.59 to 0.31 is substantial. When we plot the data again, we can see that the dots are fairly randomly distributed across the plotting area. Thanks to geom_smooth we get an idea of a slight positive relationship between these two variables. imdb_top_250 %&gt;% filter(votes &lt; votes_out &amp; imdb_rating &lt; imdb_r_out) %&gt;% ggplot(aes(x = imdb_rating, y = votes)) + geom_jitter() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, colour = &quot;red&quot;) In conclusion, while there is some relationship between the rating and the number of votes, it is by far not as strong as we might have thought, especially after removing outliers, which had a considerable effect on the effect size. 9.2 Significance: A way to help you judge your findings One of the most common pitfalls of novice statisticans is related to the interpretation of what counts as significant and not significant. Most basic tests offer a ‘p value,’ which stands for ‘probability value.’ The p value can range from 1 (for 100%) to 0 (for 0%) and implies: p = 1, there is a 100% chance that the result is a pure coincidence p = 0, there is a 0% chance that the results is a pure coincidence, i.e. we can be certain this is not just luck. Technically, we would not find that p is ever truly zero, and instead denote very small p values with p &lt; 0.01 or even p&lt; 0.001. There are also commonly considered thresholds for the p value: \\(p &gt; 0.05\\), the result is not significant. There is chance of 5% that our finding is a pure coincidence. p \\(\\leq 0.05\\) , the result is significant. p \\(\\leq 0.01\\), the result is highly significant. We will cover more about the p value in Chapter 10 and Chapter 11. For now, it is important to know that a significant correlation is one that we should look at more closely. Usually, correlations that nor significant suffer from low effect sizes. However, different samples can lead to different effect sizes and different significant levels. Consider the following examples: ## # Correlation Matrix (pearson-method) ## ## Parameter1 | Parameter2 | r | 95% CI | t(2) | p ## ------------------------------------------------------------- ## x | y | 0.77 | [-0.73, 0.99] | 1.73 | 0.225 ## ## p-value adjustment method: Holm (1979) ## Observations: 4 ## # Correlation Matrix (pearson-method) ## ## Parameter1 | Parameter2 | r | 95% CI | t(10) | p ## --------------------------------------------------------------- ## x | y | 0.77 | [0.36, 0.93] | 3.87 | 0.003** ## ## p-value adjustment method: Holm (1979) ## Observations: 12 In both examples \\(r = 0.77\\), but the sample sizes are different (4 vs 12) and the p values differ as well. In the first example \\(p = 0.225\\), which means the relationship is not significant, while in the second example, we find that \\(p &lt; 0.01\\) and is therefore highly significant. As a general rule, we find that the bigger the sample, the more likely we find significant results, even though the effect size is small. Therefore, it is important to interpret correlations base on at least three factors: the p value, i.e. significance level the r value, i.e. the effect size, and the sample size. The interplay of all three can help determine whether a relationship is truly important. Therefore, when we include correlation tables in publications, we have to make sure we provide information about all three. It is common that we do not only compute correlations for two variables at a time, instead, we can do this for multiple variables simultaneously. imdb_top_250 %&gt;% select(imdb_rating, metascore, year, votes, gross_in_m) %&gt;% correlation() ## # Correlation Matrix (pearson-method) ## ## Parameter1 | Parameter2 | r | 95% CI | t | df | p ## --------------------------------------------------------------------------- ## imdb_rating | metascore | 0.08 | [-0.06, 0.21] | 1.11 | 214 | 0.539 ## imdb_rating | year | 0.03 | [-0.10, 0.15] | 0.42 | 248 | 0.678 ## imdb_rating | votes | 0.59 | [ 0.51, 0.67] | 11.57 | 248 | &lt; .001*** ## imdb_rating | gross_in_m | 0.21 | [ 0.07, 0.33] | 3.07 | 213 | 0.010** ## metascore | year | -0.41 | [-0.52, -0.30] | -6.63 | 214 | &lt; .001*** ## metascore | votes | -0.25 | [-0.37, -0.12] | -3.76 | 214 | 0.001** ## metascore | gross_in_m | -0.13 | [-0.27, 0.01] | -1.83 | 193 | 0.207 ## year | votes | 0.37 | [ 0.26, 0.47] | 6.29 | 248 | &lt; .001*** ## year | gross_in_m | 0.36 | [ 0.23, 0.47] | 5.58 | 213 | &lt; .001*** ## votes | gross_in_m | 0.56 | [ 0.46, 0.64] | 9.79 | 213 | &lt; .001*** ## ## p-value adjustment method: Holm (1979) ## Observations: 195-250 If you have seen correlation tables before, you might find that correlation() does not produce the classic table by default. If you want it to look like the tables in publications, which are more compact but offers less information, you can use the function summary(). imdb_top_250 %&gt;% select(imdb_rating, metascore, year, votes, gross_in_m) %&gt;% correlation() %&gt;% summary() ## # Correlation Matrix (pearson-method) ## ## Parameter | gross_in_m | votes | year | metascore ## --------------------------------------------------------- ## imdb_rating | 0.21** | 0.59*** | 0.03 | 0.08 ## metascore | -0.13 | -0.25** | -0.41*** | ## year | 0.36*** | 0.37*** | | ## votes | 0.56*** | | | ## ## p-value adjustment method: Holm (1979) This table also provides an answer to our final question, i.e. do movies with more votes earn more money. It appears as if this is true, because \\(r = 0.56\\) and \\(p &lt; 0.001\\). In the classic correlation table you also see *. These stand for the difference significant levels: *, i.e. \\(p &lt; 0.05\\) **, i.e. \\(p &lt; 0.01\\) ***, i.e. \\(p &lt; 0.001\\) (although you might find some do not use this as a separate level) In short, the more * there are attached to each value, the more significant a result. In other words, relationships with many * likely repeat if we collect data again. 9.3 Limitations of correlations Correlations are useful, but only to some extend. The three most common limitations you should be aware of are: Correlations are not causal relationships Correlations can be spurious Correlations might only appear in sub-samples of your data 9.3.1 Correlations are not causal relationships Correlations do not offer insights into causality, i.e. whether change in one variable causes change in the other variable. Correlations only provide insights into whether these two variables tend to change when one of them changes. Still, sometimes we can infer such causality by the nature of the variables. For example, in countries with heavy rain, more umbrellas are sold. It is apparent that buying more umbrellas will not cause more rain, but if there is more rain in a country, we rightly assume that there is a higher demand for umbrellas. If we can theorise the relationship between variables we would rather opt for a regression model instead of a correlation (see Chapter 11. 9.4 Correlations can be spurious Just because we find a relationship between two variables does not necessarily mean that are truly related to each other. Instead, it might be possible that a third variable is the reason for the relationship. We call relationships between variables that are cause by a third variable ‘spurious correlations.’ This third variable can either be part of our dataset or even something we have not measured at all. The latter case would make it impossible to investigate the relationship further. However, we can always test whether some of our variables affect the relationship between the two variables of interest. This can be done by using partial correlations. A partial correlation returns the relationship between two variables minus the relationship two a third variable. Figure 9.1: Illustration of a spurious correlation 9.5 Simpson’s Paradox: When correlations betray you References "]]

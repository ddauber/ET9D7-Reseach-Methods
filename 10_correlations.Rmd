# Correlations {#correlations}

Sometimes counting and measuring means, medians and standard deviations is not enough, because they are all based on a single variable. Instead, we might have questions related to the relationship of two or more variables. In this section we will explore how correlations can (and kind of cannot - see Chapter \@ref(simpsons-paradox)) provide insights into the following questions:

-   Do movie viewers agree with critics regarding the rating of movies?

-   Do popular movies receive more votes from users than less popular movies?

-   Do movies with more votes also make more money?

There are many different ways of how one can compute the correlation and it partially depends on the type of data you want to relate. The 'Pearson's correlation' is by far the most frequently used correlation technique for data that is normally distributed. On the other hand, if data is not normally distributed, we can opt for the 'Spearman's rank' correlation. One could argue that the relationship between these two correlations is like the mean (Pearson) to the median (Spearman). Both approaches require numeric values to be computed properly. If our data is ordinal, or worse dichotomous (like a logical variables), we have to opt for different options.

+-----------------+-------------------------------------------------------------------------+
| Correlation     | Used                                                                    |
+=================+=========================================================================+
| *Pearson*       | Requires variables to be parametric and therefore numeric               |
+-----------------+-------------------------------------------------------------------------+
| *Spearman*      | Used when data is non-parametric and requires numeric variables         |
+-----------------+-------------------------------------------------------------------------+
| *Polychoric*    | Used when investigating two ordinal variables                           |
+-----------------+-------------------------------------------------------------------------+
| *Tetrachoric*   | Used when both variables are dichotomous, e.g. 'yes/no', 'True/False'.  |
+-----------------+-------------------------------------------------------------------------+
| *Rank-biserial* | Used when one variable is dichotomous and the other variable is ordinal |
+-----------------+-------------------------------------------------------------------------+

: (\#tab:different-correlations) Different ways of computing correlations

There are many more variations of correlations, which you can explore on the [website of the package](https://easystats.github.io/correlation/articles/types.html "website of the package"){target="blank"} we will use in this chapter: `correlation`. We will primarily focus on Pearson, Spearman and the Chi-Squared test, because they are the most commonly used correlation types in academic publications to understand the relationship between two variables. In addition, we also look at 'partial correlations', which allow us to introduce a third variable into this mix.

## Parametric or non-parametric: That is the question {#parametric-or-non-parametric}

`TODO: ADD SECTION`

## Plotting correlations {#plotting-correlations}

Since correlations only show the relationship between two variables, we can easily put one variable onto the x axis and one variable onto the y axis creating a so-called 'scatterplot'. We used the functions to create scatterplots before, i.e. `geom_point()` and `geom_jitter`. Let's try to answer our first research question, i.e. whether regular movie viewers and critics (people who review movies as a profession) rate the top 250 in the same way. One assumption could be that it does not matter whether you are a regular movie viewer or someone who does it professionally. After all, we are just human beings. A counter thesis could be that critics have a different perspective on movies and might use different evaluation criteria. Either way, we first need to identify the two variables of interest:

-   `imdb_rating` is based on IMDb users

-   `metascore` is based on movie critics

```{r Correlation between imdb_rating and metascore, echo=TRUE}
imdb_top_250 %>%
  filter(!is.na(metascore)) %>% 
  ggplot(aes(imdb_rating, metascore)) +
  geom_jitter() +
  see::theme_modern()
```

The results from our scatterplot are, well, somewhat random. We can see that some movies receive high `imdb_rating`s as well as high `metascore`s. However, there are also some movies that receive high `imdb_rating`s, but low `metascore`s. Overall, the points look like they are randomly scattered all over our canvas. The only visible pattern we can notice is that there are more movies at the lower end of the rating system relative to all the movies in the top 250. In fact, ther are only 2 movies which actually received an IMDb rating of over 9. Be aware, `geom_jitter()` makes it look like there were more than 9!).

Since correlations only explain linear relationships, a perfect correlation would be represented by a straight line. Consider the following examples of correlations:

```{r Examples of correlations, echo=FALSE}
set.seed(1234)

# Positive correlation of 0.89
cm <- tribble(
    ~a,    ~b,
     1,   0.9, 
   0.9,     1)

means <- c(2, 4)
data <- as_tibble(MASS::mvrnorm(n = 100, mu = means, Sigma = cm)) %>% 
  rename("x" = `1`,
         "y" = `2`)

p1 <- data %>% ggplot(aes(x, y)) + geom_point() + ggtitle("r = 0.89") + 
  see::theme_modern()

# Positive correlation of 0.30
cm <- tribble(
    ~a,   ~b,
     1,  0.3,
   0.3,    1
)

means <- c(10, 4)
data <- as_tibble(MASS::mvrnorm(n = 100, mu = means, Sigma = cm)) %>% 
  rename("x" = `1`,
         "y" = `2`)

p2 <- data %>% ggplot(aes(x, y)) + geom_point() + ggtitle("r = 0.30") +
    see::theme_modern()

# Negative correlation of -0.75
cm <- tribble(
    ~a,   ~b,
     1,  -0.8,
   -0.8,    1
)

means <- c(10, 4)
data <- as_tibble(MASS::mvrnorm(n = 100, mu = means, Sigma = cm)) %>% 
  rename("x" = `1`,
         "y" = `2`)

p3 <- data %>% ggplot(aes(x, y)) + geom_point() + ggtitle("r = -0.75") +
    see::theme_modern()

# Negative correlation of -0.42
cm <- tribble(
    ~a,   ~b,
     1,  -0.5,
   -0.5,    1
)

means <- c(10, 4)
data <- as_tibble(MASS::mvrnorm(n = 100, mu = means, Sigma = cm)) %>% 
  rename("x" = `1`,
         "y" = `2`)

p4 <- data %>% ggplot(aes(x, y)) + geom_point()  + ggtitle("r = -0.42") +
  see::theme_modern()


# Negative correlation of -1
cm <- tribble(
    ~a,   ~b,
     1,  -1,
   -1,    1
)

means <- c(10, 4)
data <- as_tibble(MASS::mvrnorm(n = 100, mu = means, Sigma = cm)) %>% 
  rename("x" = `1`,
         "y" = `2`)

p5 <- data %>% ggplot(aes(x, y)) + geom_point()  + ggtitle("r = -1") +
  see::theme_modern()

# Negative correlation of 1
cm <- tribble(
    ~a,   ~b,
     1,  1,
     1,  1
)

means <- c(10, 4)
data <- as_tibble(MASS::mvrnorm(n = 100, mu = means, Sigma = cm)) %>% 
  rename("x" = `1`,
         "y" = `2`)

p6 <- data %>% ggplot(aes(x, y)) + geom_point()  + ggtitle("r = 1") +
  see::theme_modern()


p6 + p1 + p2 + p4 + p3 + p5
```

A correlation can be either positive or negative and its score (i.e. `r` in case of Pearson) can range from -1 to 1:

-   -1 defines a perfectly negative correlation,

-   0 defines no correlation (completely random), and

-   1 defines a perfectly positive correlation.

In other words, the further the score is away from zero, the stronger is the relationship between variables. We also have benchmarks which we can use to asses the strength of a relationship, for example the one by @cohen1988statistical. The strength of the relationship is also called 'effect size'. Table \@ref(tab:effect-size-cohen) shows the relevant benchmarks. Note that effect sizes are always provided as absolute figures. Therefore, -0.4 would also count as a moderate relationship.

| effect size         | interpretation |
|---------------------|----------------|
| r \< 0.1            | very small     |
| 0.1 $\leq$ r \< 0.3 | small          |
| 0.3 $\leq$ r \< 0.5 | moderate       |
| r $\geq$ 0.5        | large          |

: (\#tab:effect-size-cohen) Assessing effect size of relationships according to Cohen (1988)

If we compare the plot from our data with the sample plots, we would come to the conclusion that the relationship is weak, and therefore the `r` must be close to zero as well. We can test this with the Pearson correlation.

```{r Pearson correlation for ratings, echo=TRUE}
imdb_top_250 %>% 
  select(imdb_rating, metascore) %>% 
  correlation()
```

Indeed, our analysis reveals that the effect size is very small (r = 0.08 \< 0.1). Therefore, critics appear to rate movies differently than regular movie viewers. This triggers an interesting follow-up question: Which movie is the most controversial, i.e. where is the difference between `imdb_rating` and `metascore` the highest?

We can answer this question with the tools we already know. We create a new variable to subtract the `metascore` from the `imdb_rating` and plot it. We have to make sure both scales are the same length. The variable `imdb_rating` ranges from 0-10, but the `metascore` ranges from 0-100. As such, I used the percentile score instead by dividing the scores by 10 and 100 respectively. Since plotting 250 movies would have been too much and would also not help us find the answer to our question, I chose arbitrary values to pick only those movies with the highest differences in scores. Feel free to adjust the filter to your liking to see more or less movies.

```{r Biggest discrepancy in movie ratings, echo=TRUE}
plot <- imdb_top_250 %>%
  mutate(r_diff = imdb_rating/10 - metascore/100) %>% 
  filter(!is.na(r_diff) & r_diff >= 0.25 | r_diff <= -0.165)

plot %>%
  ggplot(aes(x = reorder(title, r_diff), y = r_diff, label = title)) + 
  geom_col(aes(fill = ifelse(r_diff > 0, "Viewers", "Critics"))) + 
  
  geom_text(aes(x = title, y = 0,
            label = title),
            size = 2.5,
            
            # to adjust the labels of the plot
            vjust = ifelse(plot$r_diff >= 0, 0.5, 0.5),
            hjust = ifelse(plot$r_diff >= 0, 1.05, -0.05)
            ) +
  coord_flip() +
  
  # Cleaning up the plot to make it look more readable and colourful
  scale_fill_manual(values = c("#FFCE7D", "#7DC0FF")) +
  theme(axis.title = element_blank(),
        plot.background = element_blank(),
        panel.background = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        legend.title = element_blank()
        )
```

Another question we posed at the beginning was: Do popular movies receive more votes from users than less popular movies? Our intuition might say 'yes'. More popular movies are likely seen by more people, which makes them more popular. Consequently, the more people have seen a movie, the more likely they might vote for this movie. For less popular movies, the opposite should be true. Let's create another scatterplot to find out.

```{r Ratings vs votes, echo=TRUE}
imdb_top_250 %>%
  ggplot(aes(x = imdb_rating, y = votes)) + 
  geom_jitter() +
  scale_y_continuous(labels = scales::label_number(big.mark = ","))
```

The last line of code helps to make the labels on the y axis more readable. The numbers are in the millions and it helps to have the indicators present using `label_number()` from the `scales` package.

The scatterplot shows a somewhat positive trend. Often, it can be tough to see the trend clearly. In order to improve our plot we can make use of the function `geom_smooth()`, which can help us draw a straight line that fits our data points the best. We need to set the `method` for drawing the line to `lm`, which stands for linear model. Remember, correlations assume a linear relationship between two variables.

```{r Ratings vs votes plus regression line, echo=TRUE}
imdb_top_250 %>%
  ggplot(aes(x = imdb_rating, y = votes)) + 
  geom_jitter() +
  geom_smooth(method = "lm",
              formula = y ~ x,
              se = FALSE,
              colour = "red")
```

Another problem we face with this plot (and correlations) are extreme values, i.e. outliers. We already know that outliers tend to cause trouble for our analysis (see Chapter \@ref(dealing-with-outliers) and in correlations they can affect the strength of relationships. If we compute the the Pearson correlation with and without some of the outliers, the differences are very significant. Be aware, since we work with two variables at the same time, we should consider outliers on both. There our `filter()` will have to include two conditions.

```{r Outliers in regressions, echo=TRUE}
# The correlation with outliers
imdb_top_250 %>%
  select(imdb_rating, votes) %>%
  correlation()

# Define outliers
votes_out <- 1.5*IQR(imdb_top_250$votes) + median(imdb_top_250$votes)
imdb_r_out <- 1.5*IQR(imdb_top_250$imdb_rating) + median(imdb_top_250$imdb_rating)

# The correlation with outliers (based on 1.5 * IQR)
imdb_top_250 %>%
  filter(votes <  votes_out & imdb_rating < imdb_r_out) %>%
  select(imdb_rating, votes) %>% 
  correlation()
```

While both correlations are highly significant (p \< 0.01), the drop in `r` from 0.59 to 0.31 is substantial. When we plot the data again, we can see that the dots are fairly randomly distributed across the plotting area. Thanks to `geom_smooth` we get an idea of a slight positive relationship between these two variables.

```{r Scatterplot without outliers, echo=TRUE}
imdb_top_250 %>%
  filter(votes <  votes_out & imdb_rating < imdb_r_out) %>%
  ggplot(aes(x = imdb_rating, y = votes)) + 
  geom_jitter() +
  geom_smooth(method = "lm",
              formula = y ~ x,
              se = FALSE,
              colour = "red")
```

In conclusion, while there is some relationship between the rating and the number of votes, it is by far not as strong as we might have thought, especially after removing outliers, which had a considerable effect on the effect size.

## Significance: A way to help you judge your findings {#significance}

One of the most common pitfalls of novice statisticians is related to the interpretation of what counts as significant and not significant. Most basic tests offer a 'p value', which stands for 'probability value'. The `p` value can range from 1 (for 100%) to 0 (for 0%) and implies:

-   $p = 1$, there is a 100% chance that the result is a pure coincidence

-   $p = 0$, there is a 0% chance that the results is a pure coincidence, i.e. we can be certain this is not just luck.

Technically, we would not find that `p` is ever truly zero, and instead denote very small `p` values with `p < 0.01` or even `p< 0.001`.

There are also commonly considered thresholds for the `p` value:

-   $p > 0.05$, the result is not significant. There is chance of 5% that our finding is a pure coincidence.

-   p $\leq 0.05$ , the result is significant.

-   p $\leq 0.01$, the result is highly significant.

We will cover more about the `p` value in Chapter \@ref(comparing-groups) and Chapter \@ref(regression). For now, it is important to know that a significant correlation is one that we should look at more closely. Usually, correlations that nor significant suffer from low effect sizes. However, different samples can lead to different effect sizes and different significant levels. Consider the following examples:

```{r Significance for different sample sizes, echo=FALSE}

df_01 <- tribble(
  ~x, ~y,
  1,   1,
  2,   3,
  3,   3,
  4,   3
)

df_01 %>% correlation()

df_02 <- tribble(
  ~x, ~y,
  1,   1,
  2,   3,
  3,   3,
  4,   3,
  1,   1,
  2,   3,
  3,   3,
  4,   3,
  1,   1,
  2,   3,
  3,   3,
  4,   3
)

df_02 %>% correlation()

```

In both examples $r = 0.77$, but the sample sizes are different (4 vs 12) and the `p` values differ as well. In the first example $p = 0.225$, which means the relationship is not significant, while in the second example, we find that $p < 0.01$ and is therefore highly significant. As a general rule, we find that the bigger the sample, the more likely we find significant results, even though the effect size is small. Therefore, it is important to interpret correlations base on at least three factors:

-   the `p` value, i.e. significance level

-   the `r` value, i.e. the effect size, and

-   the sample size.

The interplay of all three can help determine whether a relationship is truly important. Therefore, when we include correlation tables in publications, we have to make sure we provide information about all three.

It is common that we do not only compute correlations for two variables at a time, instead, we can do this for multiple variables simultaneously.

```{r Correlation table format list, echo=TRUE}
imdb_top_250 %>% 
  select(imdb_rating, metascore, year, votes, gross_in_m) %>% 
  correlation()
```

If you have seen correlation tables before, you might find that `correlation()` does not produce the classic table by default. If you want it to look like the tables in publications, which are more compact but offers less information, you can use the function `summary()`.

```{r Correlation table format table, echo=TRUE}
imdb_top_250 %>% 
  select(imdb_rating, metascore, year, votes, gross_in_m) %>% 
  correlation() %>% 
  summary()
```

This table also provides an answer to our final question, i.e. do movies with more votes earn more money. It appears as if this is true, because $r = 0.56$ and $p < 0.001$. In the classic correlation table you also see `*`. These stand for the difference significant levels:

-   `*`, i.e. $p < 0.05$

-   `**`, i.e. $p < 0.01$

-   `***`, i.e. $p < 0.001$ (although you might find some do not use this as a separate level)

In short, the more `*` there are attached to each value, the more significant a result. In other words, relationships with many `*` likely repeat if we collect data again.

## Limitations of correlations {#limitations-of-correlations}

Correlations are useful, but only to some extend. The three most common limitations you should be aware of are:

-   Correlations are not causal relationships

-   Correlations can be spurious

-   Correlations might only appear in sub-samples of your data

### Correlations are not causal relationships {#correlations-are-not-causal-relationships}

Correlations do not offer insights into causality, i.e. whether change in one variable causes change in the other variable. Correlations only provide insights into whether these two variables tend to change when one of them changes. Still, sometimes we can infer such causality by the nature of the variables. For example, in countries with heavy rain, more umbrellas are sold. It is apparent that buying more umbrellas will not cause more rain, but if there is more rain in a country, we rightly assume that there is a higher demand for umbrellas. If we can theorise the relationship between variables we would rather opt for a regression model instead of a correlation (see Chapter \@ref(regression).

### Correlations can be spurious {#correlations-can-be-spurious}

Just because we find a relationship between two variables does not necessarily mean that are truly related to each other. Instead, it might be possible that a third variable is the reason for the relationship. We call relationships between variables that are cause by a third variable 'spurious correlations'. This third variable can either be part of our dataset or even something we have not measured at all. The latter case would make it impossible to investigate the relationship further. However, we can always test whether some of our variables affect the relationship between the two variables of interest. This can be done by using partial correlations. A partial correlation returns the relationship between two variables minus the relationship two a third variable. Figure \@ref(fig:illustration-spurious-correlation) depicts this visually. While `a` and `b` appear to be correlated with each other, the correlation might only exist because of their correlation with `x`.

```{r Illustration of spurious correlations, fig.cap="Illustration of a spurious correlation", label="illustration-spurious-correlation", echo=FALSE}
library(DiagrammeR)

grViz("
      digraph boxes_and_circles {
      graph [layout = circo]
      
      node [shape = circle,
            style = filled]
      
      node [fillcolor = Salmon]
      x

      node [fillcolor = AliceBlue]
      a b
      
      # edges
      x -> {a b} [arrowsize = 0.5,
                  dir = both]

      a -> b [style = dashed, arrowsize = 0.5, dir = both,
              fontsize = 8]
              }
      ")
```

Let's consider a practical example. We found that `votes` and `gross_in_m` are positively correlated with each other. However, could it be possible that these are affected by the year in which the movies was published. We could assume that movies that appeared later received more votes, because it has become more of a cultural phenomenon to vote about almost everything online[^correlations-1].

[^correlations-1]: If you want to know what people vote on these days, have a look at [www.ranker.com](https://www.ranker.com)

```{r Partial correlation example, echo=TRUE}
# Correlation between variables without considering partial correlations
imdb_top_250 %>% 
  select(votes, gross_in_m, year) %>% 
  correlation()

# Correlation between variables with consideration of partial correlations
imdb_top_250 %>% 
  select(votes, gross_in_m, year) %>% 
  correlation(partial = TRUE)
```

The first table reveals that there is a strong relationship between the variables of interest, i.e. between `votes` and `gross_in_m` with an effect size of $r = 0.56$. There is also a significant relationship between `votes` and `year`, $r = 0.37$ and between `gross_in_m` and `year`, $r = 0.36$. Thus, we should control for `year` to see how it might affect the relationship between `votes` and `gross_in_m`. The second table, suggests that `year` does not much affect the relationship between `votes` and `gross_in_m`. However, we do notice that the relationship between `gross_in_m` and `year` substantially goes down to $r = 0.16$. It appears, as if the `year` has not managed to impact the relationship between `votes` and `gross_in_m` all that much. Therefore, we can be more confident that this relationship is likely not spurious. However, we can never be fully sure, because we might not have all data that could explain this correlation.

### Simpson's Paradox: When correlations betray you {#simpsons-paradox}

The final limitation is so important that it even has its own name: the 'Simpson's Paradox'. Let's find out what is so paradox about some correlations. For this demonstration we have to make use of a different dataset: `simpson` of the `r4np` package. It contains information about changes in student `performance` and changes in `happiness`. The dataset includes responses from three different groups: `Teachers`, `Students` and `Parents`.

We would assume that an increased in students' `performance` will likely increase the `happiness` of participants. After all, all three have stakes in students's `performance`.

```{r Simpson paradox plot and correlation all, echo=TRUE}
simpson %>% 
  ggplot(aes(performance, happiness)) +
  geom_point() +
  geom_smooth(method = "lm",
              formula = y ~ x,
              se = FALSE,
              color = "red")

simpson %>% 
  select(performance, happiness) %>% 
  correlation()
```

It appears we were terrible wrong. It seems as if `performance` and `happiness` are moderately negatively correlated with each other. Thus, the more a student improves their performance, the less happy teachers, parents and even students are. I hope you agree that this is quite counter-intuitive. However, what could be the cause for such a finding.

The Simpson's paradox explains the phenomenon were correlation might not exist, or even is reversed when considering an entire sample. However, when inspecting sub-samples, different correlations are found. So, what happens if we look at the correlations for each group separately?

```{r Simpson paradox plot and correlation all, echo=TRUE}
simpson %>% 
  ggplot(aes(performance, happiness, group = group, colour = group)) +
  geom_point() +
  geom_smooth(method = "lm",
              formula = y ~ x,
              se = FALSE,
              color = "black") +
  
  # Changing colours to make the reference line more visible
  scale_color_manual(values = c("#42BDFF","#FF5C67", "#B3AF25"))

simpson %>% 
  group_by(group) %>% 
  select(performance, happiness) %>% 
  correlation()
```

The results have magically been inverted. Instead of a negative correlation we now find a strong positive correlation for among all groups. This seems to make much sense.

When compute correlations we need to be aware that subsets of our data might show different directions of correlations, or where we found now correlation, there might suddenly be one.

If your study relies on correlations only to detect relationships between variables, which it hopefully does not, it is important to investigate whether the detected or undetected correlations exist. Of course, such an investigation can only be based on data you obtain. The rest remains pure speculation. Nevertheless, as descriptive statistics correlations are very helpful to review the bilateral relationship of your variables. It is often used as a pre-test for regressions (see Chapter \@ref(regression)) and similarly more advanced computations. As a technique to make inferences, the correlation is a good starting point, but should be complemented by other steps, if possible.
